{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"skorecard \u00b6 skorecard is a scikit-learn compatible python package that helps streamline the development of credit risk acceptance models (scorecards). Scorecards are \u2018traditional\u2019 models used by banks in the credit decision process. Internally, scorecards are Logistic Regression models that make use of features that are binned into different groups. The process of binning is usually done manually by experts, and skorecard provides tools to makes this process easier. skorecard is built on top of scikit-learn as well as other excellent open source projects like optbinning , dash and plotly . Features \u2b50 \u00b6 Automate bucketing of features inside scikit-learn pipelines. Dash webapp to help manually tweak bucketing of features with business knowledge Extension to sklearn.linear_model.LogisticRegression that is also able to report p-values Plots and reports to speed up analysis and writing technical documentation. Quick demo \u00b6 skorecard offers a range of bucketers : import pandas as pd from skorecard.bucketers import EqualWidthBucketer df = pd . DataFrame ({ 'column' : range ( 100 )}) ewb = EqualWidthBucketer ( n_bins = 5 ) ewb . fit_transform ( df ) ewb . bucket_table ( 'column' ) #> bucket label Count Count (%) #> 0 -1 Missing 0.0 0.0 #> 1 0 (-inf, 19.8] 20.0 20.0 #> 2 1 (19.8, 39.6] 20.0 20.0 #> 3 2 (39.6, 59.400000000000006] 20.0 20.0 #> 4 3 (59.400000000000006, 79.2] 20.0 20.0 #> 5 4 (79.2, inf] 20.0 20.0 That also support a dash app to explore and update bucket boundaries: ewb . fit_interactive ( df ) #> Dash app running on http://127.0.0.1:8050/ Installation \u00b6 pip3 install skorecard Documentation \u00b6 See ing-bank.github.io/skorecard/ . Presentations \u00b6 Title Host Date Speaker(s) Skorecard: Making logistic regressions great again ING Data Science Meetup 10 June 2021 Daniel Timbrell, Sandro Bjelogrlic, Tim Vink","title":"Index"},{"location":"#skorecard","text":"skorecard is a scikit-learn compatible python package that helps streamline the development of credit risk acceptance models (scorecards). Scorecards are \u2018traditional\u2019 models used by banks in the credit decision process. Internally, scorecards are Logistic Regression models that make use of features that are binned into different groups. The process of binning is usually done manually by experts, and skorecard provides tools to makes this process easier. skorecard is built on top of scikit-learn as well as other excellent open source projects like optbinning , dash and plotly .","title":"skorecard"},{"location":"#features","text":"Automate bucketing of features inside scikit-learn pipelines. Dash webapp to help manually tweak bucketing of features with business knowledge Extension to sklearn.linear_model.LogisticRegression that is also able to report p-values Plots and reports to speed up analysis and writing technical documentation.","title":"Features \u2b50"},{"location":"#quick-demo","text":"skorecard offers a range of bucketers : import pandas as pd from skorecard.bucketers import EqualWidthBucketer df = pd . DataFrame ({ 'column' : range ( 100 )}) ewb = EqualWidthBucketer ( n_bins = 5 ) ewb . fit_transform ( df ) ewb . bucket_table ( 'column' ) #> bucket label Count Count (%) #> 0 -1 Missing 0.0 0.0 #> 1 0 (-inf, 19.8] 20.0 20.0 #> 2 1 (19.8, 39.6] 20.0 20.0 #> 3 2 (39.6, 59.400000000000006] 20.0 20.0 #> 4 3 (59.400000000000006, 79.2] 20.0 20.0 #> 5 4 (79.2, inf] 20.0 20.0 That also support a dash app to explore and update bucket boundaries: ewb . fit_interactive ( df ) #> Dash app running on http://127.0.0.1:8050/","title":"Quick demo"},{"location":"#installation","text":"pip3 install skorecard","title":"Installation"},{"location":"#documentation","text":"See ing-bank.github.io/skorecard/ .","title":"Documentation"},{"location":"#presentations","text":"Title Host Date Speaker(s) Skorecard: Making logistic regressions great again ING Data Science Meetup 10 June 2021 Daniel Timbrell, Sandro Bjelogrlic, Tim Vink","title":"Presentations"},{"location":"contributing/","text":"Contributing guidelines \u00b6 Make sure to discuss any changes you would like to make in the issue board, before putting in any work. Setup \u00b6 Development install: pip install -e '.[all]' Unit testing: pytest We use pre-commit hooks to ensure code styling. Install with: pre-commit install Documentation \u00b6 We use mkdocs with mkdocs-material theme. The docs are structured using the divio documentation system . To view the docs locally: pip install mkdocs-material mkdocs serve Releases and versioning \u00b6 We use semver for versioning. When we are ready for a release, the maintainer runs: git tag -a v0.1 -m \"skorecard v0.1\" && git push origin v0.1 When we create a new github release a github action is triggered that: a new version will be deployed to pypi the docs will be re-built and deployed Logo \u00b6 We adapted the 'scores' noun We used this color scheme from coolors.co We edited the logo using https://boxy-svg.com/app Terminology \u00b6 BucketMapping is a custom class that stores all the information needed for bucketing, including the map itself (either boundaries for binning, or a list of lists for categoricals) FeaturesBucketMapping is simply a collection of BucketMapping s, and is used to store all info for bucketing transformations for a dataset.","title":"Contributing"},{"location":"contributing/#contributing-guidelines","text":"Make sure to discuss any changes you would like to make in the issue board, before putting in any work.","title":"Contributing guidelines"},{"location":"contributing/#setup","text":"Development install: pip install -e '.[all]' Unit testing: pytest We use pre-commit hooks to ensure code styling. Install with: pre-commit install","title":"Setup"},{"location":"contributing/#documentation","text":"We use mkdocs with mkdocs-material theme. The docs are structured using the divio documentation system . To view the docs locally: pip install mkdocs-material mkdocs serve","title":"Documentation"},{"location":"contributing/#releases-and-versioning","text":"We use semver for versioning. When we are ready for a release, the maintainer runs: git tag -a v0.1 -m \"skorecard v0.1\" && git push origin v0.1 When we create a new github release a github action is triggered that: a new version will be deployed to pypi the docs will be re-built and deployed","title":"Releases and versioning"},{"location":"contributing/#logo","text":"We adapted the 'scores' noun We used this color scheme from coolors.co We edited the logo using https://boxy-svg.com/app","title":"Logo"},{"location":"contributing/#terminology","text":"BucketMapping is a custom class that stores all the information needed for bucketing, including the map itself (either boundaries for binning, or a list of lists for categoricals) FeaturesBucketMapping is simply a collection of BucketMapping s, and is used to store all info for bucketing transformations for a dataset.","title":"Terminology"},{"location":"api/bucketers/AgglomerativeClusteringBucketer/","text":"The AgglomerativeClusteringBucketer transformer creates buckets using sklearn.AgglomerativeClustering . Support Examples: from skorecard import datasets from skorecard.bucketers import AgglomerativeClusteringBucketer specials = { \"LIMIT_BAL\" : { \"=50000\" : [ 50000 ], \"in [20001,30000]\" : [ 20000 , 30000 ]}} X , y = datasets . load_uci_credit_card ( return_X_y = True ) bucketer = AgglomerativeClusteringBucketer ( n_bins = 10 , variables = [ 'LIMIT_BAL' ], specials = specials ) bucketer . fit_transform ( X ) bucketer . fit_transform ( X )[ 'LIMIT_BAL' ] . value_counts () variables_type property readonly \u00b6 Signals variables type supported by this bucketer. __init__ ( self , n_bins = 5 , variables = [], specials = {}, missing_treatment = 'separate' , remainder = 'passthrough' , ** kwargs ) special \u00b6 Init the class. Parameters: Name Type Description Default n_bins int Number of bins to create. 5 variables list The features to bucket. Uses all features if not defined. [] specials (dict) of special values that require their own binning. The dictionary has the following format: {\" \" : {\"name of special bucket\" : }} For every feature that needs a special value, a dictionary must be passed as value. This dictionary contains a name of a bucket (key) and an array of unique values that should be put in that bucket. When special values are defined, they are not considered in the fitting procedure. {} missing_treatment Defines how we treat the missing values present in the data. If a string, it must be one of the following options: separate: Missing values get put in a separate 'Other' bucket: -1 most_risky: Missing values are put into the bucket containing the largest percentage of Class 1. least_risky: Missing values are put into the bucket containing the largest percentage of Class 0. most_frequent: Missing values are put into the most common bucket. neutral: Missing values are put into the bucket with WoE closest to 0. similar: Missing values are put into the bucket with WoE closest to the bucket with only missing values. passthrough: Leaves missing values untouched. If a dict, it must be of the following format: {\" \": } This bucket number is where we will put the missing values. 'separate' remainder How we want the non-specified columns to be transformed. It must be in [\"passthrough\", \"drop\"]. passthrough (Default): all columns that were not specified in \"variables\" will be passed through. drop: all remaining columns that were not specified in \"variables\" will be dropped. 'passthrough' kwargs Other parameters passed to AgglomerativeBucketer {} bucket_table ( self , column ) inherited \u00b6 Generates the statistics for the buckets of a particular column. The pre-buckets are matched to the post-buckets, so that the user has a much clearer understanding of how the BucketingProcess ends up with the final buckets. An example: bucket label Count Count (%) Non-event Event % Event % Non-event Event Rate WoE IV 0 (-inf, 25.0) 61.0 1.36 57.0 4.0 0.41 1.62 0.066 1.380 0.017 1 [25.0, 45.0) 2024.0 44.98 1536.0 488.0 49.64 43.67 0.241 -0.128 0.008 Parameters: Name Type Description Default column The column we wish to analyse required Returns: Type Description df (pd.DataFrame) A pandas dataframe of the format above fit ( self , X , y = None ) inherited \u00b6 Fit X, y. fit_interactive ( self , X , y = None , mode = 'external' , ** server_kwargs ) inherited \u00b6 Fit a bucketer and then interactive edit the fit using a dash app. Note we are using a jupyterdash app, which supports 3 different modes: 'external' (default): Start dash server and print URL 'inline': Start dash app inside an Iframe in the jupyter notebook 'jupyterlab': Start dash app as a new tab inside jupyterlab fit_transform ( self , X , y = None , ** fit_params ) inherited \u00b6 Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X . Parameters \u00b6 X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters. Returns \u00b6 X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params ( self , deep = True ) inherited \u00b6 Get parameters for this estimator. Parameters \u00b6 deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns \u00b6 params : dict Parameter names mapped to their values. plot_bucket ( self , column , line = 'event_rate' , format = None , scale = None , width = None , height = None ) inherited \u00b6 Plot the buckets. Parameters: Name Type Description Default column The column we want to visualise required line The line to plot on the secondary axis. Default is Event Rate. 'event_rate' format The format of the image, such as 'png'. The default None returns a plotly image. None scale If format is specified, the scale of the image None width If format is specified, the width of the image None height If format is specified, the image of the image None Returns: Type Description plot plotly fig predict ( self , X ) inherited \u00b6 Applies the transform method. To be used for the grid searches. Parameters: Name Type Description Default X pd.DataFrame The numerical data which will be transformed into the corresponding buckets required Returns: Type Description y (np.array) Transformed X, such that the values of X are replaced by the corresponding bucket numbers predict_proba ( self , X ) inherited \u00b6 Applies the transform method. To be used for the grid searches. Parameters: Name Type Description Default X pd.DataFrame The numerical data which will be transformed into the corresponding buckets required Returns: Type Description yhat (np.array) transformed X, such that the values of X are replaced by the corresponding bucket numbers save_yml ( self , fout ) inherited \u00b6 Save the features bucket to a yaml file. Parameters: Name Type Description Default fout ~PathLike file output required set_params ( self , ** params ) inherited \u00b6 Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class: ~sklearn.pipeline.Pipeline ). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters \u00b6 **params : dict Estimator parameters. Returns \u00b6 self : estimator instance Estimator instance. summary ( self ) inherited \u00b6 Display a summary table for columns passed to .fit() . The format is the following: column num_prebuckets num_buckets dtype LIMIT_BAL 15 10 float64 BILL_AMT1 15 6 float64 transform ( self , X , y = None ) inherited \u00b6 Transforms an array into the corresponding buckets fitted by the Transformer. Parameters: Name Type Description Default X pd.DataFrame dataframe which will be transformed into the corresponding buckets required y array target None Returns: Type Description df (pd.DataFrame) dataset with transformed features","title":"AgglomerativeClusteringBucketer"},{"location":"api/bucketers/AgglomerativeClusteringBucketer/#skorecard.bucketers.bucketers.AgglomerativeClusteringBucketer.variables_type","text":"Signals variables type supported by this bucketer.","title":"variables_type"},{"location":"api/bucketers/AgglomerativeClusteringBucketer/#skorecard.bucketers.bucketers.AgglomerativeClusteringBucketer.__init__","text":"Init the class. Parameters: Name Type Description Default n_bins int Number of bins to create. 5 variables list The features to bucket. Uses all features if not defined. [] specials (dict) of special values that require their own binning. The dictionary has the following format: {\" \" : {\"name of special bucket\" : }} For every feature that needs a special value, a dictionary must be passed as value. This dictionary contains a name of a bucket (key) and an array of unique values that should be put in that bucket. When special values are defined, they are not considered in the fitting procedure. {} missing_treatment Defines how we treat the missing values present in the data. If a string, it must be one of the following options: separate: Missing values get put in a separate 'Other' bucket: -1 most_risky: Missing values are put into the bucket containing the largest percentage of Class 1. least_risky: Missing values are put into the bucket containing the largest percentage of Class 0. most_frequent: Missing values are put into the most common bucket. neutral: Missing values are put into the bucket with WoE closest to 0. similar: Missing values are put into the bucket with WoE closest to the bucket with only missing values. passthrough: Leaves missing values untouched. If a dict, it must be of the following format: {\" \": } This bucket number is where we will put the missing values. 'separate' remainder How we want the non-specified columns to be transformed. It must be in [\"passthrough\", \"drop\"]. passthrough (Default): all columns that were not specified in \"variables\" will be passed through. drop: all remaining columns that were not specified in \"variables\" will be dropped. 'passthrough' kwargs Other parameters passed to AgglomerativeBucketer {}","title":"__init__()"},{"location":"api/bucketers/AgglomerativeClusteringBucketer/#skorecard.bucketers.bucketers.AgglomerativeClusteringBucketer.bucket_table","text":"Generates the statistics for the buckets of a particular column. The pre-buckets are matched to the post-buckets, so that the user has a much clearer understanding of how the BucketingProcess ends up with the final buckets. An example: bucket label Count Count (%) Non-event Event % Event % Non-event Event Rate WoE IV 0 (-inf, 25.0) 61.0 1.36 57.0 4.0 0.41 1.62 0.066 1.380 0.017 1 [25.0, 45.0) 2024.0 44.98 1536.0 488.0 49.64 43.67 0.241 -0.128 0.008 Parameters: Name Type Description Default column The column we wish to analyse required Returns: Type Description df (pd.DataFrame) A pandas dataframe of the format above","title":"bucket_table()"},{"location":"api/bucketers/AgglomerativeClusteringBucketer/#skorecard.bucketers.bucketers.AgglomerativeClusteringBucketer.fit","text":"Fit X, y.","title":"fit()"},{"location":"api/bucketers/AgglomerativeClusteringBucketer/#skorecard.bucketers.bucketers.AgglomerativeClusteringBucketer.fit_interactive","text":"Fit a bucketer and then interactive edit the fit using a dash app. Note we are using a jupyterdash app, which supports 3 different modes: 'external' (default): Start dash server and print URL 'inline': Start dash app inside an Iframe in the jupyter notebook 'jupyterlab': Start dash app as a new tab inside jupyterlab","title":"fit_interactive()"},{"location":"api/bucketers/AgglomerativeClusteringBucketer/#skorecard.bucketers.bucketers.AgglomerativeClusteringBucketer.fit_transform","text":"Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X .","title":"fit_transform()"},{"location":"api/bucketers/AgglomerativeClusteringBucketer/#skorecard.bucketers.bucketers.AgglomerativeClusteringBucketer.fit_transform--parameters","text":"X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters.","title":"Parameters"},{"location":"api/bucketers/AgglomerativeClusteringBucketer/#skorecard.bucketers.bucketers.AgglomerativeClusteringBucketer.fit_transform--returns","text":"X_new : ndarray array of shape (n_samples, n_features_new) Transformed array.","title":"Returns"},{"location":"api/bucketers/AgglomerativeClusteringBucketer/#skorecard.bucketers.bucketers.AgglomerativeClusteringBucketer.get_params","text":"Get parameters for this estimator.","title":"get_params()"},{"location":"api/bucketers/AgglomerativeClusteringBucketer/#skorecard.bucketers.bucketers.AgglomerativeClusteringBucketer.get_params--parameters","text":"deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators.","title":"Parameters"},{"location":"api/bucketers/AgglomerativeClusteringBucketer/#skorecard.bucketers.bucketers.AgglomerativeClusteringBucketer.get_params--returns","text":"params : dict Parameter names mapped to their values.","title":"Returns"},{"location":"api/bucketers/AgglomerativeClusteringBucketer/#skorecard.bucketers.bucketers.AgglomerativeClusteringBucketer.plot_bucket","text":"Plot the buckets. Parameters: Name Type Description Default column The column we want to visualise required line The line to plot on the secondary axis. Default is Event Rate. 'event_rate' format The format of the image, such as 'png'. The default None returns a plotly image. None scale If format is specified, the scale of the image None width If format is specified, the width of the image None height If format is specified, the image of the image None Returns: Type Description plot plotly fig","title":"plot_bucket()"},{"location":"api/bucketers/AgglomerativeClusteringBucketer/#skorecard.bucketers.bucketers.AgglomerativeClusteringBucketer.predict","text":"Applies the transform method. To be used for the grid searches. Parameters: Name Type Description Default X pd.DataFrame The numerical data which will be transformed into the corresponding buckets required Returns: Type Description y (np.array) Transformed X, such that the values of X are replaced by the corresponding bucket numbers","title":"predict()"},{"location":"api/bucketers/AgglomerativeClusteringBucketer/#skorecard.bucketers.bucketers.AgglomerativeClusteringBucketer.predict_proba","text":"Applies the transform method. To be used for the grid searches. Parameters: Name Type Description Default X pd.DataFrame The numerical data which will be transformed into the corresponding buckets required Returns: Type Description yhat (np.array) transformed X, such that the values of X are replaced by the corresponding bucket numbers","title":"predict_proba()"},{"location":"api/bucketers/AgglomerativeClusteringBucketer/#skorecard.bucketers.bucketers.AgglomerativeClusteringBucketer.save_yml","text":"Save the features bucket to a yaml file. Parameters: Name Type Description Default fout ~PathLike file output required","title":"save_yml()"},{"location":"api/bucketers/AgglomerativeClusteringBucketer/#skorecard.bucketers.bucketers.AgglomerativeClusteringBucketer.set_params","text":"Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class: ~sklearn.pipeline.Pipeline ). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object.","title":"set_params()"},{"location":"api/bucketers/AgglomerativeClusteringBucketer/#skorecard.bucketers.bucketers.AgglomerativeClusteringBucketer.set_params--parameters","text":"**params : dict Estimator parameters.","title":"Parameters"},{"location":"api/bucketers/AgglomerativeClusteringBucketer/#skorecard.bucketers.bucketers.AgglomerativeClusteringBucketer.set_params--returns","text":"self : estimator instance Estimator instance.","title":"Returns"},{"location":"api/bucketers/AgglomerativeClusteringBucketer/#skorecard.bucketers.bucketers.AgglomerativeClusteringBucketer.summary","text":"Display a summary table for columns passed to .fit() . The format is the following: column num_prebuckets num_buckets dtype LIMIT_BAL 15 10 float64 BILL_AMT1 15 6 float64","title":"summary()"},{"location":"api/bucketers/AgglomerativeClusteringBucketer/#skorecard.bucketers.bucketers.AgglomerativeClusteringBucketer.transform","text":"Transforms an array into the corresponding buckets fitted by the Transformer. Parameters: Name Type Description Default X pd.DataFrame dataframe which will be transformed into the corresponding buckets required y array target None Returns: Type Description df (pd.DataFrame) dataset with transformed features","title":"transform()"},{"location":"api/bucketers/AsIsCategoricalBucketer/","text":"The AsIsCategoricalBucketer treats unique values as categories. Support: It will assign each a bucket number in the order of appearance. If new data contains new, unknown labels they will be replaced by 'Other'. This is bucketer is useful when you have data that is already sufficiented bucketed, but you would like to be able to bucket new data in the same way. Examples: from skorecard import datasets from skorecard.bucketers import AsIsCategoricalBucketer X , y = datasets . load_uci_credit_card ( return_X_y = True ) bucketer = AsIsCategoricalBucketer ( variables = [ 'EDUCATION' ]) bucketer . fit_transform ( X ) variables_type property readonly \u00b6 Signals variables type supported by this bucketer. __init__ ( self , variables = [], specials = {}, missing_treatment = 'separate' , remainder = 'passthrough' ) special \u00b6 Init the class. Parameters: Name Type Description Default variables list The features to bucket. Uses all features if not defined. [] specials (nested) dictionary of special values that require their own binning. The dictionary has the following format: {\" \" : {\"name of special bucket\" : }} For every feature that needs a special value, a dictionary must be passed as value. This dictionary contains a name of a bucket (key) and an array of unique values that should be put in that bucket. When special values are defined, they are not considered in the fitting procedure. {} missing_treatment Defines how we treat the missing values present in the data. If a string, it must be one of the following options: separate: Missing values get put in a separate 'Other' bucket: -1 most_risky: Missing values are put into the bucket containing the largest percentage of Class 1. least_risky: Missing values are put into the bucket containing the largest percentage of Class 0. most_frequent: Missing values are put into the most common bucket. neutral: Missing values are put into the bucket with WoE closest to 0. similar: Missing values are put into the bucket with WoE closest to the bucket with only missing values. passthrough: Leaves missing values untouched. If a dict, it must be of the following format: {\" \": } This bucket number is where we will put the missing values. 'separate' remainder How we want the non-specified columns to be transformed. It must be in [\"passthrough\", \"drop\"]. passthrough (Default): all columns that were not specified in \"variables\" will be passed through. drop: all remaining columns that were not specified in \"variables\" will be dropped. 'passthrough' bucket_table ( self , column ) inherited \u00b6 Generates the statistics for the buckets of a particular column. The pre-buckets are matched to the post-buckets, so that the user has a much clearer understanding of how the BucketingProcess ends up with the final buckets. An example: bucket label Count Count (%) Non-event Event % Event % Non-event Event Rate WoE IV 0 (-inf, 25.0) 61.0 1.36 57.0 4.0 0.41 1.62 0.066 1.380 0.017 1 [25.0, 45.0) 2024.0 44.98 1536.0 488.0 49.64 43.67 0.241 -0.128 0.008 Parameters: Name Type Description Default column The column we wish to analyse required Returns: Type Description df (pd.DataFrame) A pandas dataframe of the format above fit ( self , X , y = None ) inherited \u00b6 Fit X, y. fit_interactive ( self , X , y = None , mode = 'external' , ** server_kwargs ) inherited \u00b6 Fit a bucketer and then interactive edit the fit using a dash app. Note we are using a jupyterdash app, which supports 3 different modes: 'external' (default): Start dash server and print URL 'inline': Start dash app inside an Iframe in the jupyter notebook 'jupyterlab': Start dash app as a new tab inside jupyterlab fit_transform ( self , X , y = None , ** fit_params ) inherited \u00b6 Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X . Parameters \u00b6 X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters. Returns \u00b6 X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params ( self , deep = True ) inherited \u00b6 Get parameters for this estimator. Parameters \u00b6 deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns \u00b6 params : dict Parameter names mapped to their values. plot_bucket ( self , column , line = 'event_rate' , format = None , scale = None , width = None , height = None ) inherited \u00b6 Plot the buckets. Parameters: Name Type Description Default column The column we want to visualise required line The line to plot on the secondary axis. Default is Event Rate. 'event_rate' format The format of the image, such as 'png'. The default None returns a plotly image. None scale If format is specified, the scale of the image None width If format is specified, the width of the image None height If format is specified, the image of the image None Returns: Type Description plot plotly fig predict ( self , X ) inherited \u00b6 Applies the transform method. To be used for the grid searches. Parameters: Name Type Description Default X pd.DataFrame The numerical data which will be transformed into the corresponding buckets required Returns: Type Description y (np.array) Transformed X, such that the values of X are replaced by the corresponding bucket numbers predict_proba ( self , X ) inherited \u00b6 Applies the transform method. To be used for the grid searches. Parameters: Name Type Description Default X pd.DataFrame The numerical data which will be transformed into the corresponding buckets required Returns: Type Description yhat (np.array) transformed X, such that the values of X are replaced by the corresponding bucket numbers save_yml ( self , fout ) inherited \u00b6 Save the features bucket to a yaml file. Parameters: Name Type Description Default fout ~PathLike file output required set_params ( self , ** params ) inherited \u00b6 Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class: ~sklearn.pipeline.Pipeline ). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters \u00b6 **params : dict Estimator parameters. Returns \u00b6 self : estimator instance Estimator instance. summary ( self ) inherited \u00b6 Display a summary table for columns passed to .fit() . The format is the following: column num_prebuckets num_buckets dtype LIMIT_BAL 15 10 float64 BILL_AMT1 15 6 float64 transform ( self , X , y = None ) inherited \u00b6 Transforms an array into the corresponding buckets fitted by the Transformer. Parameters: Name Type Description Default X pd.DataFrame dataframe which will be transformed into the corresponding buckets required y array target None Returns: Type Description df (pd.DataFrame) dataset with transformed features","title":"AsIsCategoricalBucketer"},{"location":"api/bucketers/AsIsCategoricalBucketer/#skorecard.bucketers.bucketers.AsIsCategoricalBucketer.variables_type","text":"Signals variables type supported by this bucketer.","title":"variables_type"},{"location":"api/bucketers/AsIsCategoricalBucketer/#skorecard.bucketers.bucketers.AsIsCategoricalBucketer.__init__","text":"Init the class. Parameters: Name Type Description Default variables list The features to bucket. Uses all features if not defined. [] specials (nested) dictionary of special values that require their own binning. The dictionary has the following format: {\" \" : {\"name of special bucket\" : }} For every feature that needs a special value, a dictionary must be passed as value. This dictionary contains a name of a bucket (key) and an array of unique values that should be put in that bucket. When special values are defined, they are not considered in the fitting procedure. {} missing_treatment Defines how we treat the missing values present in the data. If a string, it must be one of the following options: separate: Missing values get put in a separate 'Other' bucket: -1 most_risky: Missing values are put into the bucket containing the largest percentage of Class 1. least_risky: Missing values are put into the bucket containing the largest percentage of Class 0. most_frequent: Missing values are put into the most common bucket. neutral: Missing values are put into the bucket with WoE closest to 0. similar: Missing values are put into the bucket with WoE closest to the bucket with only missing values. passthrough: Leaves missing values untouched. If a dict, it must be of the following format: {\" \": } This bucket number is where we will put the missing values. 'separate' remainder How we want the non-specified columns to be transformed. It must be in [\"passthrough\", \"drop\"]. passthrough (Default): all columns that were not specified in \"variables\" will be passed through. drop: all remaining columns that were not specified in \"variables\" will be dropped. 'passthrough'","title":"__init__()"},{"location":"api/bucketers/AsIsCategoricalBucketer/#skorecard.bucketers.bucketers.AsIsCategoricalBucketer.bucket_table","text":"Generates the statistics for the buckets of a particular column. The pre-buckets are matched to the post-buckets, so that the user has a much clearer understanding of how the BucketingProcess ends up with the final buckets. An example: bucket label Count Count (%) Non-event Event % Event % Non-event Event Rate WoE IV 0 (-inf, 25.0) 61.0 1.36 57.0 4.0 0.41 1.62 0.066 1.380 0.017 1 [25.0, 45.0) 2024.0 44.98 1536.0 488.0 49.64 43.67 0.241 -0.128 0.008 Parameters: Name Type Description Default column The column we wish to analyse required Returns: Type Description df (pd.DataFrame) A pandas dataframe of the format above","title":"bucket_table()"},{"location":"api/bucketers/AsIsCategoricalBucketer/#skorecard.bucketers.bucketers.AsIsCategoricalBucketer.fit","text":"Fit X, y.","title":"fit()"},{"location":"api/bucketers/AsIsCategoricalBucketer/#skorecard.bucketers.bucketers.AsIsCategoricalBucketer.fit_interactive","text":"Fit a bucketer and then interactive edit the fit using a dash app. Note we are using a jupyterdash app, which supports 3 different modes: 'external' (default): Start dash server and print URL 'inline': Start dash app inside an Iframe in the jupyter notebook 'jupyterlab': Start dash app as a new tab inside jupyterlab","title":"fit_interactive()"},{"location":"api/bucketers/AsIsCategoricalBucketer/#skorecard.bucketers.bucketers.AsIsCategoricalBucketer.fit_transform","text":"Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X .","title":"fit_transform()"},{"location":"api/bucketers/AsIsCategoricalBucketer/#skorecard.bucketers.bucketers.AsIsCategoricalBucketer.fit_transform--parameters","text":"X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters.","title":"Parameters"},{"location":"api/bucketers/AsIsCategoricalBucketer/#skorecard.bucketers.bucketers.AsIsCategoricalBucketer.fit_transform--returns","text":"X_new : ndarray array of shape (n_samples, n_features_new) Transformed array.","title":"Returns"},{"location":"api/bucketers/AsIsCategoricalBucketer/#skorecard.bucketers.bucketers.AsIsCategoricalBucketer.get_params","text":"Get parameters for this estimator.","title":"get_params()"},{"location":"api/bucketers/AsIsCategoricalBucketer/#skorecard.bucketers.bucketers.AsIsCategoricalBucketer.get_params--parameters","text":"deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators.","title":"Parameters"},{"location":"api/bucketers/AsIsCategoricalBucketer/#skorecard.bucketers.bucketers.AsIsCategoricalBucketer.get_params--returns","text":"params : dict Parameter names mapped to their values.","title":"Returns"},{"location":"api/bucketers/AsIsCategoricalBucketer/#skorecard.bucketers.bucketers.AsIsCategoricalBucketer.plot_bucket","text":"Plot the buckets. Parameters: Name Type Description Default column The column we want to visualise required line The line to plot on the secondary axis. Default is Event Rate. 'event_rate' format The format of the image, such as 'png'. The default None returns a plotly image. None scale If format is specified, the scale of the image None width If format is specified, the width of the image None height If format is specified, the image of the image None Returns: Type Description plot plotly fig","title":"plot_bucket()"},{"location":"api/bucketers/AsIsCategoricalBucketer/#skorecard.bucketers.bucketers.AsIsCategoricalBucketer.predict","text":"Applies the transform method. To be used for the grid searches. Parameters: Name Type Description Default X pd.DataFrame The numerical data which will be transformed into the corresponding buckets required Returns: Type Description y (np.array) Transformed X, such that the values of X are replaced by the corresponding bucket numbers","title":"predict()"},{"location":"api/bucketers/AsIsCategoricalBucketer/#skorecard.bucketers.bucketers.AsIsCategoricalBucketer.predict_proba","text":"Applies the transform method. To be used for the grid searches. Parameters: Name Type Description Default X pd.DataFrame The numerical data which will be transformed into the corresponding buckets required Returns: Type Description yhat (np.array) transformed X, such that the values of X are replaced by the corresponding bucket numbers","title":"predict_proba()"},{"location":"api/bucketers/AsIsCategoricalBucketer/#skorecard.bucketers.bucketers.AsIsCategoricalBucketer.save_yml","text":"Save the features bucket to a yaml file. Parameters: Name Type Description Default fout ~PathLike file output required","title":"save_yml()"},{"location":"api/bucketers/AsIsCategoricalBucketer/#skorecard.bucketers.bucketers.AsIsCategoricalBucketer.set_params","text":"Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class: ~sklearn.pipeline.Pipeline ). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object.","title":"set_params()"},{"location":"api/bucketers/AsIsCategoricalBucketer/#skorecard.bucketers.bucketers.AsIsCategoricalBucketer.set_params--parameters","text":"**params : dict Estimator parameters.","title":"Parameters"},{"location":"api/bucketers/AsIsCategoricalBucketer/#skorecard.bucketers.bucketers.AsIsCategoricalBucketer.set_params--returns","text":"self : estimator instance Estimator instance.","title":"Returns"},{"location":"api/bucketers/AsIsCategoricalBucketer/#skorecard.bucketers.bucketers.AsIsCategoricalBucketer.summary","text":"Display a summary table for columns passed to .fit() . The format is the following: column num_prebuckets num_buckets dtype LIMIT_BAL 15 10 float64 BILL_AMT1 15 6 float64","title":"summary()"},{"location":"api/bucketers/AsIsCategoricalBucketer/#skorecard.bucketers.bucketers.AsIsCategoricalBucketer.transform","text":"Transforms an array into the corresponding buckets fitted by the Transformer. Parameters: Name Type Description Default X pd.DataFrame dataframe which will be transformed into the corresponding buckets required y array target None Returns: Type Description df (pd.DataFrame) dataset with transformed features","title":"transform()"},{"location":"api/bucketers/AsIsNumericalBucketer/","text":"The AsIsNumericalBucketer transformer creates buckets by treating the existing unique values as boundaries. Support: This is bucketer is useful when you have data that is already sufficiented bucketed, but you would like to be able to bucket new data in the same way. Examples: from skorecard import datasets from skorecard.bucketers import AsIsNumericalBucketer X , y = datasets . load_uci_credit_card ( return_X_y = True ) bucketer = AsIsNumericalBucketer ( variables = [ 'LIMIT_BAL' ]) bucketer . fit_transform ( X ) variables_type property readonly \u00b6 Signals variables type supported by this bucketer. __init__ ( self , right = True , variables = [], specials = {}, missing_treatment = 'separate' , remainder = 'passthrough' ) special \u00b6 Init the class. Parameters: Name Type Description Default right boolean Is the right value included in a range (default) or is 'up to not but including'. For example, if you have [5, 10], the ranges for right=True would be (-Inf, 5], (5, 10], (10, Inf] or [-Inf, 5), [5, 10), [10, Inf) for right=False True variables list The features to bucket. Uses all features if not defined. [] specials dict (nested) dictionary of special values that require their own binning. The dictionary has the following format: {\" \" : {\"name of special bucket\" : }} For every feature that needs a special value, a dictionary must be passed as value. This dictionary contains a name of a bucket (key) and an array of unique values that should be put in that bucket. When special values are defined, they are not considered in the fitting procedure. {} missing_treatment str or dict Defines how we treat the missing values present in the data. If a string, it must be one of the following options: separate: Missing values get put in a separate 'Other' bucket: -1 most_risky: Missing values are put into the bucket containing the largest percentage of Class 1. least_risky: Missing values are put into the bucket containing the largest percentage of Class 0. most_frequent: Missing values are put into the most common bucket. neutral: Missing values are put into the bucket with WoE closest to 0. similar: Missing values are put into the bucket with WoE closest to the bucket with only missing values. passthrough: Leaves missing values untouched. If a dict, it must be of the following format: {\" \": } This bucket number is where we will put the missing values.. 'separate' remainder str How we want the non-specified columns to be transformed. It must be in [\"passthrough\", \"drop\"]. passthrough (Default): all columns that were not specified in \"variables\" will be passed through. drop: all remaining columns that were not specified in \"variables\" will be dropped. 'passthrough' bucket_table ( self , column ) inherited \u00b6 Generates the statistics for the buckets of a particular column. The pre-buckets are matched to the post-buckets, so that the user has a much clearer understanding of how the BucketingProcess ends up with the final buckets. An example: bucket label Count Count (%) Non-event Event % Event % Non-event Event Rate WoE IV 0 (-inf, 25.0) 61.0 1.36 57.0 4.0 0.41 1.62 0.066 1.380 0.017 1 [25.0, 45.0) 2024.0 44.98 1536.0 488.0 49.64 43.67 0.241 -0.128 0.008 Parameters: Name Type Description Default column The column we wish to analyse required Returns: Type Description df (pd.DataFrame) A pandas dataframe of the format above fit ( self , X , y = None ) inherited \u00b6 Fit X, y. fit_interactive ( self , X , y = None , mode = 'external' , ** server_kwargs ) inherited \u00b6 Fit a bucketer and then interactive edit the fit using a dash app. Note we are using a jupyterdash app, which supports 3 different modes: 'external' (default): Start dash server and print URL 'inline': Start dash app inside an Iframe in the jupyter notebook 'jupyterlab': Start dash app as a new tab inside jupyterlab fit_transform ( self , X , y = None , ** fit_params ) inherited \u00b6 Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X . Parameters \u00b6 X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters. Returns \u00b6 X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params ( self , deep = True ) inherited \u00b6 Get parameters for this estimator. Parameters \u00b6 deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns \u00b6 params : dict Parameter names mapped to their values. plot_bucket ( self , column , line = 'event_rate' , format = None , scale = None , width = None , height = None ) inherited \u00b6 Plot the buckets. Parameters: Name Type Description Default column The column we want to visualise required line The line to plot on the secondary axis. Default is Event Rate. 'event_rate' format The format of the image, such as 'png'. The default None returns a plotly image. None scale If format is specified, the scale of the image None width If format is specified, the width of the image None height If format is specified, the image of the image None Returns: Type Description plot plotly fig predict ( self , X ) inherited \u00b6 Applies the transform method. To be used for the grid searches. Parameters: Name Type Description Default X pd.DataFrame The numerical data which will be transformed into the corresponding buckets required Returns: Type Description y (np.array) Transformed X, such that the values of X are replaced by the corresponding bucket numbers predict_proba ( self , X ) inherited \u00b6 Applies the transform method. To be used for the grid searches. Parameters: Name Type Description Default X pd.DataFrame The numerical data which will be transformed into the corresponding buckets required Returns: Type Description yhat (np.array) transformed X, such that the values of X are replaced by the corresponding bucket numbers save_yml ( self , fout ) inherited \u00b6 Save the features bucket to a yaml file. Parameters: Name Type Description Default fout ~PathLike file output required set_params ( self , ** params ) inherited \u00b6 Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class: ~sklearn.pipeline.Pipeline ). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters \u00b6 **params : dict Estimator parameters. Returns \u00b6 self : estimator instance Estimator instance. summary ( self ) inherited \u00b6 Display a summary table for columns passed to .fit() . The format is the following: column num_prebuckets num_buckets dtype LIMIT_BAL 15 10 float64 BILL_AMT1 15 6 float64 transform ( self , X , y = None ) inherited \u00b6 Transforms an array into the corresponding buckets fitted by the Transformer. Parameters: Name Type Description Default X pd.DataFrame dataframe which will be transformed into the corresponding buckets required y array target None Returns: Type Description df (pd.DataFrame) dataset with transformed features","title":"AsIsNumericalBucketer"},{"location":"api/bucketers/AsIsNumericalBucketer/#skorecard.bucketers.bucketers.AsIsNumericalBucketer.variables_type","text":"Signals variables type supported by this bucketer.","title":"variables_type"},{"location":"api/bucketers/AsIsNumericalBucketer/#skorecard.bucketers.bucketers.AsIsNumericalBucketer.__init__","text":"Init the class. Parameters: Name Type Description Default right boolean Is the right value included in a range (default) or is 'up to not but including'. For example, if you have [5, 10], the ranges for right=True would be (-Inf, 5], (5, 10], (10, Inf] or [-Inf, 5), [5, 10), [10, Inf) for right=False True variables list The features to bucket. Uses all features if not defined. [] specials dict (nested) dictionary of special values that require their own binning. The dictionary has the following format: {\" \" : {\"name of special bucket\" : }} For every feature that needs a special value, a dictionary must be passed as value. This dictionary contains a name of a bucket (key) and an array of unique values that should be put in that bucket. When special values are defined, they are not considered in the fitting procedure. {} missing_treatment str or dict Defines how we treat the missing values present in the data. If a string, it must be one of the following options: separate: Missing values get put in a separate 'Other' bucket: -1 most_risky: Missing values are put into the bucket containing the largest percentage of Class 1. least_risky: Missing values are put into the bucket containing the largest percentage of Class 0. most_frequent: Missing values are put into the most common bucket. neutral: Missing values are put into the bucket with WoE closest to 0. similar: Missing values are put into the bucket with WoE closest to the bucket with only missing values. passthrough: Leaves missing values untouched. If a dict, it must be of the following format: {\" \": } This bucket number is where we will put the missing values.. 'separate' remainder str How we want the non-specified columns to be transformed. It must be in [\"passthrough\", \"drop\"]. passthrough (Default): all columns that were not specified in \"variables\" will be passed through. drop: all remaining columns that were not specified in \"variables\" will be dropped. 'passthrough'","title":"__init__()"},{"location":"api/bucketers/AsIsNumericalBucketer/#skorecard.bucketers.bucketers.AsIsNumericalBucketer.bucket_table","text":"Generates the statistics for the buckets of a particular column. The pre-buckets are matched to the post-buckets, so that the user has a much clearer understanding of how the BucketingProcess ends up with the final buckets. An example: bucket label Count Count (%) Non-event Event % Event % Non-event Event Rate WoE IV 0 (-inf, 25.0) 61.0 1.36 57.0 4.0 0.41 1.62 0.066 1.380 0.017 1 [25.0, 45.0) 2024.0 44.98 1536.0 488.0 49.64 43.67 0.241 -0.128 0.008 Parameters: Name Type Description Default column The column we wish to analyse required Returns: Type Description df (pd.DataFrame) A pandas dataframe of the format above","title":"bucket_table()"},{"location":"api/bucketers/AsIsNumericalBucketer/#skorecard.bucketers.bucketers.AsIsNumericalBucketer.fit","text":"Fit X, y.","title":"fit()"},{"location":"api/bucketers/AsIsNumericalBucketer/#skorecard.bucketers.bucketers.AsIsNumericalBucketer.fit_interactive","text":"Fit a bucketer and then interactive edit the fit using a dash app. Note we are using a jupyterdash app, which supports 3 different modes: 'external' (default): Start dash server and print URL 'inline': Start dash app inside an Iframe in the jupyter notebook 'jupyterlab': Start dash app as a new tab inside jupyterlab","title":"fit_interactive()"},{"location":"api/bucketers/AsIsNumericalBucketer/#skorecard.bucketers.bucketers.AsIsNumericalBucketer.fit_transform","text":"Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X .","title":"fit_transform()"},{"location":"api/bucketers/AsIsNumericalBucketer/#skorecard.bucketers.bucketers.AsIsNumericalBucketer.fit_transform--parameters","text":"X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters.","title":"Parameters"},{"location":"api/bucketers/AsIsNumericalBucketer/#skorecard.bucketers.bucketers.AsIsNumericalBucketer.fit_transform--returns","text":"X_new : ndarray array of shape (n_samples, n_features_new) Transformed array.","title":"Returns"},{"location":"api/bucketers/AsIsNumericalBucketer/#skorecard.bucketers.bucketers.AsIsNumericalBucketer.get_params","text":"Get parameters for this estimator.","title":"get_params()"},{"location":"api/bucketers/AsIsNumericalBucketer/#skorecard.bucketers.bucketers.AsIsNumericalBucketer.get_params--parameters","text":"deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators.","title":"Parameters"},{"location":"api/bucketers/AsIsNumericalBucketer/#skorecard.bucketers.bucketers.AsIsNumericalBucketer.get_params--returns","text":"params : dict Parameter names mapped to their values.","title":"Returns"},{"location":"api/bucketers/AsIsNumericalBucketer/#skorecard.bucketers.bucketers.AsIsNumericalBucketer.plot_bucket","text":"Plot the buckets. Parameters: Name Type Description Default column The column we want to visualise required line The line to plot on the secondary axis. Default is Event Rate. 'event_rate' format The format of the image, such as 'png'. The default None returns a plotly image. None scale If format is specified, the scale of the image None width If format is specified, the width of the image None height If format is specified, the image of the image None Returns: Type Description plot plotly fig","title":"plot_bucket()"},{"location":"api/bucketers/AsIsNumericalBucketer/#skorecard.bucketers.bucketers.AsIsNumericalBucketer.predict","text":"Applies the transform method. To be used for the grid searches. Parameters: Name Type Description Default X pd.DataFrame The numerical data which will be transformed into the corresponding buckets required Returns: Type Description y (np.array) Transformed X, such that the values of X are replaced by the corresponding bucket numbers","title":"predict()"},{"location":"api/bucketers/AsIsNumericalBucketer/#skorecard.bucketers.bucketers.AsIsNumericalBucketer.predict_proba","text":"Applies the transform method. To be used for the grid searches. Parameters: Name Type Description Default X pd.DataFrame The numerical data which will be transformed into the corresponding buckets required Returns: Type Description yhat (np.array) transformed X, such that the values of X are replaced by the corresponding bucket numbers","title":"predict_proba()"},{"location":"api/bucketers/AsIsNumericalBucketer/#skorecard.bucketers.bucketers.AsIsNumericalBucketer.save_yml","text":"Save the features bucket to a yaml file. Parameters: Name Type Description Default fout ~PathLike file output required","title":"save_yml()"},{"location":"api/bucketers/AsIsNumericalBucketer/#skorecard.bucketers.bucketers.AsIsNumericalBucketer.set_params","text":"Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class: ~sklearn.pipeline.Pipeline ). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object.","title":"set_params()"},{"location":"api/bucketers/AsIsNumericalBucketer/#skorecard.bucketers.bucketers.AsIsNumericalBucketer.set_params--parameters","text":"**params : dict Estimator parameters.","title":"Parameters"},{"location":"api/bucketers/AsIsNumericalBucketer/#skorecard.bucketers.bucketers.AsIsNumericalBucketer.set_params--returns","text":"self : estimator instance Estimator instance.","title":"Returns"},{"location":"api/bucketers/AsIsNumericalBucketer/#skorecard.bucketers.bucketers.AsIsNumericalBucketer.summary","text":"Display a summary table for columns passed to .fit() . The format is the following: column num_prebuckets num_buckets dtype LIMIT_BAL 15 10 float64 BILL_AMT1 15 6 float64","title":"summary()"},{"location":"api/bucketers/AsIsNumericalBucketer/#skorecard.bucketers.bucketers.AsIsNumericalBucketer.transform","text":"Transforms an array into the corresponding buckets fitted by the Transformer. Parameters: Name Type Description Default X pd.DataFrame dataframe which will be transformed into the corresponding buckets required y array target None Returns: Type Description df (pd.DataFrame) dataset with transformed features","title":"transform()"},{"location":"api/bucketers/DecisionTreeBucketer/","text":"The DecisionTreeBucketer transformer creates buckets by training a decision tree. Support: It uses sklearn.tree.DecisionTreeClassifier to find the splits. Examples: from skorecard import datasets from skorecard.bucketers import DecisionTreeBucketer X , y = datasets . load_uci_credit_card ( return_X_y = True ) # make sure that those cases specials = { \"LIMIT_BAL\" :{ \"=50000\" :[ 50000 ], \"in [20001,30000]\" :[ 20000 , 30000 ], } } dt_bucketer = DecisionTreeBucketer ( variables = [ 'LIMIT_BAL' ], specials = specials ) dt_bucketer . fit ( X , y ) dt_bucketer . fit_transform ( X , y )[ 'LIMIT_BAL' ] . value_counts () variables_type property readonly \u00b6 Signals variables type supported by this bucketer. __init__ ( self , variables = [], specials = {}, max_n_bins = 100 , missing_treatment = 'separate' , min_bin_size = 0.05 , random_state = None , remainder = 'passthrough' , dt_kwargs = { 'random_state' : None }) special \u00b6 Init the class. Parameters: Name Type Description Default variables list The features to bucket. Uses all features if not defined. [] specials dict dictionary of special values that require their own binning. The dictionary has the following format: {\" \" : {\"name of special bucket\" : }} For every feature that needs a special value, a dictionary must be passed as value. This dictionary contains a name of a bucket (key) and an array of unique values that should be put in that bucket. When special values are defined, they are not considered in the fitting procedure. {} min_bin_size float Minimum fraction of observations in a bucket. Passed directly to min_samples_leaf. 0.05 max_n_bins int Maximum numbers of after the bucketing. Passed directly to max_leaf_nodes of the DecisionTreeClassifier. If specials are defined, max_leaf_nodes will be redefined to max_n_bins - (number of special bins). The DecisionTreeClassifier requires max_leaf_nodes>=2: therefore, max_n_bins must always be >= (number of special bins + 2) if specials are defined, otherwise must be >=2. 100 missing_treatment str or dict Defines how we treat the missing values present in the data. If a string, it must be one of the following options: separate: Missing values get put in a separate 'Other' bucket: -1 most_risky: Missing values are put into the bucket containing the largest percentage of Class 1. least_risky: Missing values are put into the bucket containing the largest percentage of Class 0. most_frequent: Missing values are put into the most common bucket. neutral: Missing values are put into the bucket with WoE closest to 0. similar: Missing values are put into the bucket with WoE closest to the bucket with only missing values. passthrough: Leaves missing values untouched. If a dict, it must be of the following format: {\" \": } This bucket number is where we will put the missing values. 'separate' random_state int The random state, Passed directly to DecisionTreeClassifier None remainder str How we want the non-specified columns to be transformed. It must be in [\"passthrough\", \"drop\"]. passthrough (Default): all columns that were not specified in \"variables\" will be passed through. drop: all remaining columns that were not specified in \"variables\" will be dropped. 'passthrough' dt_kwargs Other parameters passed to DecisionTreeClassifier {'random_state': None} bucket_table ( self , column ) inherited \u00b6 Generates the statistics for the buckets of a particular column. The pre-buckets are matched to the post-buckets, so that the user has a much clearer understanding of how the BucketingProcess ends up with the final buckets. An example: bucket label Count Count (%) Non-event Event % Event % Non-event Event Rate WoE IV 0 (-inf, 25.0) 61.0 1.36 57.0 4.0 0.41 1.62 0.066 1.380 0.017 1 [25.0, 45.0) 2024.0 44.98 1536.0 488.0 49.64 43.67 0.241 -0.128 0.008 Parameters: Name Type Description Default column The column we wish to analyse required Returns: Type Description df (pd.DataFrame) A pandas dataframe of the format above fit ( self , X , y = None ) inherited \u00b6 Fit X, y. fit_interactive ( self , X , y = None , mode = 'external' , ** server_kwargs ) inherited \u00b6 Fit a bucketer and then interactive edit the fit using a dash app. Note we are using a jupyterdash app, which supports 3 different modes: 'external' (default): Start dash server and print URL 'inline': Start dash app inside an Iframe in the jupyter notebook 'jupyterlab': Start dash app as a new tab inside jupyterlab fit_transform ( self , X , y = None , ** fit_params ) inherited \u00b6 Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X . Parameters \u00b6 X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters. Returns \u00b6 X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params ( self , deep = True ) inherited \u00b6 Get parameters for this estimator. Parameters \u00b6 deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns \u00b6 params : dict Parameter names mapped to their values. plot_bucket ( self , column , line = 'event_rate' , format = None , scale = None , width = None , height = None ) inherited \u00b6 Plot the buckets. Parameters: Name Type Description Default column The column we want to visualise required line The line to plot on the secondary axis. Default is Event Rate. 'event_rate' format The format of the image, such as 'png'. The default None returns a plotly image. None scale If format is specified, the scale of the image None width If format is specified, the width of the image None height If format is specified, the image of the image None Returns: Type Description plot plotly fig predict ( self , X ) inherited \u00b6 Applies the transform method. To be used for the grid searches. Parameters: Name Type Description Default X pd.DataFrame The numerical data which will be transformed into the corresponding buckets required Returns: Type Description y (np.array) Transformed X, such that the values of X are replaced by the corresponding bucket numbers predict_proba ( self , X ) inherited \u00b6 Applies the transform method. To be used for the grid searches. Parameters: Name Type Description Default X pd.DataFrame The numerical data which will be transformed into the corresponding buckets required Returns: Type Description yhat (np.array) transformed X, such that the values of X are replaced by the corresponding bucket numbers save_yml ( self , fout ) inherited \u00b6 Save the features bucket to a yaml file. Parameters: Name Type Description Default fout ~PathLike file output required set_params ( self , ** params ) inherited \u00b6 Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class: ~sklearn.pipeline.Pipeline ). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters \u00b6 **params : dict Estimator parameters. Returns \u00b6 self : estimator instance Estimator instance. summary ( self ) inherited \u00b6 Display a summary table for columns passed to .fit() . The format is the following: column num_prebuckets num_buckets dtype LIMIT_BAL 15 10 float64 BILL_AMT1 15 6 float64 transform ( self , X , y = None ) inherited \u00b6 Transforms an array into the corresponding buckets fitted by the Transformer. Parameters: Name Type Description Default X pd.DataFrame dataframe which will be transformed into the corresponding buckets required y array target None Returns: Type Description df (pd.DataFrame) dataset with transformed features","title":"DecisionTreeBucketer"},{"location":"api/bucketers/DecisionTreeBucketer/#skorecard.bucketers.bucketers.DecisionTreeBucketer.variables_type","text":"Signals variables type supported by this bucketer.","title":"variables_type"},{"location":"api/bucketers/DecisionTreeBucketer/#skorecard.bucketers.bucketers.DecisionTreeBucketer.__init__","text":"Init the class. Parameters: Name Type Description Default variables list The features to bucket. Uses all features if not defined. [] specials dict dictionary of special values that require their own binning. The dictionary has the following format: {\" \" : {\"name of special bucket\" : }} For every feature that needs a special value, a dictionary must be passed as value. This dictionary contains a name of a bucket (key) and an array of unique values that should be put in that bucket. When special values are defined, they are not considered in the fitting procedure. {} min_bin_size float Minimum fraction of observations in a bucket. Passed directly to min_samples_leaf. 0.05 max_n_bins int Maximum numbers of after the bucketing. Passed directly to max_leaf_nodes of the DecisionTreeClassifier. If specials are defined, max_leaf_nodes will be redefined to max_n_bins - (number of special bins). The DecisionTreeClassifier requires max_leaf_nodes>=2: therefore, max_n_bins must always be >= (number of special bins + 2) if specials are defined, otherwise must be >=2. 100 missing_treatment str or dict Defines how we treat the missing values present in the data. If a string, it must be one of the following options: separate: Missing values get put in a separate 'Other' bucket: -1 most_risky: Missing values are put into the bucket containing the largest percentage of Class 1. least_risky: Missing values are put into the bucket containing the largest percentage of Class 0. most_frequent: Missing values are put into the most common bucket. neutral: Missing values are put into the bucket with WoE closest to 0. similar: Missing values are put into the bucket with WoE closest to the bucket with only missing values. passthrough: Leaves missing values untouched. If a dict, it must be of the following format: {\" \": } This bucket number is where we will put the missing values. 'separate' random_state int The random state, Passed directly to DecisionTreeClassifier None remainder str How we want the non-specified columns to be transformed. It must be in [\"passthrough\", \"drop\"]. passthrough (Default): all columns that were not specified in \"variables\" will be passed through. drop: all remaining columns that were not specified in \"variables\" will be dropped. 'passthrough' dt_kwargs Other parameters passed to DecisionTreeClassifier {'random_state': None}","title":"__init__()"},{"location":"api/bucketers/DecisionTreeBucketer/#skorecard.bucketers.bucketers.DecisionTreeBucketer.bucket_table","text":"Generates the statistics for the buckets of a particular column. The pre-buckets are matched to the post-buckets, so that the user has a much clearer understanding of how the BucketingProcess ends up with the final buckets. An example: bucket label Count Count (%) Non-event Event % Event % Non-event Event Rate WoE IV 0 (-inf, 25.0) 61.0 1.36 57.0 4.0 0.41 1.62 0.066 1.380 0.017 1 [25.0, 45.0) 2024.0 44.98 1536.0 488.0 49.64 43.67 0.241 -0.128 0.008 Parameters: Name Type Description Default column The column we wish to analyse required Returns: Type Description df (pd.DataFrame) A pandas dataframe of the format above","title":"bucket_table()"},{"location":"api/bucketers/DecisionTreeBucketer/#skorecard.bucketers.bucketers.DecisionTreeBucketer.fit","text":"Fit X, y.","title":"fit()"},{"location":"api/bucketers/DecisionTreeBucketer/#skorecard.bucketers.bucketers.DecisionTreeBucketer.fit_interactive","text":"Fit a bucketer and then interactive edit the fit using a dash app. Note we are using a jupyterdash app, which supports 3 different modes: 'external' (default): Start dash server and print URL 'inline': Start dash app inside an Iframe in the jupyter notebook 'jupyterlab': Start dash app as a new tab inside jupyterlab","title":"fit_interactive()"},{"location":"api/bucketers/DecisionTreeBucketer/#skorecard.bucketers.bucketers.DecisionTreeBucketer.fit_transform","text":"Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X .","title":"fit_transform()"},{"location":"api/bucketers/DecisionTreeBucketer/#skorecard.bucketers.bucketers.DecisionTreeBucketer.fit_transform--parameters","text":"X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters.","title":"Parameters"},{"location":"api/bucketers/DecisionTreeBucketer/#skorecard.bucketers.bucketers.DecisionTreeBucketer.fit_transform--returns","text":"X_new : ndarray array of shape (n_samples, n_features_new) Transformed array.","title":"Returns"},{"location":"api/bucketers/DecisionTreeBucketer/#skorecard.bucketers.bucketers.DecisionTreeBucketer.get_params","text":"Get parameters for this estimator.","title":"get_params()"},{"location":"api/bucketers/DecisionTreeBucketer/#skorecard.bucketers.bucketers.DecisionTreeBucketer.get_params--parameters","text":"deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators.","title":"Parameters"},{"location":"api/bucketers/DecisionTreeBucketer/#skorecard.bucketers.bucketers.DecisionTreeBucketer.get_params--returns","text":"params : dict Parameter names mapped to their values.","title":"Returns"},{"location":"api/bucketers/DecisionTreeBucketer/#skorecard.bucketers.bucketers.DecisionTreeBucketer.plot_bucket","text":"Plot the buckets. Parameters: Name Type Description Default column The column we want to visualise required line The line to plot on the secondary axis. Default is Event Rate. 'event_rate' format The format of the image, such as 'png'. The default None returns a plotly image. None scale If format is specified, the scale of the image None width If format is specified, the width of the image None height If format is specified, the image of the image None Returns: Type Description plot plotly fig","title":"plot_bucket()"},{"location":"api/bucketers/DecisionTreeBucketer/#skorecard.bucketers.bucketers.DecisionTreeBucketer.predict","text":"Applies the transform method. To be used for the grid searches. Parameters: Name Type Description Default X pd.DataFrame The numerical data which will be transformed into the corresponding buckets required Returns: Type Description y (np.array) Transformed X, such that the values of X are replaced by the corresponding bucket numbers","title":"predict()"},{"location":"api/bucketers/DecisionTreeBucketer/#skorecard.bucketers.bucketers.DecisionTreeBucketer.predict_proba","text":"Applies the transform method. To be used for the grid searches. Parameters: Name Type Description Default X pd.DataFrame The numerical data which will be transformed into the corresponding buckets required Returns: Type Description yhat (np.array) transformed X, such that the values of X are replaced by the corresponding bucket numbers","title":"predict_proba()"},{"location":"api/bucketers/DecisionTreeBucketer/#skorecard.bucketers.bucketers.DecisionTreeBucketer.save_yml","text":"Save the features bucket to a yaml file. Parameters: Name Type Description Default fout ~PathLike file output required","title":"save_yml()"},{"location":"api/bucketers/DecisionTreeBucketer/#skorecard.bucketers.bucketers.DecisionTreeBucketer.set_params","text":"Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class: ~sklearn.pipeline.Pipeline ). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object.","title":"set_params()"},{"location":"api/bucketers/DecisionTreeBucketer/#skorecard.bucketers.bucketers.DecisionTreeBucketer.set_params--parameters","text":"**params : dict Estimator parameters.","title":"Parameters"},{"location":"api/bucketers/DecisionTreeBucketer/#skorecard.bucketers.bucketers.DecisionTreeBucketer.set_params--returns","text":"self : estimator instance Estimator instance.","title":"Returns"},{"location":"api/bucketers/DecisionTreeBucketer/#skorecard.bucketers.bucketers.DecisionTreeBucketer.summary","text":"Display a summary table for columns passed to .fit() . The format is the following: column num_prebuckets num_buckets dtype LIMIT_BAL 15 10 float64 BILL_AMT1 15 6 float64","title":"summary()"},{"location":"api/bucketers/DecisionTreeBucketer/#skorecard.bucketers.bucketers.DecisionTreeBucketer.transform","text":"Transforms an array into the corresponding buckets fitted by the Transformer. Parameters: Name Type Description Default X pd.DataFrame dataframe which will be transformed into the corresponding buckets required y array target None Returns: Type Description df (pd.DataFrame) dataset with transformed features","title":"transform()"},{"location":"api/bucketers/EqualFrequencyBucketer/","text":"The EqualFrequencyBucketer transformer creates buckets with equal number of elements. Support: Examples: from skorecard import datasets from skorecard.bucketers import EqualFrequencyBucketer X , y = datasets . load_uci_credit_card ( return_X_y = True ) bucketer = EqualFrequencyBucketer ( n_bins = 10 , variables = [ 'LIMIT_BAL' ]) bucketer . fit_transform ( X ) bucketer . fit_transform ( X )[ 'LIMIT_BAL' ] . value_counts () variables_type property readonly \u00b6 Signals variables type supported by this bucketer. __init__ ( self , n_bins = 5 , variables = [], specials = {}, missing_treatment = 'separate' , remainder = 'passthrough' ) special \u00b6 Init the class. Parameters: Name Type Description Default n_bins int Number of bins to create. 5 variables list The features to bucket. Uses all features if not defined. [] specials (nested) dictionary of special values that require their own binning. The dictionary has the following format: {\" \" : {\"name of special bucket\" : }} For every feature that needs a special value, a dictionary must be passed as value. This dictionary contains a name of a bucket (key) and an array of unique values that should be put in that bucket. When special values are defined, they are not considered in the fitting procedure. {} missing_treatment Defines how we treat the missing values present in the data. If a string, it must be one of the following options: separate: Missing values get put in a separate 'Other' bucket: -1 most_risky: Missing values are put into the bucket containing the largest percentage of Class 1. least_risky: Missing values are put into the bucket containing the largest percentage of Class 0. most_frequent: Missing values are put into the most common bucket. neutral: Missing values are put into the bucket with WoE closest to 0. similar: Missing values are put into the bucket with WoE closest to the bucket with only missing values. passthrough: Leaves missing values untouched. If a dict, it must be of the following format: {\" \": } This bucket number is where we will put the missing values.. 'separate' remainder How we want the non-specified columns to be transformed. It must be in [\"passthrough\", \"drop\"]. passthrough (Default): all columns that were not specified in \"variables\" will be passed through. drop: all remaining columns that were not specified in \"variables\" will be dropped. 'passthrough' bucket_table ( self , column ) inherited \u00b6 Generates the statistics for the buckets of a particular column. The pre-buckets are matched to the post-buckets, so that the user has a much clearer understanding of how the BucketingProcess ends up with the final buckets. An example: bucket label Count Count (%) Non-event Event % Event % Non-event Event Rate WoE IV 0 (-inf, 25.0) 61.0 1.36 57.0 4.0 0.41 1.62 0.066 1.380 0.017 1 [25.0, 45.0) 2024.0 44.98 1536.0 488.0 49.64 43.67 0.241 -0.128 0.008 Parameters: Name Type Description Default column The column we wish to analyse required Returns: Type Description df (pd.DataFrame) A pandas dataframe of the format above fit ( self , X , y = None ) inherited \u00b6 Fit X, y. fit_interactive ( self , X , y = None , mode = 'external' , ** server_kwargs ) inherited \u00b6 Fit a bucketer and then interactive edit the fit using a dash app. Note we are using a jupyterdash app, which supports 3 different modes: 'external' (default): Start dash server and print URL 'inline': Start dash app inside an Iframe in the jupyter notebook 'jupyterlab': Start dash app as a new tab inside jupyterlab fit_transform ( self , X , y = None , ** fit_params ) inherited \u00b6 Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X . Parameters \u00b6 X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters. Returns \u00b6 X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params ( self , deep = True ) inherited \u00b6 Get parameters for this estimator. Parameters \u00b6 deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns \u00b6 params : dict Parameter names mapped to their values. plot_bucket ( self , column , line = 'event_rate' , format = None , scale = None , width = None , height = None ) inherited \u00b6 Plot the buckets. Parameters: Name Type Description Default column The column we want to visualise required line The line to plot on the secondary axis. Default is Event Rate. 'event_rate' format The format of the image, such as 'png'. The default None returns a plotly image. None scale If format is specified, the scale of the image None width If format is specified, the width of the image None height If format is specified, the image of the image None Returns: Type Description plot plotly fig predict ( self , X ) inherited \u00b6 Applies the transform method. To be used for the grid searches. Parameters: Name Type Description Default X pd.DataFrame The numerical data which will be transformed into the corresponding buckets required Returns: Type Description y (np.array) Transformed X, such that the values of X are replaced by the corresponding bucket numbers predict_proba ( self , X ) inherited \u00b6 Applies the transform method. To be used for the grid searches. Parameters: Name Type Description Default X pd.DataFrame The numerical data which will be transformed into the corresponding buckets required Returns: Type Description yhat (np.array) transformed X, such that the values of X are replaced by the corresponding bucket numbers save_yml ( self , fout ) inherited \u00b6 Save the features bucket to a yaml file. Parameters: Name Type Description Default fout ~PathLike file output required set_params ( self , ** params ) inherited \u00b6 Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class: ~sklearn.pipeline.Pipeline ). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters \u00b6 **params : dict Estimator parameters. Returns \u00b6 self : estimator instance Estimator instance. summary ( self ) inherited \u00b6 Display a summary table for columns passed to .fit() . The format is the following: column num_prebuckets num_buckets dtype LIMIT_BAL 15 10 float64 BILL_AMT1 15 6 float64 transform ( self , X , y = None ) inherited \u00b6 Transforms an array into the corresponding buckets fitted by the Transformer. Parameters: Name Type Description Default X pd.DataFrame dataframe which will be transformed into the corresponding buckets required y array target None Returns: Type Description df (pd.DataFrame) dataset with transformed features","title":"EqualFrequencyBucketer"},{"location":"api/bucketers/EqualFrequencyBucketer/#skorecard.bucketers.bucketers.EqualFrequencyBucketer.variables_type","text":"Signals variables type supported by this bucketer.","title":"variables_type"},{"location":"api/bucketers/EqualFrequencyBucketer/#skorecard.bucketers.bucketers.EqualFrequencyBucketer.__init__","text":"Init the class. Parameters: Name Type Description Default n_bins int Number of bins to create. 5 variables list The features to bucket. Uses all features if not defined. [] specials (nested) dictionary of special values that require their own binning. The dictionary has the following format: {\" \" : {\"name of special bucket\" : }} For every feature that needs a special value, a dictionary must be passed as value. This dictionary contains a name of a bucket (key) and an array of unique values that should be put in that bucket. When special values are defined, they are not considered in the fitting procedure. {} missing_treatment Defines how we treat the missing values present in the data. If a string, it must be one of the following options: separate: Missing values get put in a separate 'Other' bucket: -1 most_risky: Missing values are put into the bucket containing the largest percentage of Class 1. least_risky: Missing values are put into the bucket containing the largest percentage of Class 0. most_frequent: Missing values are put into the most common bucket. neutral: Missing values are put into the bucket with WoE closest to 0. similar: Missing values are put into the bucket with WoE closest to the bucket with only missing values. passthrough: Leaves missing values untouched. If a dict, it must be of the following format: {\" \": } This bucket number is where we will put the missing values.. 'separate' remainder How we want the non-specified columns to be transformed. It must be in [\"passthrough\", \"drop\"]. passthrough (Default): all columns that were not specified in \"variables\" will be passed through. drop: all remaining columns that were not specified in \"variables\" will be dropped. 'passthrough'","title":"__init__()"},{"location":"api/bucketers/EqualFrequencyBucketer/#skorecard.bucketers.bucketers.EqualFrequencyBucketer.bucket_table","text":"Generates the statistics for the buckets of a particular column. The pre-buckets are matched to the post-buckets, so that the user has a much clearer understanding of how the BucketingProcess ends up with the final buckets. An example: bucket label Count Count (%) Non-event Event % Event % Non-event Event Rate WoE IV 0 (-inf, 25.0) 61.0 1.36 57.0 4.0 0.41 1.62 0.066 1.380 0.017 1 [25.0, 45.0) 2024.0 44.98 1536.0 488.0 49.64 43.67 0.241 -0.128 0.008 Parameters: Name Type Description Default column The column we wish to analyse required Returns: Type Description df (pd.DataFrame) A pandas dataframe of the format above","title":"bucket_table()"},{"location":"api/bucketers/EqualFrequencyBucketer/#skorecard.bucketers.bucketers.EqualFrequencyBucketer.fit","text":"Fit X, y.","title":"fit()"},{"location":"api/bucketers/EqualFrequencyBucketer/#skorecard.bucketers.bucketers.EqualFrequencyBucketer.fit_interactive","text":"Fit a bucketer and then interactive edit the fit using a dash app. Note we are using a jupyterdash app, which supports 3 different modes: 'external' (default): Start dash server and print URL 'inline': Start dash app inside an Iframe in the jupyter notebook 'jupyterlab': Start dash app as a new tab inside jupyterlab","title":"fit_interactive()"},{"location":"api/bucketers/EqualFrequencyBucketer/#skorecard.bucketers.bucketers.EqualFrequencyBucketer.fit_transform","text":"Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X .","title":"fit_transform()"},{"location":"api/bucketers/EqualFrequencyBucketer/#skorecard.bucketers.bucketers.EqualFrequencyBucketer.fit_transform--parameters","text":"X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters.","title":"Parameters"},{"location":"api/bucketers/EqualFrequencyBucketer/#skorecard.bucketers.bucketers.EqualFrequencyBucketer.fit_transform--returns","text":"X_new : ndarray array of shape (n_samples, n_features_new) Transformed array.","title":"Returns"},{"location":"api/bucketers/EqualFrequencyBucketer/#skorecard.bucketers.bucketers.EqualFrequencyBucketer.get_params","text":"Get parameters for this estimator.","title":"get_params()"},{"location":"api/bucketers/EqualFrequencyBucketer/#skorecard.bucketers.bucketers.EqualFrequencyBucketer.get_params--parameters","text":"deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators.","title":"Parameters"},{"location":"api/bucketers/EqualFrequencyBucketer/#skorecard.bucketers.bucketers.EqualFrequencyBucketer.get_params--returns","text":"params : dict Parameter names mapped to their values.","title":"Returns"},{"location":"api/bucketers/EqualFrequencyBucketer/#skorecard.bucketers.bucketers.EqualFrequencyBucketer.plot_bucket","text":"Plot the buckets. Parameters: Name Type Description Default column The column we want to visualise required line The line to plot on the secondary axis. Default is Event Rate. 'event_rate' format The format of the image, such as 'png'. The default None returns a plotly image. None scale If format is specified, the scale of the image None width If format is specified, the width of the image None height If format is specified, the image of the image None Returns: Type Description plot plotly fig","title":"plot_bucket()"},{"location":"api/bucketers/EqualFrequencyBucketer/#skorecard.bucketers.bucketers.EqualFrequencyBucketer.predict","text":"Applies the transform method. To be used for the grid searches. Parameters: Name Type Description Default X pd.DataFrame The numerical data which will be transformed into the corresponding buckets required Returns: Type Description y (np.array) Transformed X, such that the values of X are replaced by the corresponding bucket numbers","title":"predict()"},{"location":"api/bucketers/EqualFrequencyBucketer/#skorecard.bucketers.bucketers.EqualFrequencyBucketer.predict_proba","text":"Applies the transform method. To be used for the grid searches. Parameters: Name Type Description Default X pd.DataFrame The numerical data which will be transformed into the corresponding buckets required Returns: Type Description yhat (np.array) transformed X, such that the values of X are replaced by the corresponding bucket numbers","title":"predict_proba()"},{"location":"api/bucketers/EqualFrequencyBucketer/#skorecard.bucketers.bucketers.EqualFrequencyBucketer.save_yml","text":"Save the features bucket to a yaml file. Parameters: Name Type Description Default fout ~PathLike file output required","title":"save_yml()"},{"location":"api/bucketers/EqualFrequencyBucketer/#skorecard.bucketers.bucketers.EqualFrequencyBucketer.set_params","text":"Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class: ~sklearn.pipeline.Pipeline ). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object.","title":"set_params()"},{"location":"api/bucketers/EqualFrequencyBucketer/#skorecard.bucketers.bucketers.EqualFrequencyBucketer.set_params--parameters","text":"**params : dict Estimator parameters.","title":"Parameters"},{"location":"api/bucketers/EqualFrequencyBucketer/#skorecard.bucketers.bucketers.EqualFrequencyBucketer.set_params--returns","text":"self : estimator instance Estimator instance.","title":"Returns"},{"location":"api/bucketers/EqualFrequencyBucketer/#skorecard.bucketers.bucketers.EqualFrequencyBucketer.summary","text":"Display a summary table for columns passed to .fit() . The format is the following: column num_prebuckets num_buckets dtype LIMIT_BAL 15 10 float64 BILL_AMT1 15 6 float64","title":"summary()"},{"location":"api/bucketers/EqualFrequencyBucketer/#skorecard.bucketers.bucketers.EqualFrequencyBucketer.transform","text":"Transforms an array into the corresponding buckets fitted by the Transformer. Parameters: Name Type Description Default X pd.DataFrame dataframe which will be transformed into the corresponding buckets required y array target None Returns: Type Description df (pd.DataFrame) dataset with transformed features","title":"transform()"},{"location":"api/bucketers/EqualWidthBucketer/","text":"The EqualWidthBucketer transformer creates equally spaced bins using numpy.histogram function. Support: Examples: from skorecard import datasets from skorecard.bucketers import EqualWidthBucketer specials = { \"LIMIT_BAL\" : { \"=50000\" : [ 50000 ], \"in [20001,30000]\" : [ 20000 , 30000 ]}} X , y = datasets . load_uci_credit_card ( return_X_y = True ) bucketer = EqualWidthBucketer ( n_bins = 10 , variables = [ 'LIMIT_BAL' ], specials = specials ) bucketer . fit_transform ( X ) bucketer . fit_transform ( X )[ 'LIMIT_BAL' ] . value_counts () variables_type property readonly \u00b6 Signals variables type supported by this bucketer. __init__ ( self , n_bins = 5 , variables = [], specials = {}, missing_treatment = 'separate' , remainder = 'passthrough' ) special \u00b6 Init the class. Parameters: Name Type Description Default n_bins int Number of bins to create. 5 variables list The features to bucket. Uses all features if not defined. [] specials (dict) of special values that require their own binning. The dictionary has the following format: {\" \" : {\"name of special bucket\" : }} For every feature that needs a special value, a dictionary must be passed as value. This dictionary contains a name of a bucket (key) and an array of unique values that should be put in that bucket. When special values are defined, they are not considered in the fitting procedure. {} missing_treatment Defines how we treat the missing values present in the data. If a string, it must be one of the following options: separate: Missing values get put in a separate 'Other' bucket: -1 most_risky: Missing values are put into the bucket containing the largest percentage of Class 1. least_risky: Missing values are put into the bucket containing the largest percentage of Class 0. most_frequent: Missing values are put into the most common bucket. neutral: Missing values are put into the bucket with WoE closest to 0. similar: Missing values are put into the bucket with WoE closest to the bucket with only missing values. passthrough: Leaves missing values untouched. If a dict, it must be of the following format: {\" \": } This bucket number is where we will put the missing values. 'separate' remainder How we want the non-specified columns to be transformed. It must be in [\"passthrough\", \"drop\"]. passthrough (Default): all columns that were not specified in \"variables\" will be passed through. drop: all remaining columns that were not specified in \"variables\" will be dropped. 'passthrough' bucket_table ( self , column ) inherited \u00b6 Generates the statistics for the buckets of a particular column. The pre-buckets are matched to the post-buckets, so that the user has a much clearer understanding of how the BucketingProcess ends up with the final buckets. An example: bucket label Count Count (%) Non-event Event % Event % Non-event Event Rate WoE IV 0 (-inf, 25.0) 61.0 1.36 57.0 4.0 0.41 1.62 0.066 1.380 0.017 1 [25.0, 45.0) 2024.0 44.98 1536.0 488.0 49.64 43.67 0.241 -0.128 0.008 Parameters: Name Type Description Default column The column we wish to analyse required Returns: Type Description df (pd.DataFrame) A pandas dataframe of the format above fit ( self , X , y = None ) inherited \u00b6 Fit X, y. fit_interactive ( self , X , y = None , mode = 'external' , ** server_kwargs ) inherited \u00b6 Fit a bucketer and then interactive edit the fit using a dash app. Note we are using a jupyterdash app, which supports 3 different modes: 'external' (default): Start dash server and print URL 'inline': Start dash app inside an Iframe in the jupyter notebook 'jupyterlab': Start dash app as a new tab inside jupyterlab fit_transform ( self , X , y = None , ** fit_params ) inherited \u00b6 Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X . Parameters \u00b6 X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters. Returns \u00b6 X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params ( self , deep = True ) inherited \u00b6 Get parameters for this estimator. Parameters \u00b6 deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns \u00b6 params : dict Parameter names mapped to their values. plot_bucket ( self , column , line = 'event_rate' , format = None , scale = None , width = None , height = None ) inherited \u00b6 Plot the buckets. Parameters: Name Type Description Default column The column we want to visualise required line The line to plot on the secondary axis. Default is Event Rate. 'event_rate' format The format of the image, such as 'png'. The default None returns a plotly image. None scale If format is specified, the scale of the image None width If format is specified, the width of the image None height If format is specified, the image of the image None Returns: Type Description plot plotly fig predict ( self , X ) inherited \u00b6 Applies the transform method. To be used for the grid searches. Parameters: Name Type Description Default X pd.DataFrame The numerical data which will be transformed into the corresponding buckets required Returns: Type Description y (np.array) Transformed X, such that the values of X are replaced by the corresponding bucket numbers predict_proba ( self , X ) inherited \u00b6 Applies the transform method. To be used for the grid searches. Parameters: Name Type Description Default X pd.DataFrame The numerical data which will be transformed into the corresponding buckets required Returns: Type Description yhat (np.array) transformed X, such that the values of X are replaced by the corresponding bucket numbers save_yml ( self , fout ) inherited \u00b6 Save the features bucket to a yaml file. Parameters: Name Type Description Default fout ~PathLike file output required set_params ( self , ** params ) inherited \u00b6 Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class: ~sklearn.pipeline.Pipeline ). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters \u00b6 **params : dict Estimator parameters. Returns \u00b6 self : estimator instance Estimator instance. summary ( self ) inherited \u00b6 Display a summary table for columns passed to .fit() . The format is the following: column num_prebuckets num_buckets dtype LIMIT_BAL 15 10 float64 BILL_AMT1 15 6 float64 transform ( self , X , y = None ) inherited \u00b6 Transforms an array into the corresponding buckets fitted by the Transformer. Parameters: Name Type Description Default X pd.DataFrame dataframe which will be transformed into the corresponding buckets required y array target None Returns: Type Description df (pd.DataFrame) dataset with transformed features","title":"EqualWidthBucketer"},{"location":"api/bucketers/EqualWidthBucketer/#skorecard.bucketers.bucketers.EqualWidthBucketer.variables_type","text":"Signals variables type supported by this bucketer.","title":"variables_type"},{"location":"api/bucketers/EqualWidthBucketer/#skorecard.bucketers.bucketers.EqualWidthBucketer.__init__","text":"Init the class. Parameters: Name Type Description Default n_bins int Number of bins to create. 5 variables list The features to bucket. Uses all features if not defined. [] specials (dict) of special values that require their own binning. The dictionary has the following format: {\" \" : {\"name of special bucket\" : }} For every feature that needs a special value, a dictionary must be passed as value. This dictionary contains a name of a bucket (key) and an array of unique values that should be put in that bucket. When special values are defined, they are not considered in the fitting procedure. {} missing_treatment Defines how we treat the missing values present in the data. If a string, it must be one of the following options: separate: Missing values get put in a separate 'Other' bucket: -1 most_risky: Missing values are put into the bucket containing the largest percentage of Class 1. least_risky: Missing values are put into the bucket containing the largest percentage of Class 0. most_frequent: Missing values are put into the most common bucket. neutral: Missing values are put into the bucket with WoE closest to 0. similar: Missing values are put into the bucket with WoE closest to the bucket with only missing values. passthrough: Leaves missing values untouched. If a dict, it must be of the following format: {\" \": } This bucket number is where we will put the missing values. 'separate' remainder How we want the non-specified columns to be transformed. It must be in [\"passthrough\", \"drop\"]. passthrough (Default): all columns that were not specified in \"variables\" will be passed through. drop: all remaining columns that were not specified in \"variables\" will be dropped. 'passthrough'","title":"__init__()"},{"location":"api/bucketers/EqualWidthBucketer/#skorecard.bucketers.bucketers.EqualWidthBucketer.bucket_table","text":"Generates the statistics for the buckets of a particular column. The pre-buckets are matched to the post-buckets, so that the user has a much clearer understanding of how the BucketingProcess ends up with the final buckets. An example: bucket label Count Count (%) Non-event Event % Event % Non-event Event Rate WoE IV 0 (-inf, 25.0) 61.0 1.36 57.0 4.0 0.41 1.62 0.066 1.380 0.017 1 [25.0, 45.0) 2024.0 44.98 1536.0 488.0 49.64 43.67 0.241 -0.128 0.008 Parameters: Name Type Description Default column The column we wish to analyse required Returns: Type Description df (pd.DataFrame) A pandas dataframe of the format above","title":"bucket_table()"},{"location":"api/bucketers/EqualWidthBucketer/#skorecard.bucketers.bucketers.EqualWidthBucketer.fit","text":"Fit X, y.","title":"fit()"},{"location":"api/bucketers/EqualWidthBucketer/#skorecard.bucketers.bucketers.EqualWidthBucketer.fit_interactive","text":"Fit a bucketer and then interactive edit the fit using a dash app. Note we are using a jupyterdash app, which supports 3 different modes: 'external' (default): Start dash server and print URL 'inline': Start dash app inside an Iframe in the jupyter notebook 'jupyterlab': Start dash app as a new tab inside jupyterlab","title":"fit_interactive()"},{"location":"api/bucketers/EqualWidthBucketer/#skorecard.bucketers.bucketers.EqualWidthBucketer.fit_transform","text":"Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X .","title":"fit_transform()"},{"location":"api/bucketers/EqualWidthBucketer/#skorecard.bucketers.bucketers.EqualWidthBucketer.fit_transform--parameters","text":"X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters.","title":"Parameters"},{"location":"api/bucketers/EqualWidthBucketer/#skorecard.bucketers.bucketers.EqualWidthBucketer.fit_transform--returns","text":"X_new : ndarray array of shape (n_samples, n_features_new) Transformed array.","title":"Returns"},{"location":"api/bucketers/EqualWidthBucketer/#skorecard.bucketers.bucketers.EqualWidthBucketer.get_params","text":"Get parameters for this estimator.","title":"get_params()"},{"location":"api/bucketers/EqualWidthBucketer/#skorecard.bucketers.bucketers.EqualWidthBucketer.get_params--parameters","text":"deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators.","title":"Parameters"},{"location":"api/bucketers/EqualWidthBucketer/#skorecard.bucketers.bucketers.EqualWidthBucketer.get_params--returns","text":"params : dict Parameter names mapped to their values.","title":"Returns"},{"location":"api/bucketers/EqualWidthBucketer/#skorecard.bucketers.bucketers.EqualWidthBucketer.plot_bucket","text":"Plot the buckets. Parameters: Name Type Description Default column The column we want to visualise required line The line to plot on the secondary axis. Default is Event Rate. 'event_rate' format The format of the image, such as 'png'. The default None returns a plotly image. None scale If format is specified, the scale of the image None width If format is specified, the width of the image None height If format is specified, the image of the image None Returns: Type Description plot plotly fig","title":"plot_bucket()"},{"location":"api/bucketers/EqualWidthBucketer/#skorecard.bucketers.bucketers.EqualWidthBucketer.predict","text":"Applies the transform method. To be used for the grid searches. Parameters: Name Type Description Default X pd.DataFrame The numerical data which will be transformed into the corresponding buckets required Returns: Type Description y (np.array) Transformed X, such that the values of X are replaced by the corresponding bucket numbers","title":"predict()"},{"location":"api/bucketers/EqualWidthBucketer/#skorecard.bucketers.bucketers.EqualWidthBucketer.predict_proba","text":"Applies the transform method. To be used for the grid searches. Parameters: Name Type Description Default X pd.DataFrame The numerical data which will be transformed into the corresponding buckets required Returns: Type Description yhat (np.array) transformed X, such that the values of X are replaced by the corresponding bucket numbers","title":"predict_proba()"},{"location":"api/bucketers/EqualWidthBucketer/#skorecard.bucketers.bucketers.EqualWidthBucketer.save_yml","text":"Save the features bucket to a yaml file. Parameters: Name Type Description Default fout ~PathLike file output required","title":"save_yml()"},{"location":"api/bucketers/EqualWidthBucketer/#skorecard.bucketers.bucketers.EqualWidthBucketer.set_params","text":"Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class: ~sklearn.pipeline.Pipeline ). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object.","title":"set_params()"},{"location":"api/bucketers/EqualWidthBucketer/#skorecard.bucketers.bucketers.EqualWidthBucketer.set_params--parameters","text":"**params : dict Estimator parameters.","title":"Parameters"},{"location":"api/bucketers/EqualWidthBucketer/#skorecard.bucketers.bucketers.EqualWidthBucketer.set_params--returns","text":"self : estimator instance Estimator instance.","title":"Returns"},{"location":"api/bucketers/EqualWidthBucketer/#skorecard.bucketers.bucketers.EqualWidthBucketer.summary","text":"Display a summary table for columns passed to .fit() . The format is the following: column num_prebuckets num_buckets dtype LIMIT_BAL 15 10 float64 BILL_AMT1 15 6 float64","title":"summary()"},{"location":"api/bucketers/EqualWidthBucketer/#skorecard.bucketers.bucketers.EqualWidthBucketer.transform","text":"Transforms an array into the corresponding buckets fitted by the Transformer. Parameters: Name Type Description Default X pd.DataFrame dataframe which will be transformed into the corresponding buckets required y array target None Returns: Type Description df (pd.DataFrame) dataset with transformed features","title":"transform()"},{"location":"api/bucketers/OptimalBucketer/","text":"The OptimalBucketer transformer uses the optbinning package to find optimal buckets. Support: This bucketer basically wraps optbinning.OptimalBinning to be consistent with skorecard. Requires a feature to be pre-bucketed to max 100 buckets. Optbinning uses a constrained programming solver to merge buckets, taking into account the following constraints 1) monotonicity in bad rate, 2) at least 5% of records per bin. Examples: from skorecard import datasets from skorecard.bucketers import OptimalBucketer X , y = datasets . load_uci_credit_card ( return_X_y = True ) bucketer = OptimalBucketer ( variables = [ 'LIMIT_BAL' ]) bucketer . fit_transform ( X , y ) __init__ ( self , variables = [], specials = {}, variables_type = 'numerical' , max_n_bins = 10 , missing_treatment = 'separate' , min_bin_size = 0.05 , cat_cutoff = None , time_limit = 25 , remainder = 'passthrough' , solver = 'cp' , monotonic_trend = 'auto_asc_desc' , gamma = 0 , ob_kwargs = {}) special \u00b6 Initialize Optimal Bucketer. Parameters: Name Type Description Default variables List of variables to bucket. [] specials (nested) dictionary of special values that require their own binning. The dictionary has the following format: {\" \" : {\"name of special bucket\" : }} For every feature that needs a special value, a dictionary must be passed as value. This dictionary contains a name of a bucket (key) and an array of unique values that should be put in that bucket. When special values are passed, they are not considered in the fitting procedure. {} variables_type Passed to optbinning.OptimalBinning : Type of the variables. Must be either 'categorical' or 'numerical'. 'numerical' missing_treatment Defines how we treat the missing values present in the data. If a string, it must be one of the following options: separate: Missing values get put in a separate 'Other' bucket: -1 most_risky: Missing values are put into the bucket containing the largest percentage of Class 1. least_risky: Missing values are put into the bucket containing the largest percentage of Class 0. most_frequent: Missing values are put into the most common bucket. neutral: Missing values are put into the bucket with WoE closest to 0. similar: Missing values are put into the bucket with WoE closest to the bucket with only missing values. passthrough: Leaves missing values untouched. If a dict, it must be of the following format: {\" \": } This bucket number is where we will put the missing values. 'separate' min_bin_size Passed to optbinning.OptimalBinning : Minimum fraction of observations in a bucket. 0.05 max_n_bins Passed to optbinning.OptimalBinning : Maximum numbers of bins to return. 10 cat_cutoff Passed to optbinning.OptimalBinning : Threshold ratio (None, or >0 and <=1) below which categories are grouped together in a bucket 'other'. None time_limit float Passed to optbinning.OptimalBinning : Time limit in seconds to find an optimal solution. 25 remainder How we want the non-specified columns to be transformed. It must be in [\"passthrough\", \"drop\"]. passthrough (Default): all columns that were not specified in \"variables\" will be passed through. drop: all remaining columns that were not specified in \"variables\" will be dropped. 'passthrough' solver str Passed to optbinning.OptimalBinning : The optimizer to solve the optimal binning problem. Supported solvers are \u201cmip\u201d to choose a mixed-integer programming solver, \u201ccp\u201d (default) to choose a constrained programming solver or \u201cls\u201d to choose LocalSolver. 'cp' monotonic_trend str Passed to optbinning.OptimalBinning : The event rate monotonic trend. Supported trends are \u201cauto\u201d, \u201cauto_heuristic\u201d and \u201cauto_asc_desc\u201d to automatically determine the trend maximizing IV using a machine learning classifier, \u201cascending\u201d, \u201cdescending\u201d, \u201cconcave\u201d, \u201cconvex\u201d, \u201cpeak\u201d and \u201cpeak_heuristic\u201d to allow a peak change point, and \u201cvalley\u201d and \u201cvalley_heuristic\u201d to allow a valley change point. Trends \u201cauto_heuristic\u201d, \u201cpeak_heuristic\u201d and \u201cvalley_heuristic\u201d use a heuristic to determine the change point, and are significantly faster for large size instances (max_n_prebins > 20). Trend \u201cauto_asc_desc\u201d is used to automatically select the best monotonic trend between \u201cascending\u201d and \u201cdescending\u201d. If None, then the monotonic constraint is disabled. 'auto_asc_desc' gamma float Passed to optbinning.OptimalBinning : Regularization strength to reduce the number of dominating bins. Larger values specify stronger regularization. Default is 0. Option supported by solvers \u201ccp\u201d and \u201cmip\u201d. 0 ob_kwargs dict Other parameters passed to optbinning.OptimalBinning . {} bucket_table ( self , column ) inherited \u00b6 Generates the statistics for the buckets of a particular column. The pre-buckets are matched to the post-buckets, so that the user has a much clearer understanding of how the BucketingProcess ends up with the final buckets. An example: bucket label Count Count (%) Non-event Event % Event % Non-event Event Rate WoE IV 0 (-inf, 25.0) 61.0 1.36 57.0 4.0 0.41 1.62 0.066 1.380 0.017 1 [25.0, 45.0) 2024.0 44.98 1536.0 488.0 49.64 43.67 0.241 -0.128 0.008 Parameters: Name Type Description Default column The column we wish to analyse required Returns: Type Description df (pd.DataFrame) A pandas dataframe of the format above fit ( self , X , y = None ) inherited \u00b6 Fit X, y. fit_interactive ( self , X , y = None , mode = 'external' , ** server_kwargs ) inherited \u00b6 Fit a bucketer and then interactive edit the fit using a dash app. Note we are using a jupyterdash app, which supports 3 different modes: 'external' (default): Start dash server and print URL 'inline': Start dash app inside an Iframe in the jupyter notebook 'jupyterlab': Start dash app as a new tab inside jupyterlab fit_transform ( self , X , y = None , ** fit_params ) inherited \u00b6 Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X . Parameters \u00b6 X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters. Returns \u00b6 X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params ( self , deep = True ) inherited \u00b6 Get parameters for this estimator. Parameters \u00b6 deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns \u00b6 params : dict Parameter names mapped to their values. plot_bucket ( self , column , line = 'event_rate' , format = None , scale = None , width = None , height = None ) inherited \u00b6 Plot the buckets. Parameters: Name Type Description Default column The column we want to visualise required line The line to plot on the secondary axis. Default is Event Rate. 'event_rate' format The format of the image, such as 'png'. The default None returns a plotly image. None scale If format is specified, the scale of the image None width If format is specified, the width of the image None height If format is specified, the image of the image None Returns: Type Description plot plotly fig predict ( self , X ) inherited \u00b6 Applies the transform method. To be used for the grid searches. Parameters: Name Type Description Default X pd.DataFrame The numerical data which will be transformed into the corresponding buckets required Returns: Type Description y (np.array) Transformed X, such that the values of X are replaced by the corresponding bucket numbers predict_proba ( self , X ) inherited \u00b6 Applies the transform method. To be used for the grid searches. Parameters: Name Type Description Default X pd.DataFrame The numerical data which will be transformed into the corresponding buckets required Returns: Type Description yhat (np.array) transformed X, such that the values of X are replaced by the corresponding bucket numbers save_yml ( self , fout ) inherited \u00b6 Save the features bucket to a yaml file. Parameters: Name Type Description Default fout ~PathLike file output required set_params ( self , ** params ) inherited \u00b6 Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class: ~sklearn.pipeline.Pipeline ). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters \u00b6 **params : dict Estimator parameters. Returns \u00b6 self : estimator instance Estimator instance. summary ( self ) inherited \u00b6 Display a summary table for columns passed to .fit() . The format is the following: column num_prebuckets num_buckets dtype LIMIT_BAL 15 10 float64 BILL_AMT1 15 6 float64 transform ( self , X , y = None ) inherited \u00b6 Transforms an array into the corresponding buckets fitted by the Transformer. Parameters: Name Type Description Default X pd.DataFrame dataframe which will be transformed into the corresponding buckets required y array target None Returns: Type Description df (pd.DataFrame) dataset with transformed features","title":"OptimalBucketer"},{"location":"api/bucketers/OptimalBucketer/#skorecard.bucketers.bucketers.OptimalBucketer.__init__","text":"Initialize Optimal Bucketer. Parameters: Name Type Description Default variables List of variables to bucket. [] specials (nested) dictionary of special values that require their own binning. The dictionary has the following format: {\" \" : {\"name of special bucket\" : }} For every feature that needs a special value, a dictionary must be passed as value. This dictionary contains a name of a bucket (key) and an array of unique values that should be put in that bucket. When special values are passed, they are not considered in the fitting procedure. {} variables_type Passed to optbinning.OptimalBinning : Type of the variables. Must be either 'categorical' or 'numerical'. 'numerical' missing_treatment Defines how we treat the missing values present in the data. If a string, it must be one of the following options: separate: Missing values get put in a separate 'Other' bucket: -1 most_risky: Missing values are put into the bucket containing the largest percentage of Class 1. least_risky: Missing values are put into the bucket containing the largest percentage of Class 0. most_frequent: Missing values are put into the most common bucket. neutral: Missing values are put into the bucket with WoE closest to 0. similar: Missing values are put into the bucket with WoE closest to the bucket with only missing values. passthrough: Leaves missing values untouched. If a dict, it must be of the following format: {\" \": } This bucket number is where we will put the missing values. 'separate' min_bin_size Passed to optbinning.OptimalBinning : Minimum fraction of observations in a bucket. 0.05 max_n_bins Passed to optbinning.OptimalBinning : Maximum numbers of bins to return. 10 cat_cutoff Passed to optbinning.OptimalBinning : Threshold ratio (None, or >0 and <=1) below which categories are grouped together in a bucket 'other'. None time_limit float Passed to optbinning.OptimalBinning : Time limit in seconds to find an optimal solution. 25 remainder How we want the non-specified columns to be transformed. It must be in [\"passthrough\", \"drop\"]. passthrough (Default): all columns that were not specified in \"variables\" will be passed through. drop: all remaining columns that were not specified in \"variables\" will be dropped. 'passthrough' solver str Passed to optbinning.OptimalBinning : The optimizer to solve the optimal binning problem. Supported solvers are \u201cmip\u201d to choose a mixed-integer programming solver, \u201ccp\u201d (default) to choose a constrained programming solver or \u201cls\u201d to choose LocalSolver. 'cp' monotonic_trend str Passed to optbinning.OptimalBinning : The event rate monotonic trend. Supported trends are \u201cauto\u201d, \u201cauto_heuristic\u201d and \u201cauto_asc_desc\u201d to automatically determine the trend maximizing IV using a machine learning classifier, \u201cascending\u201d, \u201cdescending\u201d, \u201cconcave\u201d, \u201cconvex\u201d, \u201cpeak\u201d and \u201cpeak_heuristic\u201d to allow a peak change point, and \u201cvalley\u201d and \u201cvalley_heuristic\u201d to allow a valley change point. Trends \u201cauto_heuristic\u201d, \u201cpeak_heuristic\u201d and \u201cvalley_heuristic\u201d use a heuristic to determine the change point, and are significantly faster for large size instances (max_n_prebins > 20). Trend \u201cauto_asc_desc\u201d is used to automatically select the best monotonic trend between \u201cascending\u201d and \u201cdescending\u201d. If None, then the monotonic constraint is disabled. 'auto_asc_desc' gamma float Passed to optbinning.OptimalBinning : Regularization strength to reduce the number of dominating bins. Larger values specify stronger regularization. Default is 0. Option supported by solvers \u201ccp\u201d and \u201cmip\u201d. 0 ob_kwargs dict Other parameters passed to optbinning.OptimalBinning . {}","title":"__init__()"},{"location":"api/bucketers/OptimalBucketer/#skorecard.bucketers.bucketers.OptimalBucketer.bucket_table","text":"Generates the statistics for the buckets of a particular column. The pre-buckets are matched to the post-buckets, so that the user has a much clearer understanding of how the BucketingProcess ends up with the final buckets. An example: bucket label Count Count (%) Non-event Event % Event % Non-event Event Rate WoE IV 0 (-inf, 25.0) 61.0 1.36 57.0 4.0 0.41 1.62 0.066 1.380 0.017 1 [25.0, 45.0) 2024.0 44.98 1536.0 488.0 49.64 43.67 0.241 -0.128 0.008 Parameters: Name Type Description Default column The column we wish to analyse required Returns: Type Description df (pd.DataFrame) A pandas dataframe of the format above","title":"bucket_table()"},{"location":"api/bucketers/OptimalBucketer/#skorecard.bucketers.bucketers.OptimalBucketer.fit","text":"Fit X, y.","title":"fit()"},{"location":"api/bucketers/OptimalBucketer/#skorecard.bucketers.bucketers.OptimalBucketer.fit_interactive","text":"Fit a bucketer and then interactive edit the fit using a dash app. Note we are using a jupyterdash app, which supports 3 different modes: 'external' (default): Start dash server and print URL 'inline': Start dash app inside an Iframe in the jupyter notebook 'jupyterlab': Start dash app as a new tab inside jupyterlab","title":"fit_interactive()"},{"location":"api/bucketers/OptimalBucketer/#skorecard.bucketers.bucketers.OptimalBucketer.fit_transform","text":"Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X .","title":"fit_transform()"},{"location":"api/bucketers/OptimalBucketer/#skorecard.bucketers.bucketers.OptimalBucketer.fit_transform--parameters","text":"X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters.","title":"Parameters"},{"location":"api/bucketers/OptimalBucketer/#skorecard.bucketers.bucketers.OptimalBucketer.fit_transform--returns","text":"X_new : ndarray array of shape (n_samples, n_features_new) Transformed array.","title":"Returns"},{"location":"api/bucketers/OptimalBucketer/#skorecard.bucketers.bucketers.OptimalBucketer.get_params","text":"Get parameters for this estimator.","title":"get_params()"},{"location":"api/bucketers/OptimalBucketer/#skorecard.bucketers.bucketers.OptimalBucketer.get_params--parameters","text":"deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators.","title":"Parameters"},{"location":"api/bucketers/OptimalBucketer/#skorecard.bucketers.bucketers.OptimalBucketer.get_params--returns","text":"params : dict Parameter names mapped to their values.","title":"Returns"},{"location":"api/bucketers/OptimalBucketer/#skorecard.bucketers.bucketers.OptimalBucketer.plot_bucket","text":"Plot the buckets. Parameters: Name Type Description Default column The column we want to visualise required line The line to plot on the secondary axis. Default is Event Rate. 'event_rate' format The format of the image, such as 'png'. The default None returns a plotly image. None scale If format is specified, the scale of the image None width If format is specified, the width of the image None height If format is specified, the image of the image None Returns: Type Description plot plotly fig","title":"plot_bucket()"},{"location":"api/bucketers/OptimalBucketer/#skorecard.bucketers.bucketers.OptimalBucketer.predict","text":"Applies the transform method. To be used for the grid searches. Parameters: Name Type Description Default X pd.DataFrame The numerical data which will be transformed into the corresponding buckets required Returns: Type Description y (np.array) Transformed X, such that the values of X are replaced by the corresponding bucket numbers","title":"predict()"},{"location":"api/bucketers/OptimalBucketer/#skorecard.bucketers.bucketers.OptimalBucketer.predict_proba","text":"Applies the transform method. To be used for the grid searches. Parameters: Name Type Description Default X pd.DataFrame The numerical data which will be transformed into the corresponding buckets required Returns: Type Description yhat (np.array) transformed X, such that the values of X are replaced by the corresponding bucket numbers","title":"predict_proba()"},{"location":"api/bucketers/OptimalBucketer/#skorecard.bucketers.bucketers.OptimalBucketer.save_yml","text":"Save the features bucket to a yaml file. Parameters: Name Type Description Default fout ~PathLike file output required","title":"save_yml()"},{"location":"api/bucketers/OptimalBucketer/#skorecard.bucketers.bucketers.OptimalBucketer.set_params","text":"Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class: ~sklearn.pipeline.Pipeline ). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object.","title":"set_params()"},{"location":"api/bucketers/OptimalBucketer/#skorecard.bucketers.bucketers.OptimalBucketer.set_params--parameters","text":"**params : dict Estimator parameters.","title":"Parameters"},{"location":"api/bucketers/OptimalBucketer/#skorecard.bucketers.bucketers.OptimalBucketer.set_params--returns","text":"self : estimator instance Estimator instance.","title":"Returns"},{"location":"api/bucketers/OptimalBucketer/#skorecard.bucketers.bucketers.OptimalBucketer.summary","text":"Display a summary table for columns passed to .fit() . The format is the following: column num_prebuckets num_buckets dtype LIMIT_BAL 15 10 float64 BILL_AMT1 15 6 float64","title":"summary()"},{"location":"api/bucketers/OptimalBucketer/#skorecard.bucketers.bucketers.OptimalBucketer.transform","text":"Transforms an array into the corresponding buckets fitted by the Transformer. Parameters: Name Type Description Default X pd.DataFrame dataframe which will be transformed into the corresponding buckets required y array target None Returns: Type Description df (pd.DataFrame) dataset with transformed features","title":"transform()"},{"location":"api/bucketers/OrdinalCategoricalBucketer/","text":"The OrdinalCategoricalBucketer replaces categories by ordinal numbers. Support When sort_by_target is false the buckets are assigned in order of frequency. When sort_by_target is true the buckets are ordered based on the mean of the target per category. For example, if for a variable colour the means of the target for blue , red and grey is 0.5 , 0.8 and 0.1 respectively, grey will be the first bucket ( 0 ), blue the second ( 1 ) and red the third ( 3 ). If new data contains unknown labels (f.e. yellow), they will be replaced by the 'Other' bucket ( -2 ), and if new data contains missing values, they will be replaced by the 'Missing' bucket ( -1 ). Examples: from skorecard import datasets from skorecard.bucketers import OrdinalCategoricalBucketer X , y = datasets . load_uci_credit_card ( return_X_y = True ) bucketer = OrdinalCategoricalBucketer ( variables = [ 'EDUCATION' ]) bucketer . fit_transform ( X , y ) bucketer = OrdinalCategoricalBucketer ( max_n_categories = 2 , variables = [ 'EDUCATION' ]) bucketer . fit_transform ( X , y ) Credits: Code & ideas adapted from: feature_engine.categorical_encoders.OrdinalCategoricalEncoder feature_engine.categorical_encoders.RareLabelCategoricalEncoder variables_type property readonly \u00b6 Signals variables type supported by this bucketer. __init__ ( self , tol = 0.05 , max_n_categories = None , variables = [], specials = {}, encoding_method = 'frequency' , missing_treatment = 'separate' , remainder = 'passthrough' ) special \u00b6 Init the class. Parameters: Name Type Description Default tol float the minimum frequency a label should have to be considered frequent. Categories with frequencies lower than tol will be grouped together (in the 'other' bucket). 0.05 max_n_categories int the maximum number of categories that should be considered frequent. If None, all categories with frequency above the tolerance (tol) will be considered. None variables list The features to bucket. Uses all features if not defined. [] specials dict (nested) dictionary of special values that require their own binning. The dictionary has the following format: {\" \" : {\"name of special bucket\" : }} For every feature that needs a special value, a dictionary must be passed as value. This dictionary contains a name of a bucket (key) and an array of unique values that should be put in that bucket. When special values are defined, they are not considered in the fitting procedure. {} encoding_method string encoding method. - \"frequency\" (default): orders the buckets based on the frequency of observations in the bucket. The lower the number of the bucket the most frequent are the observations in that bucket. - \"ordered\": orders the buckets based on the average class 1 rate in the bucket. The lower the number of the bucket the lower the fraction of class 1 in that bucket. 'frequency' missing_treatment str or dict Defines how we treat the missing values present in the data. If a string, it must be one of the following options: separate: Missing values get put in a separate 'Other' bucket: -1 most_risky: Missing values are put into the bucket containing the largest percentage of Class 1. least_risky: Missing values are put into the bucket containing the largest percentage of Class 0. most_frequent: Missing values are put into the most common bucket. neutral: Missing values are put into the bucket with WoE closest to 0. similar: Missing values are put into the bucket with WoE closest to the bucket with only missing values. passthrough: Leaves missing values untouched. If a dict, it must be of the following format: {\" \": } This bucket number is where we will put the missing values. 'separate' remainder str How we want the non-specified columns to be transformed. It must be in [\"passthrough\", \"drop\"]. passthrough (Default): all columns that were not specified in \"variables\" will be passed through. drop: all remaining columns that were not specified in \"variables\" will be dropped. 'passthrough' bucket_table ( self , column ) inherited \u00b6 Generates the statistics for the buckets of a particular column. The pre-buckets are matched to the post-buckets, so that the user has a much clearer understanding of how the BucketingProcess ends up with the final buckets. An example: bucket label Count Count (%) Non-event Event % Event % Non-event Event Rate WoE IV 0 (-inf, 25.0) 61.0 1.36 57.0 4.0 0.41 1.62 0.066 1.380 0.017 1 [25.0, 45.0) 2024.0 44.98 1536.0 488.0 49.64 43.67 0.241 -0.128 0.008 Parameters: Name Type Description Default column The column we wish to analyse required Returns: Type Description df (pd.DataFrame) A pandas dataframe of the format above fit ( self , X , y = None ) inherited \u00b6 Fit X, y. fit_interactive ( self , X , y = None , mode = 'external' , ** server_kwargs ) inherited \u00b6 Fit a bucketer and then interactive edit the fit using a dash app. Note we are using a jupyterdash app, which supports 3 different modes: 'external' (default): Start dash server and print URL 'inline': Start dash app inside an Iframe in the jupyter notebook 'jupyterlab': Start dash app as a new tab inside jupyterlab fit_transform ( self , X , y = None , ** fit_params ) inherited \u00b6 Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X . Parameters \u00b6 X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters. Returns \u00b6 X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params ( self , deep = True ) inherited \u00b6 Get parameters for this estimator. Parameters \u00b6 deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns \u00b6 params : dict Parameter names mapped to their values. plot_bucket ( self , column , line = 'event_rate' , format = None , scale = None , width = None , height = None ) inherited \u00b6 Plot the buckets. Parameters: Name Type Description Default column The column we want to visualise required line The line to plot on the secondary axis. Default is Event Rate. 'event_rate' format The format of the image, such as 'png'. The default None returns a plotly image. None scale If format is specified, the scale of the image None width If format is specified, the width of the image None height If format is specified, the image of the image None Returns: Type Description plot plotly fig predict ( self , X ) inherited \u00b6 Applies the transform method. To be used for the grid searches. Parameters: Name Type Description Default X pd.DataFrame The numerical data which will be transformed into the corresponding buckets required Returns: Type Description y (np.array) Transformed X, such that the values of X are replaced by the corresponding bucket numbers predict_proba ( self , X ) inherited \u00b6 Applies the transform method. To be used for the grid searches. Parameters: Name Type Description Default X pd.DataFrame The numerical data which will be transformed into the corresponding buckets required Returns: Type Description yhat (np.array) transformed X, such that the values of X are replaced by the corresponding bucket numbers save_yml ( self , fout ) inherited \u00b6 Save the features bucket to a yaml file. Parameters: Name Type Description Default fout ~PathLike file output required set_params ( self , ** params ) inherited \u00b6 Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class: ~sklearn.pipeline.Pipeline ). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters \u00b6 **params : dict Estimator parameters. Returns \u00b6 self : estimator instance Estimator instance. summary ( self ) inherited \u00b6 Display a summary table for columns passed to .fit() . The format is the following: column num_prebuckets num_buckets dtype LIMIT_BAL 15 10 float64 BILL_AMT1 15 6 float64 transform ( self , X , y = None ) inherited \u00b6 Transforms an array into the corresponding buckets fitted by the Transformer. Parameters: Name Type Description Default X pd.DataFrame dataframe which will be transformed into the corresponding buckets required y array target None Returns: Type Description df (pd.DataFrame) dataset with transformed features","title":"OrdinalCategoricalBucketer"},{"location":"api/bucketers/OrdinalCategoricalBucketer/#skorecard.bucketers.bucketers.OrdinalCategoricalBucketer.variables_type","text":"Signals variables type supported by this bucketer.","title":"variables_type"},{"location":"api/bucketers/OrdinalCategoricalBucketer/#skorecard.bucketers.bucketers.OrdinalCategoricalBucketer.__init__","text":"Init the class. Parameters: Name Type Description Default tol float the minimum frequency a label should have to be considered frequent. Categories with frequencies lower than tol will be grouped together (in the 'other' bucket). 0.05 max_n_categories int the maximum number of categories that should be considered frequent. If None, all categories with frequency above the tolerance (tol) will be considered. None variables list The features to bucket. Uses all features if not defined. [] specials dict (nested) dictionary of special values that require their own binning. The dictionary has the following format: {\" \" : {\"name of special bucket\" : }} For every feature that needs a special value, a dictionary must be passed as value. This dictionary contains a name of a bucket (key) and an array of unique values that should be put in that bucket. When special values are defined, they are not considered in the fitting procedure. {} encoding_method string encoding method. - \"frequency\" (default): orders the buckets based on the frequency of observations in the bucket. The lower the number of the bucket the most frequent are the observations in that bucket. - \"ordered\": orders the buckets based on the average class 1 rate in the bucket. The lower the number of the bucket the lower the fraction of class 1 in that bucket. 'frequency' missing_treatment str or dict Defines how we treat the missing values present in the data. If a string, it must be one of the following options: separate: Missing values get put in a separate 'Other' bucket: -1 most_risky: Missing values are put into the bucket containing the largest percentage of Class 1. least_risky: Missing values are put into the bucket containing the largest percentage of Class 0. most_frequent: Missing values are put into the most common bucket. neutral: Missing values are put into the bucket with WoE closest to 0. similar: Missing values are put into the bucket with WoE closest to the bucket with only missing values. passthrough: Leaves missing values untouched. If a dict, it must be of the following format: {\" \": } This bucket number is where we will put the missing values. 'separate' remainder str How we want the non-specified columns to be transformed. It must be in [\"passthrough\", \"drop\"]. passthrough (Default): all columns that were not specified in \"variables\" will be passed through. drop: all remaining columns that were not specified in \"variables\" will be dropped. 'passthrough'","title":"__init__()"},{"location":"api/bucketers/OrdinalCategoricalBucketer/#skorecard.bucketers.bucketers.OrdinalCategoricalBucketer.bucket_table","text":"Generates the statistics for the buckets of a particular column. The pre-buckets are matched to the post-buckets, so that the user has a much clearer understanding of how the BucketingProcess ends up with the final buckets. An example: bucket label Count Count (%) Non-event Event % Event % Non-event Event Rate WoE IV 0 (-inf, 25.0) 61.0 1.36 57.0 4.0 0.41 1.62 0.066 1.380 0.017 1 [25.0, 45.0) 2024.0 44.98 1536.0 488.0 49.64 43.67 0.241 -0.128 0.008 Parameters: Name Type Description Default column The column we wish to analyse required Returns: Type Description df (pd.DataFrame) A pandas dataframe of the format above","title":"bucket_table()"},{"location":"api/bucketers/OrdinalCategoricalBucketer/#skorecard.bucketers.bucketers.OrdinalCategoricalBucketer.fit","text":"Fit X, y.","title":"fit()"},{"location":"api/bucketers/OrdinalCategoricalBucketer/#skorecard.bucketers.bucketers.OrdinalCategoricalBucketer.fit_interactive","text":"Fit a bucketer and then interactive edit the fit using a dash app. Note we are using a jupyterdash app, which supports 3 different modes: 'external' (default): Start dash server and print URL 'inline': Start dash app inside an Iframe in the jupyter notebook 'jupyterlab': Start dash app as a new tab inside jupyterlab","title":"fit_interactive()"},{"location":"api/bucketers/OrdinalCategoricalBucketer/#skorecard.bucketers.bucketers.OrdinalCategoricalBucketer.fit_transform","text":"Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X .","title":"fit_transform()"},{"location":"api/bucketers/OrdinalCategoricalBucketer/#skorecard.bucketers.bucketers.OrdinalCategoricalBucketer.fit_transform--parameters","text":"X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters.","title":"Parameters"},{"location":"api/bucketers/OrdinalCategoricalBucketer/#skorecard.bucketers.bucketers.OrdinalCategoricalBucketer.fit_transform--returns","text":"X_new : ndarray array of shape (n_samples, n_features_new) Transformed array.","title":"Returns"},{"location":"api/bucketers/OrdinalCategoricalBucketer/#skorecard.bucketers.bucketers.OrdinalCategoricalBucketer.get_params","text":"Get parameters for this estimator.","title":"get_params()"},{"location":"api/bucketers/OrdinalCategoricalBucketer/#skorecard.bucketers.bucketers.OrdinalCategoricalBucketer.get_params--parameters","text":"deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators.","title":"Parameters"},{"location":"api/bucketers/OrdinalCategoricalBucketer/#skorecard.bucketers.bucketers.OrdinalCategoricalBucketer.get_params--returns","text":"params : dict Parameter names mapped to their values.","title":"Returns"},{"location":"api/bucketers/OrdinalCategoricalBucketer/#skorecard.bucketers.bucketers.OrdinalCategoricalBucketer.plot_bucket","text":"Plot the buckets. Parameters: Name Type Description Default column The column we want to visualise required line The line to plot on the secondary axis. Default is Event Rate. 'event_rate' format The format of the image, such as 'png'. The default None returns a plotly image. None scale If format is specified, the scale of the image None width If format is specified, the width of the image None height If format is specified, the image of the image None Returns: Type Description plot plotly fig","title":"plot_bucket()"},{"location":"api/bucketers/OrdinalCategoricalBucketer/#skorecard.bucketers.bucketers.OrdinalCategoricalBucketer.predict","text":"Applies the transform method. To be used for the grid searches. Parameters: Name Type Description Default X pd.DataFrame The numerical data which will be transformed into the corresponding buckets required Returns: Type Description y (np.array) Transformed X, such that the values of X are replaced by the corresponding bucket numbers","title":"predict()"},{"location":"api/bucketers/OrdinalCategoricalBucketer/#skorecard.bucketers.bucketers.OrdinalCategoricalBucketer.predict_proba","text":"Applies the transform method. To be used for the grid searches. Parameters: Name Type Description Default X pd.DataFrame The numerical data which will be transformed into the corresponding buckets required Returns: Type Description yhat (np.array) transformed X, such that the values of X are replaced by the corresponding bucket numbers","title":"predict_proba()"},{"location":"api/bucketers/OrdinalCategoricalBucketer/#skorecard.bucketers.bucketers.OrdinalCategoricalBucketer.save_yml","text":"Save the features bucket to a yaml file. Parameters: Name Type Description Default fout ~PathLike file output required","title":"save_yml()"},{"location":"api/bucketers/OrdinalCategoricalBucketer/#skorecard.bucketers.bucketers.OrdinalCategoricalBucketer.set_params","text":"Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class: ~sklearn.pipeline.Pipeline ). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object.","title":"set_params()"},{"location":"api/bucketers/OrdinalCategoricalBucketer/#skorecard.bucketers.bucketers.OrdinalCategoricalBucketer.set_params--parameters","text":"**params : dict Estimator parameters.","title":"Parameters"},{"location":"api/bucketers/OrdinalCategoricalBucketer/#skorecard.bucketers.bucketers.OrdinalCategoricalBucketer.set_params--returns","text":"self : estimator instance Estimator instance.","title":"Returns"},{"location":"api/bucketers/OrdinalCategoricalBucketer/#skorecard.bucketers.bucketers.OrdinalCategoricalBucketer.summary","text":"Display a summary table for columns passed to .fit() . The format is the following: column num_prebuckets num_buckets dtype LIMIT_BAL 15 10 float64 BILL_AMT1 15 6 float64","title":"summary()"},{"location":"api/bucketers/OrdinalCategoricalBucketer/#skorecard.bucketers.bucketers.OrdinalCategoricalBucketer.transform","text":"Transforms an array into the corresponding buckets fitted by the Transformer. Parameters: Name Type Description Default X pd.DataFrame dataframe which will be transformed into the corresponding buckets required y array target None Returns: Type Description df (pd.DataFrame) dataset with transformed features","title":"transform()"},{"location":"api/bucketers/UserInputBucketer/","text":"The UserInputBucketer transformer creates buckets by implementing user-defined boundaries. Support: This is a special bucketer that is not fitted but rather relies on pre-defined user input. The most common use-case is loading bucket mapping information previously fitted by other bucketers. Examples: from skorecard import datasets from skorecard.bucketers import AgglomerativeClusteringBucketer , UserInputBucketer X , y = datasets . load_uci_credit_card ( return_X_y = True ) ac_bucketer = AgglomerativeClusteringBucketer ( n_bins = 3 , variables = [ 'LIMIT_BAL' ]) ac_bucketer . fit ( X ) mapping = ac_bucketer . features_bucket_mapping_ ui_bucketer = UserInputBucketer ( mapping ) new_X = ui_bucketer . fit_transform ( X ) assert len ( new_X [ 'LIMIT_BAL' ] . unique ()) == 3 #Map some values to the special buckets specials = { \"LIMIT_BAL\" :{ \"=50000\" :[ 50000 ], \"in [20001,30000]\" :[ 20000 , 30000 ], } } ac_bucketer = AgglomerativeClusteringBucketer ( n_bins = 3 , variables = [ 'LIMIT_BAL' ], specials = specials ) ac_bucketer . fit ( X ) mapping = ac_bucketer . features_bucket_mapping_ ui_bucketer = UserInputBucketer ( mapping ) new_X = ui_bucketer . fit_transform ( X ) assert len ( new_X [ 'LIMIT_BAL' ] . unique ()) == 5 __init__ ( self , features_bucket_mapping = None , variables = [], remainder = 'passthrough' ) special \u00b6 Initialise the user-defined boundaries with a dictionary. Notes: - features_bucket_mapping is stored without the trailing underscore (_) because it is not fitted. Parameters: Name Type Description Default features_bucket_mapping None, Dict, FeaturesBucketMapping, str or Path Contains the feature name and boundaries defined for this feature. If a dict, it will be converted to an internal FeaturesBucketMapping object. If a string or path, which will attempt to load the file as a yaml and convert to FeaturesBucketMapping object. None variables list The features to bucket. Uses all features in features_bucket_mapping if not defined. [] remainder str How we want the non-specified columns to be transformed. It must be in [\"passthrough\", \"drop\"]. passthrough (Default): all columns that were not specified in \"variables\" will be passed through. drop: all remaining columns that were not specified in \"variables\" will be dropped. 'passthrough' bucket_table ( self , column ) inherited \u00b6 Generates the statistics for the buckets of a particular column. The pre-buckets are matched to the post-buckets, so that the user has a much clearer understanding of how the BucketingProcess ends up with the final buckets. An example: bucket label Count Count (%) Non-event Event % Event % Non-event Event Rate WoE IV 0 (-inf, 25.0) 61.0 1.36 57.0 4.0 0.41 1.62 0.066 1.380 0.017 1 [25.0, 45.0) 2024.0 44.98 1536.0 488.0 49.64 43.67 0.241 -0.128 0.008 Parameters: Name Type Description Default column The column we wish to analyse required Returns: Type Description df (pd.DataFrame) A pandas dataframe of the format above fit ( self , X , y = None ) \u00b6 Init the class. fit_interactive ( self , X , y = None , mode = 'external' , ** server_kwargs ) inherited \u00b6 Fit a bucketer and then interactive edit the fit using a dash app. Note we are using a jupyterdash app, which supports 3 different modes: 'external' (default): Start dash server and print URL 'inline': Start dash app inside an Iframe in the jupyter notebook 'jupyterlab': Start dash app as a new tab inside jupyterlab fit_transform ( self , X , y = None , ** fit_params ) inherited \u00b6 Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X . Parameters \u00b6 X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters. Returns \u00b6 X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params ( self , deep = True ) inherited \u00b6 Get parameters for this estimator. Parameters \u00b6 deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns \u00b6 params : dict Parameter names mapped to their values. plot_bucket ( self , column , line = 'event_rate' , format = None , scale = None , width = None , height = None ) inherited \u00b6 Plot the buckets. Parameters: Name Type Description Default column The column we want to visualise required line The line to plot on the secondary axis. Default is Event Rate. 'event_rate' format The format of the image, such as 'png'. The default None returns a plotly image. None scale If format is specified, the scale of the image None width If format is specified, the width of the image None height If format is specified, the image of the image None Returns: Type Description plot plotly fig predict ( self , X ) inherited \u00b6 Applies the transform method. To be used for the grid searches. Parameters: Name Type Description Default X pd.DataFrame The numerical data which will be transformed into the corresponding buckets required Returns: Type Description y (np.array) Transformed X, such that the values of X are replaced by the corresponding bucket numbers predict_proba ( self , X ) inherited \u00b6 Applies the transform method. To be used for the grid searches. Parameters: Name Type Description Default X pd.DataFrame The numerical data which will be transformed into the corresponding buckets required Returns: Type Description yhat (np.array) transformed X, such that the values of X are replaced by the corresponding bucket numbers save_yml ( self , fout ) inherited \u00b6 Save the features bucket to a yaml file. Parameters: Name Type Description Default fout ~PathLike file output required set_params ( self , ** params ) inherited \u00b6 Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class: ~sklearn.pipeline.Pipeline ). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters \u00b6 **params : dict Estimator parameters. Returns \u00b6 self : estimator instance Estimator instance. summary ( self ) inherited \u00b6 Display a summary table for columns passed to .fit() . The format is the following: column num_prebuckets num_buckets dtype LIMIT_BAL 15 10 float64 BILL_AMT1 15 6 float64 transform ( self , X , y = None ) inherited \u00b6 Transforms an array into the corresponding buckets fitted by the Transformer. Parameters: Name Type Description Default X pd.DataFrame dataframe which will be transformed into the corresponding buckets required y array target None Returns: Type Description df (pd.DataFrame) dataset with transformed features","title":"UserInputBucketer"},{"location":"api/bucketers/UserInputBucketer/#skorecard.bucketers.bucketers.UserInputBucketer.__init__","text":"Initialise the user-defined boundaries with a dictionary. Notes: - features_bucket_mapping is stored without the trailing underscore (_) because it is not fitted. Parameters: Name Type Description Default features_bucket_mapping None, Dict, FeaturesBucketMapping, str or Path Contains the feature name and boundaries defined for this feature. If a dict, it will be converted to an internal FeaturesBucketMapping object. If a string or path, which will attempt to load the file as a yaml and convert to FeaturesBucketMapping object. None variables list The features to bucket. Uses all features in features_bucket_mapping if not defined. [] remainder str How we want the non-specified columns to be transformed. It must be in [\"passthrough\", \"drop\"]. passthrough (Default): all columns that were not specified in \"variables\" will be passed through. drop: all remaining columns that were not specified in \"variables\" will be dropped. 'passthrough'","title":"__init__()"},{"location":"api/bucketers/UserInputBucketer/#skorecard.bucketers.bucketers.UserInputBucketer.bucket_table","text":"Generates the statistics for the buckets of a particular column. The pre-buckets are matched to the post-buckets, so that the user has a much clearer understanding of how the BucketingProcess ends up with the final buckets. An example: bucket label Count Count (%) Non-event Event % Event % Non-event Event Rate WoE IV 0 (-inf, 25.0) 61.0 1.36 57.0 4.0 0.41 1.62 0.066 1.380 0.017 1 [25.0, 45.0) 2024.0 44.98 1536.0 488.0 49.64 43.67 0.241 -0.128 0.008 Parameters: Name Type Description Default column The column we wish to analyse required Returns: Type Description df (pd.DataFrame) A pandas dataframe of the format above","title":"bucket_table()"},{"location":"api/bucketers/UserInputBucketer/#skorecard.bucketers.bucketers.UserInputBucketer.fit","text":"Init the class.","title":"fit()"},{"location":"api/bucketers/UserInputBucketer/#skorecard.bucketers.bucketers.UserInputBucketer.fit_interactive","text":"Fit a bucketer and then interactive edit the fit using a dash app. Note we are using a jupyterdash app, which supports 3 different modes: 'external' (default): Start dash server and print URL 'inline': Start dash app inside an Iframe in the jupyter notebook 'jupyterlab': Start dash app as a new tab inside jupyterlab","title":"fit_interactive()"},{"location":"api/bucketers/UserInputBucketer/#skorecard.bucketers.bucketers.UserInputBucketer.fit_transform","text":"Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X .","title":"fit_transform()"},{"location":"api/bucketers/UserInputBucketer/#skorecard.bucketers.bucketers.UserInputBucketer.fit_transform--parameters","text":"X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters.","title":"Parameters"},{"location":"api/bucketers/UserInputBucketer/#skorecard.bucketers.bucketers.UserInputBucketer.fit_transform--returns","text":"X_new : ndarray array of shape (n_samples, n_features_new) Transformed array.","title":"Returns"},{"location":"api/bucketers/UserInputBucketer/#skorecard.bucketers.bucketers.UserInputBucketer.get_params","text":"Get parameters for this estimator.","title":"get_params()"},{"location":"api/bucketers/UserInputBucketer/#skorecard.bucketers.bucketers.UserInputBucketer.get_params--parameters","text":"deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators.","title":"Parameters"},{"location":"api/bucketers/UserInputBucketer/#skorecard.bucketers.bucketers.UserInputBucketer.get_params--returns","text":"params : dict Parameter names mapped to their values.","title":"Returns"},{"location":"api/bucketers/UserInputBucketer/#skorecard.bucketers.bucketers.UserInputBucketer.plot_bucket","text":"Plot the buckets. Parameters: Name Type Description Default column The column we want to visualise required line The line to plot on the secondary axis. Default is Event Rate. 'event_rate' format The format of the image, such as 'png'. The default None returns a plotly image. None scale If format is specified, the scale of the image None width If format is specified, the width of the image None height If format is specified, the image of the image None Returns: Type Description plot plotly fig","title":"plot_bucket()"},{"location":"api/bucketers/UserInputBucketer/#skorecard.bucketers.bucketers.UserInputBucketer.predict","text":"Applies the transform method. To be used for the grid searches. Parameters: Name Type Description Default X pd.DataFrame The numerical data which will be transformed into the corresponding buckets required Returns: Type Description y (np.array) Transformed X, such that the values of X are replaced by the corresponding bucket numbers","title":"predict()"},{"location":"api/bucketers/UserInputBucketer/#skorecard.bucketers.bucketers.UserInputBucketer.predict_proba","text":"Applies the transform method. To be used for the grid searches. Parameters: Name Type Description Default X pd.DataFrame The numerical data which will be transformed into the corresponding buckets required Returns: Type Description yhat (np.array) transformed X, such that the values of X are replaced by the corresponding bucket numbers","title":"predict_proba()"},{"location":"api/bucketers/UserInputBucketer/#skorecard.bucketers.bucketers.UserInputBucketer.save_yml","text":"Save the features bucket to a yaml file. Parameters: Name Type Description Default fout ~PathLike file output required","title":"save_yml()"},{"location":"api/bucketers/UserInputBucketer/#skorecard.bucketers.bucketers.UserInputBucketer.set_params","text":"Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class: ~sklearn.pipeline.Pipeline ). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object.","title":"set_params()"},{"location":"api/bucketers/UserInputBucketer/#skorecard.bucketers.bucketers.UserInputBucketer.set_params--parameters","text":"**params : dict Estimator parameters.","title":"Parameters"},{"location":"api/bucketers/UserInputBucketer/#skorecard.bucketers.bucketers.UserInputBucketer.set_params--returns","text":"self : estimator instance Estimator instance.","title":"Returns"},{"location":"api/bucketers/UserInputBucketer/#skorecard.bucketers.bucketers.UserInputBucketer.summary","text":"Display a summary table for columns passed to .fit() . The format is the following: column num_prebuckets num_buckets dtype LIMIT_BAL 15 10 float64 BILL_AMT1 15 6 float64","title":"summary()"},{"location":"api/bucketers/UserInputBucketer/#skorecard.bucketers.bucketers.UserInputBucketer.transform","text":"Transforms an array into the corresponding buckets fitted by the Transformer. Parameters: Name Type Description Default X pd.DataFrame dataframe which will be transformed into the corresponding buckets required y array target None Returns: Type Description df (pd.DataFrame) dataset with transformed features","title":"transform()"},{"location":"api/datasets/load_uci_credit_card/","text":"Datasets \u00b6 Loads the UCI Credit Card Dataset. This dataset contains a sample of Default of Credit Card Clients Dataset . Examples: from skorecard import datasets df = datasets . load_uci_credit_card ( as_frame = True ) Parameters: Name Type Description Default return_X_y bool If True, returns (data, target) instead of a dict object. False as_frame bool give the pandas dataframe instead of X, y matrices (default=False). False (pd.DataFrame, dict or tuple) features and target, with as follows: if as_frame is True: returns pd.DataFrame with y as a target return_X_y is True: returns a tuple: (X,y) is both are false (default setting): returns a dictionary where the key data contains the features, and the key target is the target","title":"Datasets"},{"location":"api/datasets/load_uci_credit_card/#datasets","text":"Loads the UCI Credit Card Dataset. This dataset contains a sample of Default of Credit Card Clients Dataset . Examples: from skorecard import datasets df = datasets . load_uci_credit_card ( as_frame = True ) Parameters: Name Type Description Default return_X_y bool If True, returns (data, target) instead of a dict object. False as_frame bool give the pandas dataframe instead of X, y matrices (default=False). False (pd.DataFrame, dict or tuple) features and target, with as follows: if as_frame is True: returns pd.DataFrame with y as a target return_X_y is True: returns a tuple: (X,y) is both are false (default setting): returns a dictionary where the key data contains the features, and the key target is the target","title":"Datasets"},{"location":"api/linear_model/LogisticRegression/","text":"Extended Logistic Regression. Extends sklearn.linear_model.LogisticRegression . This class provides the following extra statistics, calculated on .fit() and accessible via .get_stats() : cov_matrix_ : covariance matrix for the estimated parameters. std_err_intercept_ : estimated uncertainty for the intercept std_err_coef_ : estimated uncertainty for the coefficients z_intercept_ : estimated z-statistic for the intercept z_coef_ : estimated z-statistic for the coefficients p_value_intercept_ : estimated p-value for the intercept p_value_coef_ : estimated p-value for the coefficients Examples: from skorecard.datasets import load_uci_credit_card from skorecard.bucketers import EqualFrequencyBucketer from skorecard.linear_model import LogisticRegression from sklearn.pipeline import Pipeline from sklearn.preprocessing import OneHotEncoder X , y = load_uci_credit_card ( return_X_y = True ) pipeline = Pipeline ([ ( 'bucketer' , EqualFrequencyBucketer ( n_bins = 10 )), ( 'clf' , LogisticRegression ( calculate_stats = True )) ]) pipeline . fit ( X , y ) assert pipeline . named_steps [ 'clf' ] . p_val_coef_ [ 0 ][ 0 ] > 0 pipeline . named_steps [ 'clf' ] . get_stats () An example output of .get_stats() : Index Coef. Std.Err z Pz const -0.537571 0.096108 -5.593394 2.226735e-08 EDUCATION 0.010091 0.044874 0.224876 8.220757e-01 __init__ ( self , penalty = 'l2' , calculate_stats = False , dual = False , tol = 0.0001 , C = 1.0 , fit_intercept = True , intercept_scaling = 1 , class_weight = None , random_state = None , solver = 'lbfgs' , max_iter = 100 , multi_class = 'auto' , verbose = 0 , warm_start = False , n_jobs = None , l1_ratio = None ) special \u00b6 Extends sklearn.linear_model.LogisticRegression.fit() . Parameters: Name Type Description Default calculate_stats bool If true, calculate statistics like standard error during fit, accessible with .get_stats() False decision_function ( self , X ) inherited \u00b6 Predict confidence scores for samples. The confidence score for a sample is proportional to the signed distance of that sample to the hyperplane. Parameters \u00b6 X : array-like or sparse matrix, shape (n_samples, n_features) Samples. Returns \u00b6 array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes) Confidence scores per (sample, class) combination. In the binary case, confidence score for self.classes_[1] where >0 means this class would be predicted. densify ( self ) inherited \u00b6 Convert coefficient matrix to dense array format. Converts the coef_ member (back) to a numpy.ndarray. This is the default format of coef_ and is required for fitting, so calling this method is only required on models that have previously been sparsified; otherwise, it is a no-op. Returns \u00b6 self Fitted estimator. fit ( self , X , y , sample_weight = None , calculate_stats = False , ** kwargs ) \u00b6 Fit the model. Overwrites sklearn.linear_model.LogisticRegression.fit() . In addition to the standard fit by sklearn, this function will compute the covariance of the coefficients. Parameters: Name Type Description Default X array-like, sparse matrix Matrix of shape (n_samples, n_features) Training vector, where n_samples is the number of samples and n_features is the number of features. required y array-like of shape (n_samples,) Target vector relative to X. required sample_weight array-like of shape (n_samples,) default=None Array of weights that are assigned to individual samples. If not provided, then each sample is given unit weight. None calculate_stats bool If true, calculate statistics like standard error during fit, accessible with .get_stats() False Returns: Type Description self (LogisticRegression) Fitted estimator. get_params ( self , deep = True ) inherited \u00b6 Get parameters for this estimator. Parameters \u00b6 deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns \u00b6 params : dict Parameter names mapped to their values. get_stats ( self ) \u00b6 Puts the summary statistics of the fit() function into a pandas DataFrame. Returns: Type Description data (pandas DataFrame) The statistics dataframe, indexed by the column name plot_weights ( self ) \u00b6 Plots the relative importance of coefficients of the model. Examples: from skorecard.datasets import load_uci_credit_card from skorecard.bucketers import EqualFrequencyBucketer from skorecard.linear_model import LogisticRegression from skorecard.reporting.plotting import weight_plot from sklearn.pipeline import Pipeline from sklearn.preprocessing import OneHotEncoder X, y = load_uci_credit_card(return_X_y=True) pipeline = Pipeline([ ('bucketer', EqualFrequencyBucketer(n_bins=10)), ('clf', LogisticRegression(calculate_stats=True)) ]) pipeline.fit(X, y) assert pipeline.named_steps['clf'].p_val_coef_[0][0] > 0 stats = pipeline.named_steps['clf'].get_stats() pipeline.named_steps['clf'].plot_weights() predict ( self , X ) inherited \u00b6 Predict class labels for samples in X. Parameters \u00b6 X : array-like or sparse matrix, shape (n_samples, n_features) Samples. Returns \u00b6 C : array, shape [n_samples] Predicted class label per sample. predict_log_proba ( self , X ) inherited \u00b6 Predict logarithm of probability estimates. The returned estimates for all classes are ordered by the label of classes. Parameters \u00b6 X : array-like of shape (n_samples, n_features) Vector to be scored, where n_samples is the number of samples and n_features is the number of features. Returns \u00b6 T : array-like of shape (n_samples, n_classes) Returns the log-probability of the sample for each class in the model, where classes are ordered as they are in self.classes_ . predict_proba ( self , X ) inherited \u00b6 Probability estimates. The returned estimates for all classes are ordered by the label of classes. For a multi_class problem, if multi_class is set to be \"multinomial\" the softmax function is used to find the predicted probability of each class. Else use a one-vs-rest approach, i.e calculate the probability of each class assuming it to be positive using the logistic function. and normalize these values across all the classes. Parameters \u00b6 X : array-like of shape (n_samples, n_features) Vector to be scored, where n_samples is the number of samples and n_features is the number of features. Returns \u00b6 T : array-like of shape (n_samples, n_classes) Returns the probability of the sample for each class in the model, where classes are ordered as they are in self.classes_ . score ( self , X , y , sample_weight = None ) inherited \u00b6 Return the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted. Parameters \u00b6 X : array-like of shape (n_samples, n_features) Test samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs) True labels for X . sample_weight : array-like of shape (n_samples,), default=None Sample weights. Returns \u00b6 score : float Mean accuracy of self.predict(X) wrt. y . set_params ( self , ** params ) inherited \u00b6 Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class: ~sklearn.pipeline.Pipeline ). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters \u00b6 **params : dict Estimator parameters. Returns \u00b6 self : estimator instance Estimator instance. sparsify ( self ) inherited \u00b6 Convert coefficient matrix to sparse format. Converts the coef_ member to a scipy.sparse matrix, which for L1-regularized models can be much more memory- and storage-efficient than the usual numpy.ndarray representation. The intercept_ member is not converted. Returns \u00b6 self Fitted estimator. Notes \u00b6 For non-sparse models, i.e. when there are not many zeros in coef_ , this may actually increase memory usage, so use this method with care. A rule of thumb is that the number of zero elements, which can be computed with (coef_ == 0).sum() , must be more than 50% for this to provide significant benefits. After calling this method, further fitting with the partial_fit method (if any) will not work until you call densify.","title":"LogisticRegression"},{"location":"api/linear_model/LogisticRegression/#skorecard.linear_model.linear_model.LogisticRegression.__init__","text":"Extends sklearn.linear_model.LogisticRegression.fit() . Parameters: Name Type Description Default calculate_stats bool If true, calculate statistics like standard error during fit, accessible with .get_stats() False","title":"__init__()"},{"location":"api/linear_model/LogisticRegression/#skorecard.linear_model.linear_model.LogisticRegression.decision_function","text":"Predict confidence scores for samples. The confidence score for a sample is proportional to the signed distance of that sample to the hyperplane.","title":"decision_function()"},{"location":"api/linear_model/LogisticRegression/#skorecard.linear_model.linear_model.LogisticRegression.decision_function--parameters","text":"X : array-like or sparse matrix, shape (n_samples, n_features) Samples.","title":"Parameters"},{"location":"api/linear_model/LogisticRegression/#skorecard.linear_model.linear_model.LogisticRegression.decision_function--returns","text":"array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes) Confidence scores per (sample, class) combination. In the binary case, confidence score for self.classes_[1] where >0 means this class would be predicted.","title":"Returns"},{"location":"api/linear_model/LogisticRegression/#skorecard.linear_model.linear_model.LogisticRegression.densify","text":"Convert coefficient matrix to dense array format. Converts the coef_ member (back) to a numpy.ndarray. This is the default format of coef_ and is required for fitting, so calling this method is only required on models that have previously been sparsified; otherwise, it is a no-op.","title":"densify()"},{"location":"api/linear_model/LogisticRegression/#skorecard.linear_model.linear_model.LogisticRegression.densify--returns","text":"self Fitted estimator.","title":"Returns"},{"location":"api/linear_model/LogisticRegression/#skorecard.linear_model.linear_model.LogisticRegression.fit","text":"Fit the model. Overwrites sklearn.linear_model.LogisticRegression.fit() . In addition to the standard fit by sklearn, this function will compute the covariance of the coefficients. Parameters: Name Type Description Default X array-like, sparse matrix Matrix of shape (n_samples, n_features) Training vector, where n_samples is the number of samples and n_features is the number of features. required y array-like of shape (n_samples,) Target vector relative to X. required sample_weight array-like of shape (n_samples,) default=None Array of weights that are assigned to individual samples. If not provided, then each sample is given unit weight. None calculate_stats bool If true, calculate statistics like standard error during fit, accessible with .get_stats() False Returns: Type Description self (LogisticRegression) Fitted estimator.","title":"fit()"},{"location":"api/linear_model/LogisticRegression/#skorecard.linear_model.linear_model.LogisticRegression.get_params","text":"Get parameters for this estimator.","title":"get_params()"},{"location":"api/linear_model/LogisticRegression/#skorecard.linear_model.linear_model.LogisticRegression.get_params--parameters","text":"deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators.","title":"Parameters"},{"location":"api/linear_model/LogisticRegression/#skorecard.linear_model.linear_model.LogisticRegression.get_params--returns","text":"params : dict Parameter names mapped to their values.","title":"Returns"},{"location":"api/linear_model/LogisticRegression/#skorecard.linear_model.linear_model.LogisticRegression.get_stats","text":"Puts the summary statistics of the fit() function into a pandas DataFrame. Returns: Type Description data (pandas DataFrame) The statistics dataframe, indexed by the column name","title":"get_stats()"},{"location":"api/linear_model/LogisticRegression/#skorecard.linear_model.linear_model.LogisticRegression.plot_weights","text":"Plots the relative importance of coefficients of the model. Examples: from skorecard.datasets import load_uci_credit_card from skorecard.bucketers import EqualFrequencyBucketer from skorecard.linear_model import LogisticRegression from skorecard.reporting.plotting import weight_plot from sklearn.pipeline import Pipeline from sklearn.preprocessing import OneHotEncoder X, y = load_uci_credit_card(return_X_y=True) pipeline = Pipeline([ ('bucketer', EqualFrequencyBucketer(n_bins=10)), ('clf', LogisticRegression(calculate_stats=True)) ]) pipeline.fit(X, y) assert pipeline.named_steps['clf'].p_val_coef_[0][0] > 0 stats = pipeline.named_steps['clf'].get_stats() pipeline.named_steps['clf'].plot_weights()","title":"plot_weights()"},{"location":"api/linear_model/LogisticRegression/#skorecard.linear_model.linear_model.LogisticRegression.predict","text":"Predict class labels for samples in X.","title":"predict()"},{"location":"api/linear_model/LogisticRegression/#skorecard.linear_model.linear_model.LogisticRegression.predict--parameters","text":"X : array-like or sparse matrix, shape (n_samples, n_features) Samples.","title":"Parameters"},{"location":"api/linear_model/LogisticRegression/#skorecard.linear_model.linear_model.LogisticRegression.predict--returns","text":"C : array, shape [n_samples] Predicted class label per sample.","title":"Returns"},{"location":"api/linear_model/LogisticRegression/#skorecard.linear_model.linear_model.LogisticRegression.predict_log_proba","text":"Predict logarithm of probability estimates. The returned estimates for all classes are ordered by the label of classes.","title":"predict_log_proba()"},{"location":"api/linear_model/LogisticRegression/#skorecard.linear_model.linear_model.LogisticRegression.predict_log_proba--parameters","text":"X : array-like of shape (n_samples, n_features) Vector to be scored, where n_samples is the number of samples and n_features is the number of features.","title":"Parameters"},{"location":"api/linear_model/LogisticRegression/#skorecard.linear_model.linear_model.LogisticRegression.predict_log_proba--returns","text":"T : array-like of shape (n_samples, n_classes) Returns the log-probability of the sample for each class in the model, where classes are ordered as they are in self.classes_ .","title":"Returns"},{"location":"api/linear_model/LogisticRegression/#skorecard.linear_model.linear_model.LogisticRegression.predict_proba","text":"Probability estimates. The returned estimates for all classes are ordered by the label of classes. For a multi_class problem, if multi_class is set to be \"multinomial\" the softmax function is used to find the predicted probability of each class. Else use a one-vs-rest approach, i.e calculate the probability of each class assuming it to be positive using the logistic function. and normalize these values across all the classes.","title":"predict_proba()"},{"location":"api/linear_model/LogisticRegression/#skorecard.linear_model.linear_model.LogisticRegression.predict_proba--parameters","text":"X : array-like of shape (n_samples, n_features) Vector to be scored, where n_samples is the number of samples and n_features is the number of features.","title":"Parameters"},{"location":"api/linear_model/LogisticRegression/#skorecard.linear_model.linear_model.LogisticRegression.predict_proba--returns","text":"T : array-like of shape (n_samples, n_classes) Returns the probability of the sample for each class in the model, where classes are ordered as they are in self.classes_ .","title":"Returns"},{"location":"api/linear_model/LogisticRegression/#skorecard.linear_model.linear_model.LogisticRegression.score","text":"Return the mean accuracy on the given test data and labels. In multi-label classification, this is the subset accuracy which is a harsh metric since you require for each sample that each label set be correctly predicted.","title":"score()"},{"location":"api/linear_model/LogisticRegression/#skorecard.linear_model.linear_model.LogisticRegression.score--parameters","text":"X : array-like of shape (n_samples, n_features) Test samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs) True labels for X . sample_weight : array-like of shape (n_samples,), default=None Sample weights.","title":"Parameters"},{"location":"api/linear_model/LogisticRegression/#skorecard.linear_model.linear_model.LogisticRegression.score--returns","text":"score : float Mean accuracy of self.predict(X) wrt. y .","title":"Returns"},{"location":"api/linear_model/LogisticRegression/#skorecard.linear_model.linear_model.LogisticRegression.set_params","text":"Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class: ~sklearn.pipeline.Pipeline ). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object.","title":"set_params()"},{"location":"api/linear_model/LogisticRegression/#skorecard.linear_model.linear_model.LogisticRegression.set_params--parameters","text":"**params : dict Estimator parameters.","title":"Parameters"},{"location":"api/linear_model/LogisticRegression/#skorecard.linear_model.linear_model.LogisticRegression.set_params--returns","text":"self : estimator instance Estimator instance.","title":"Returns"},{"location":"api/linear_model/LogisticRegression/#skorecard.linear_model.linear_model.LogisticRegression.sparsify","text":"Convert coefficient matrix to sparse format. Converts the coef_ member to a scipy.sparse matrix, which for L1-regularized models can be much more memory- and storage-efficient than the usual numpy.ndarray representation. The intercept_ member is not converted.","title":"sparsify()"},{"location":"api/linear_model/LogisticRegression/#skorecard.linear_model.linear_model.LogisticRegression.sparsify--returns","text":"self Fitted estimator.","title":"Returns"},{"location":"api/linear_model/LogisticRegression/#skorecard.linear_model.linear_model.LogisticRegression.sparsify--notes","text":"For non-sparse models, i.e. when there are not many zeros in coef_ , this may actually increase memory usage, so use this method with care. A rule of thumb is that the number of zero elements, which can be computed with (coef_ == 0).sum() , must be more than 50% for this to provide significant benefits. After calling this method, further fitting with the partial_fit method (if any) will not work until you call densify.","title":"Notes"},{"location":"api/pipeline/BucketingProcess/","text":"A two-step bucketing pipeline allowing for pre-bucketing before bucketing. Often you want to pre-bucket features (f.e. to 100 buckets) before bucketing to a smaller set. This brings some additional challenges around propagating specials and defining a bucketer that is able to go from raw data to final bucket. This class facilicates the process and also provides all regular methods and attributes: .summary() : See which columns are bucketed .plot_bucket() : Plot buckets of a column .bucket_table() : Table with buckets of a column .save_to_yaml() : Save information necessary for bucketing to a YAML file .features_bucket_mapping_ : Access bucketing information Examples: from skorecard import datasets from skorecard.bucketers import DecisionTreeBucketer , OptimalBucketer , AsIsCategoricalBucketer from skorecard.pipeline import BucketingProcess from sklearn.pipeline import make_pipeline df = datasets . load_uci_credit_card ( as_frame = True ) y = df [ \"default\" ] X = df . drop ( columns = [ \"default\" ]) num_cols = [ \"LIMIT_BAL\" , \"BILL_AMT1\" ] cat_cols = [ \"EDUCATION\" , \"MARRIAGE\" ] bucketing_process = BucketingProcess ( specials = { 'LIMIT_BAL' : { '=400000.0' : [ 400000.0 ]}}, prebucketing_pipeline = make_pipeline ( DecisionTreeBucketer ( variables = num_cols , max_n_bins = 100 , min_bin_size = 0.05 ), AsIsCategoricalBucketer ( variables = cat_cols ), ), bucketing_pipeline = make_pipeline ( OptimalBucketer ( variables = num_cols , max_n_bins = 10 , min_bin_size = 0.05 ), OptimalBucketer ( variables = cat_cols , variables_type = 'categorical' , max_n_bins = 10 , min_bin_size = 0.05 ), ) ) bucketing_process . fit ( X , y ) # Details bucketing_process . summary () # all vars, and # buckets bucketing_process . bucket_table ( \"LIMIT_BAL\" ) bucketing_process . plot_bucket ( \"LIMIT_BAL\" ) bucketing_process . prebucket_table ( \"LIMIT_BAL\" ) bucketing_process . plot_prebucket ( \"LIMIT_BAL\" ) features_bucket_mapping_ property readonly \u00b6 Returns a FeaturesBucketMapping instance. In normal bucketers, you can access .features_bucket_mapping_ to retrieve a FeaturesBucketMapping instance. This contains all the info you need to transform values into their buckets. In this class, we basically have a two step bucketing process: first prebucketing, and then we bucket the prebuckets. In order to still be able to use BucketingProcess as if it were a normal bucketer, we'll need to merge both into one. name property readonly \u00b6 To be able to identity the bucketingprocess in a pipeline. __init__ ( self , prebucketing_pipeline = Pipeline ( steps = [( 'decisiontreebucketer' , DecisionTreeBucketer ( max_n_bins = 50 , min_bin_size = 0.02 ))]), bucketing_pipeline = Pipeline ( steps = [( 'optimalbucketer' , OptimalBucketer ( max_n_bins = 6 ))]), variables = [], specials = {}, random_state = None , remainder = 'passthrough' ) special \u00b6 Define a BucketingProcess to first prebucket and then bucket multiple columns in one go. Parameters: Name Type Description Default prebucketing_pipeline Pipeline The scikit-learn pipeline that does pre-bucketing. Defaults to an all-numeric DecisionTreeBucketer pipeline. Pipeline(steps=[('decisiontreebucketer', DecisionTreeBucketer(max_n_bins=50, min_bin_size=0.02))]) bucketing_pipeline Pipeline The scikit-learn pipeline that does bucketing. Defaults to an all-numeric OptimalBucketer pipeline. Must transform same features as the prebucketing pipeline. Pipeline(steps=[('optimalbucketer', OptimalBucketer(max_n_bins=6))]) variables list The features to bucket. Uses all features if not defined. [] specials Dict (nested) dictionary of special values that require their own binning. Will merge when specials are also defined in any bucketers in a (pre)bucketing pipeline, and overwrite in case there are shared keys. The dictionary has the following format: {\" \" : {\"name of special bucket\" : }} For every feature that needs a special value, a dictionary must be passed as value. This dictionary contains a name of a bucket (key) and an array of unique values that should be put in that bucket. When special values are defined, they are not considered in the fitting procedure. {} remainder str How we want the non-specified columns to be transformed. It must be in [\"passthrough\", \"drop\"]. passthrough (Default): all columns that were not specified in \"variables\" will be passed through. drop: all remaining columns that were not specified in \"variables\" will be dropped. 'passthrough' bucket_table ( self , column ) inherited \u00b6 Generates the statistics for the buckets of a particular column. The pre-buckets are matched to the post-buckets, so that the user has a much clearer understanding of how the BucketingProcess ends up with the final buckets. An example: bucket label Count Count (%) Non-event Event % Event % Non-event Event Rate WoE IV 0 (-inf, 25.0) 61.0 1.36 57.0 4.0 0.41 1.62 0.066 1.380 0.017 1 [25.0, 45.0) 2024.0 44.98 1536.0 488.0 49.64 43.67 0.241 -0.128 0.008 Parameters: Name Type Description Default column The column we wish to analyse required Returns: Type Description df (pd.DataFrame) A pandas dataframe of the format above fit ( self , X , y = None ) \u00b6 Fit the prebucketing and bucketing pipeline with X , y . Parameters: Name Type Description Default X pd.DataFrame Data to fit on. required y np.array target. Defaults to None. None fit_interactive ( self , X , y = None , mode = 'external' , ** server_kwargs ) \u00b6 Fit a bucketer and then interactive edit the fit using a dash app. Note we are using a jupyterdash app, which supports 3 different modes: 'external' (default): Start dash server and print URL 'inline': Start dash app inside an Iframe in the jupyter notebook 'jupyterlab': Start dash app as a new tab inside jupyterlab fit_transform ( self , X , y = None , ** fit_params ) inherited \u00b6 Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X . Parameters \u00b6 X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters. Returns \u00b6 X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params ( self , deep = True ) inherited \u00b6 Get parameters for this estimator. Parameters \u00b6 deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns \u00b6 params : dict Parameter names mapped to their values. plot_bucket ( self , column , line = 'event_rate' , format = None , scale = None , width = None , height = None ) inherited \u00b6 Plot the buckets. Parameters: Name Type Description Default column The column we want to visualise required line The line to plot on the secondary axis. Default is Event Rate. 'event_rate' format The format of the image, such as 'png'. The default None returns a plotly image. None scale If format is specified, the scale of the image None width If format is specified, the width of the image None height If format is specified, the image of the image None Returns: Type Description plot plotly fig plot_prebucket ( self , column , line = 'event_rate' , format = None , scale = None , width = None , height = None ) inherited \u00b6 Generates the prebucket table and produces a corresponding plotly plot. Parameters: Name Type Description Default column The column we want to visualise required line The line to plot on the secondary axis. Default is Event Rate. 'event_rate' format The format of the image, such as 'png'. The default None returns a plotly image. None scale If format is specified, the scale of the image None width If format is specified, the width of the image None height If format is specified, the image of the image None Returns: Type Description plot plotly fig prebucket_table ( self , column ) \u00b6 Generates the statistics for the buckets of a particular column. An example is seen below: pre-bucket label Count Count (%) Non-event Event Event Rate WoE IV bucket 0 (-inf, 1.0) 479 7.98 300 179 37.37 0.73 0.05 0 1 [1.0, 2.0) 370 6.17 233 137 37.03 0.71 0.04 0 Parameters: Name Type Description Default column str The column we wish to analyse required Returns: Type Description df (pd.DataFrame) A pandas dataframe of the format above save_yml ( self , fout ) \u00b6 Save the features bucket to a yaml file. Parameters: Name Type Description Default fout ~PathLike path for output file required set_params ( self , ** params ) inherited \u00b6 Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class: ~sklearn.pipeline.Pipeline ). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters \u00b6 **params : dict Estimator parameters. Returns \u00b6 self : estimator instance Estimator instance. summary ( self ) inherited \u00b6 Display a summary table for columns passed to .fit() . The format is the following: column num_prebuckets num_buckets dtype LIMIT_BAL 15 10 float64 BILL_AMT1 15 6 float64 transform ( self , X ) \u00b6 Transform X through the prebucketing and bucketing pipelines.","title":"BucketingProcess"},{"location":"api/pipeline/BucketingProcess/#skorecard.pipeline.bucketing_process.BucketingProcess.features_bucket_mapping_","text":"Returns a FeaturesBucketMapping instance. In normal bucketers, you can access .features_bucket_mapping_ to retrieve a FeaturesBucketMapping instance. This contains all the info you need to transform values into their buckets. In this class, we basically have a two step bucketing process: first prebucketing, and then we bucket the prebuckets. In order to still be able to use BucketingProcess as if it were a normal bucketer, we'll need to merge both into one.","title":"features_bucket_mapping_"},{"location":"api/pipeline/BucketingProcess/#skorecard.pipeline.bucketing_process.BucketingProcess.name","text":"To be able to identity the bucketingprocess in a pipeline.","title":"name"},{"location":"api/pipeline/BucketingProcess/#skorecard.pipeline.bucketing_process.BucketingProcess.__init__","text":"Define a BucketingProcess to first prebucket and then bucket multiple columns in one go. Parameters: Name Type Description Default prebucketing_pipeline Pipeline The scikit-learn pipeline that does pre-bucketing. Defaults to an all-numeric DecisionTreeBucketer pipeline. Pipeline(steps=[('decisiontreebucketer', DecisionTreeBucketer(max_n_bins=50, min_bin_size=0.02))]) bucketing_pipeline Pipeline The scikit-learn pipeline that does bucketing. Defaults to an all-numeric OptimalBucketer pipeline. Must transform same features as the prebucketing pipeline. Pipeline(steps=[('optimalbucketer', OptimalBucketer(max_n_bins=6))]) variables list The features to bucket. Uses all features if not defined. [] specials Dict (nested) dictionary of special values that require their own binning. Will merge when specials are also defined in any bucketers in a (pre)bucketing pipeline, and overwrite in case there are shared keys. The dictionary has the following format: {\" \" : {\"name of special bucket\" : }} For every feature that needs a special value, a dictionary must be passed as value. This dictionary contains a name of a bucket (key) and an array of unique values that should be put in that bucket. When special values are defined, they are not considered in the fitting procedure. {} remainder str How we want the non-specified columns to be transformed. It must be in [\"passthrough\", \"drop\"]. passthrough (Default): all columns that were not specified in \"variables\" will be passed through. drop: all remaining columns that were not specified in \"variables\" will be dropped. 'passthrough'","title":"__init__()"},{"location":"api/pipeline/BucketingProcess/#skorecard.pipeline.bucketing_process.BucketingProcess.bucket_table","text":"Generates the statistics for the buckets of a particular column. The pre-buckets are matched to the post-buckets, so that the user has a much clearer understanding of how the BucketingProcess ends up with the final buckets. An example: bucket label Count Count (%) Non-event Event % Event % Non-event Event Rate WoE IV 0 (-inf, 25.0) 61.0 1.36 57.0 4.0 0.41 1.62 0.066 1.380 0.017 1 [25.0, 45.0) 2024.0 44.98 1536.0 488.0 49.64 43.67 0.241 -0.128 0.008 Parameters: Name Type Description Default column The column we wish to analyse required Returns: Type Description df (pd.DataFrame) A pandas dataframe of the format above","title":"bucket_table()"},{"location":"api/pipeline/BucketingProcess/#skorecard.pipeline.bucketing_process.BucketingProcess.fit","text":"Fit the prebucketing and bucketing pipeline with X , y . Parameters: Name Type Description Default X pd.DataFrame Data to fit on. required y np.array target. Defaults to None. None","title":"fit()"},{"location":"api/pipeline/BucketingProcess/#skorecard.pipeline.bucketing_process.BucketingProcess.fit_interactive","text":"Fit a bucketer and then interactive edit the fit using a dash app. Note we are using a jupyterdash app, which supports 3 different modes: 'external' (default): Start dash server and print URL 'inline': Start dash app inside an Iframe in the jupyter notebook 'jupyterlab': Start dash app as a new tab inside jupyterlab","title":"fit_interactive()"},{"location":"api/pipeline/BucketingProcess/#skorecard.pipeline.bucketing_process.BucketingProcess.fit_transform","text":"Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X .","title":"fit_transform()"},{"location":"api/pipeline/BucketingProcess/#skorecard.pipeline.bucketing_process.BucketingProcess.fit_transform--parameters","text":"X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters.","title":"Parameters"},{"location":"api/pipeline/BucketingProcess/#skorecard.pipeline.bucketing_process.BucketingProcess.fit_transform--returns","text":"X_new : ndarray array of shape (n_samples, n_features_new) Transformed array.","title":"Returns"},{"location":"api/pipeline/BucketingProcess/#skorecard.pipeline.bucketing_process.BucketingProcess.get_params","text":"Get parameters for this estimator.","title":"get_params()"},{"location":"api/pipeline/BucketingProcess/#skorecard.pipeline.bucketing_process.BucketingProcess.get_params--parameters","text":"deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators.","title":"Parameters"},{"location":"api/pipeline/BucketingProcess/#skorecard.pipeline.bucketing_process.BucketingProcess.get_params--returns","text":"params : dict Parameter names mapped to their values.","title":"Returns"},{"location":"api/pipeline/BucketingProcess/#skorecard.pipeline.bucketing_process.BucketingProcess.plot_bucket","text":"Plot the buckets. Parameters: Name Type Description Default column The column we want to visualise required line The line to plot on the secondary axis. Default is Event Rate. 'event_rate' format The format of the image, such as 'png'. The default None returns a plotly image. None scale If format is specified, the scale of the image None width If format is specified, the width of the image None height If format is specified, the image of the image None Returns: Type Description plot plotly fig","title":"plot_bucket()"},{"location":"api/pipeline/BucketingProcess/#skorecard.pipeline.bucketing_process.BucketingProcess.plot_prebucket","text":"Generates the prebucket table and produces a corresponding plotly plot. Parameters: Name Type Description Default column The column we want to visualise required line The line to plot on the secondary axis. Default is Event Rate. 'event_rate' format The format of the image, such as 'png'. The default None returns a plotly image. None scale If format is specified, the scale of the image None width If format is specified, the width of the image None height If format is specified, the image of the image None Returns: Type Description plot plotly fig","title":"plot_prebucket()"},{"location":"api/pipeline/BucketingProcess/#skorecard.pipeline.bucketing_process.BucketingProcess.prebucket_table","text":"Generates the statistics for the buckets of a particular column. An example is seen below: pre-bucket label Count Count (%) Non-event Event Event Rate WoE IV bucket 0 (-inf, 1.0) 479 7.98 300 179 37.37 0.73 0.05 0 1 [1.0, 2.0) 370 6.17 233 137 37.03 0.71 0.04 0 Parameters: Name Type Description Default column str The column we wish to analyse required Returns: Type Description df (pd.DataFrame) A pandas dataframe of the format above","title":"prebucket_table()"},{"location":"api/pipeline/BucketingProcess/#skorecard.pipeline.bucketing_process.BucketingProcess.save_yml","text":"Save the features bucket to a yaml file. Parameters: Name Type Description Default fout ~PathLike path for output file required","title":"save_yml()"},{"location":"api/pipeline/BucketingProcess/#skorecard.pipeline.bucketing_process.BucketingProcess.set_params","text":"Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class: ~sklearn.pipeline.Pipeline ). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object.","title":"set_params()"},{"location":"api/pipeline/BucketingProcess/#skorecard.pipeline.bucketing_process.BucketingProcess.set_params--parameters","text":"**params : dict Estimator parameters.","title":"Parameters"},{"location":"api/pipeline/BucketingProcess/#skorecard.pipeline.bucketing_process.BucketingProcess.set_params--returns","text":"self : estimator instance Estimator instance.","title":"Returns"},{"location":"api/pipeline/BucketingProcess/#skorecard.pipeline.bucketing_process.BucketingProcess.summary","text":"Display a summary table for columns passed to .fit() . The format is the following: column num_prebuckets num_buckets dtype LIMIT_BAL 15 10 float64 BILL_AMT1 15 6 float64","title":"summary()"},{"location":"api/pipeline/BucketingProcess/#skorecard.pipeline.bucketing_process.BucketingProcess.transform","text":"Transform X through the prebucketing and bucketing pipelines.","title":"transform()"},{"location":"api/pipeline/KeepPandas/","text":"Wrapper to keep column names of pandas dataframes in a scikit-learn transformer. Any scikit-learn transformer wrapped in KeepPandas will return a pd.DataFrame on .transform() . Warning You should only use KeepPandas() when you know for sure scikit-learn did not change the order of your columns. Examples: from skorecard.pipeline import KeepPandas from skorecard import datasets from skorecard.bucketers import EqualWidthBucketer from sklearn.pipeline import make_pipeline from sklearn.preprocessing import StandardScaler X , y = datasets . load_uci_credit_card ( return_X_y = True ) bucket_pipeline = make_pipeline ( KeepPandas ( StandardScaler ()), EqualWidthBucketer ( n_bins = 5 , variables = [ 'LIMIT_BAL' , 'BILL_AMT1' ]), ) bucket_pipeline . fit_transform ( X , y ) __init__ ( self , transformer ) special \u00b6 Initialize. __repr__ ( self ) special \u00b6 String representation. fit ( self , X , y = None , * args , ** kwargs ) \u00b6 Fit estimator. fit_transform ( self , X , y = None , ** fit_params ) inherited \u00b6 Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X . Parameters \u00b6 X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters. Returns \u00b6 X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_feature_names ( self ) \u00b6 Return estimator feature names. get_params ( self , deep = True ) inherited \u00b6 Get parameters for this estimator. Parameters \u00b6 deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns \u00b6 params : dict Parameter names mapped to their values. set_params ( self , ** params ) inherited \u00b6 Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class: ~sklearn.pipeline.Pipeline ). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters \u00b6 **params : dict Estimator parameters. Returns \u00b6 self : estimator instance Estimator instance. transform ( self , X , * args , ** kwargs ) \u00b6 Transform X.","title":"KeepPandas"},{"location":"api/pipeline/KeepPandas/#skorecard.pipeline.pipeline.KeepPandas.__init__","text":"Initialize.","title":"__init__()"},{"location":"api/pipeline/KeepPandas/#skorecard.pipeline.pipeline.KeepPandas.__repr__","text":"String representation.","title":"__repr__()"},{"location":"api/pipeline/KeepPandas/#skorecard.pipeline.pipeline.KeepPandas.fit","text":"Fit estimator.","title":"fit()"},{"location":"api/pipeline/KeepPandas/#skorecard.pipeline.pipeline.KeepPandas.fit_transform","text":"Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X .","title":"fit_transform()"},{"location":"api/pipeline/KeepPandas/#skorecard.pipeline.pipeline.KeepPandas.fit_transform--parameters","text":"X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters.","title":"Parameters"},{"location":"api/pipeline/KeepPandas/#skorecard.pipeline.pipeline.KeepPandas.fit_transform--returns","text":"X_new : ndarray array of shape (n_samples, n_features_new) Transformed array.","title":"Returns"},{"location":"api/pipeline/KeepPandas/#skorecard.pipeline.pipeline.KeepPandas.get_feature_names","text":"Return estimator feature names.","title":"get_feature_names()"},{"location":"api/pipeline/KeepPandas/#skorecard.pipeline.pipeline.KeepPandas.get_params","text":"Get parameters for this estimator.","title":"get_params()"},{"location":"api/pipeline/KeepPandas/#skorecard.pipeline.pipeline.KeepPandas.get_params--parameters","text":"deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators.","title":"Parameters"},{"location":"api/pipeline/KeepPandas/#skorecard.pipeline.pipeline.KeepPandas.get_params--returns","text":"params : dict Parameter names mapped to their values.","title":"Returns"},{"location":"api/pipeline/KeepPandas/#skorecard.pipeline.pipeline.KeepPandas.set_params","text":"Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class: ~sklearn.pipeline.Pipeline ). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object.","title":"set_params()"},{"location":"api/pipeline/KeepPandas/#skorecard.pipeline.pipeline.KeepPandas.set_params--parameters","text":"**params : dict Estimator parameters.","title":"Parameters"},{"location":"api/pipeline/KeepPandas/#skorecard.pipeline.pipeline.KeepPandas.set_params--returns","text":"self : estimator instance Estimator instance.","title":"Returns"},{"location":"api/pipeline/KeepPandas/#skorecard.pipeline.pipeline.KeepPandas.transform","text":"Transform X.","title":"transform()"},{"location":"api/pipeline/SkorecardPipeline/","text":"A sklearn Pipeline with several attribute and methods added. This Pipeline of bucketers behaves more like a bucketer and adds: .summary() : See which columns are bucketed .plot_bucket() : Plot buckets of a column .bucket_table() : Table with buckets of a column .save_to_yaml() : Save information necessary for bucketing to a YAML file .features_bucket_mapping_ : Access bucketing information .fit_interactive() : Edit fitted buckets interactively in a dash app from skorecard.pipeline.pipeline import SkorecardPipeline from skorecard.bucketers import DecisionTreeBucketer , OrdinalCategoricalBucketer from skorecard import datasets pipe = SkorecardPipeline ([ ( 'decisiontreebucketer' , DecisionTreeBucketer ( variables = [ \"LIMIT_BAL\" , \"BILL_AMT1\" ], max_n_bins = 5 )), ( 'ordinalcategoricalbucketer' , OrdinalCategoricalBucketer ( variables = [ \"EDUCATION\" , \"MARRIAGE\" ], tol = 0.05 )), ]) df = datasets . load_uci_credit_card ( as_frame = True ) features = [ \"LIMIT_BAL\" , \"BILL_AMT1\" , \"EDUCATION\" , \"MARRIAGE\" ] X = df [ features ] y = df [ \"default\" ] . values pipe . fit ( X , y ) pipe . bucket_table ( 'LIMIT_BAL' ) bucket_tables_ property readonly \u00b6 Retrieve bucket tables. Used by .bucket_table() classes_ inherited property readonly \u00b6 The classes labels. Only exist if the last step is a classifier. feature_names_in_ inherited property readonly \u00b6 Names of features seen during first step fit method. features_bucket_mapping_ property readonly \u00b6 Retrieve features bucket mapping. n_features_in_ inherited property readonly \u00b6 Number of features seen during first step fit method. named_steps inherited property readonly \u00b6 Access the steps by name. Read-only attribute to access any step by given name. Keys are steps names and values are the steps objects. summary_dict_ : Dict property readonly \u00b6 Retrieve summary_dicts and combine. Used by .summary() __class__ ( type ) inherited \u00b6 Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()). __instancecheck__ ( cls , instance ) special \u00b6 Override for isinstance(instance, cls). __new__ ( mcls , name , bases , namespace , ** kwargs ) special staticmethod \u00b6 Create and return a new object. See help(type) for accurate signature. __subclasscheck__ ( cls , subclass ) special \u00b6 Override for issubclass(subclass, cls). register ( cls , subclass ) \u00b6 Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator. __init__ ( self , steps , * , memory = None , verbose = False ) special \u00b6 Wraps sklearn Pipeline. bucket_table ( self , column ) inherited \u00b6 Generates the statistics for the buckets of a particular column. The pre-buckets are matched to the post-buckets, so that the user has a much clearer understanding of how the BucketingProcess ends up with the final buckets. An example: bucket label Count Count (%) Non-event Event % Event % Non-event Event Rate WoE IV 0 (-inf, 25.0) 61.0 1.36 57.0 4.0 0.41 1.62 0.066 1.380 0.017 1 [25.0, 45.0) 2024.0 44.98 1536.0 488.0 49.64 43.67 0.241 -0.128 0.008 Parameters: Name Type Description Default column The column we wish to analyse required Returns: Type Description df (pd.DataFrame) A pandas dataframe of the format above decision_function ( self , X ) inherited \u00b6 Transform the data, and apply decision_function with the final estimator. Call transform of each transformer in the pipeline. The transformed data are finally passed to the final estimator that calls decision_function method. Only valid if the final estimator implements decision_function . Parameters \u00b6 X : iterable Data to predict on. Must fulfill input requirements of first step of the pipeline. Returns \u00b6 y_score : ndarray of shape (n_samples, n_classes) Result of calling decision_function on the final estimator. fit ( self , X , y = None , ** fit_params ) inherited \u00b6 Fit the model. Fit all the transformers one after the other and transform the data. Finally, fit the transformed data using the final estimator. Parameters \u00b6 X : iterable Training data. Must fulfill input requirements of first step of the pipeline. y : iterable, default=None Training targets. Must fulfill label requirements for all steps of the pipeline. **fit_params : dict of string -> object Parameters passed to the fit method of each step, where each parameter name is prefixed such that parameter p for step s has key s__p . Returns \u00b6 self : object Pipeline with fitted steps. fit_interactive ( self , X , y = None , mode = 'external' ) \u00b6 Fit a bucketer and then interactively edit the fit using a dash app. Note we are using a jupyterdash app, which supports 3 different modes: 'external' (default): Start dash server and print URL 'inline': Start dash app inside an Iframe in the jupyter notebook 'jupyterlab': Start dash app as a new tab inside jupyterlab fit_predict ( self , X , y = None , ** fit_params ) inherited \u00b6 Transform the data, and apply fit_predict with the final estimator. Call fit_transform of each transformer in the pipeline. The transformed data are finally passed to the final estimator that calls fit_predict method. Only valid if the final estimator implements fit_predict . Parameters \u00b6 X : iterable Training data. Must fulfill input requirements of first step of the pipeline. y : iterable, default=None Training targets. Must fulfill label requirements for all steps of the pipeline. **fit_params : dict of string -> object Parameters passed to the fit method of each step, where each parameter name is prefixed such that parameter p for step s has key s__p . Returns \u00b6 y_pred : ndarray Result of calling fit_predict on the final estimator. fit_transform ( self , X , y = None , ** fit_params ) inherited \u00b6 Fit the model and transform with the final estimator. Fits all the transformers one after the other and transform the data. Then uses fit_transform on transformed data with the final estimator. Parameters \u00b6 X : iterable Training data. Must fulfill input requirements of first step of the pipeline. y : iterable, default=None Training targets. Must fulfill label requirements for all steps of the pipeline. **fit_params : dict of string -> object Parameters passed to the fit method of each step, where each parameter name is prefixed such that parameter p for step s has key s__p . Returns \u00b6 Xt : ndarray of shape (n_samples, n_transformed_features) Transformed samples. get_feature_names_out ( self , input_features = None ) inherited \u00b6 Get output feature names for transformation. Transform input features using the pipeline. Parameters \u00b6 input_features : array-like of str or None, default=None Input features. Returns \u00b6 feature_names_out : ndarray of str objects Transformed feature names. get_params ( self , deep = True ) inherited \u00b6 Get parameters for this estimator. Returns the parameters given in the constructor as well as the estimators contained within the steps of the Pipeline . Parameters \u00b6 deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns \u00b6 params : mapping of string to any Parameter names mapped to their values. inverse_transform ( self , Xt ) inherited \u00b6 Apply inverse_transform for each step in a reverse order. All estimators in the pipeline must support inverse_transform . Parameters \u00b6 Xt : array-like of shape (n_samples, n_transformed_features) Data samples, where n_samples is the number of samples and n_features is the number of features. Must fulfill input requirements of last step of pipeline's inverse_transform method. Returns \u00b6 Xt : ndarray of shape (n_samples, n_features) Inverse transformed data, that is, data in the original feature space. plot_bucket ( self , column , line = 'event_rate' , format = None , scale = None , width = None , height = None ) inherited \u00b6 Plot the buckets. Parameters: Name Type Description Default column The column we want to visualise required line The line to plot on the secondary axis. Default is Event Rate. 'event_rate' format The format of the image, such as 'png'. The default None returns a plotly image. None scale If format is specified, the scale of the image None width If format is specified, the width of the image None height If format is specified, the image of the image None Returns: Type Description plot plotly fig predict ( self , X , ** predict_params ) inherited \u00b6 Transform the data, and apply predict with the final estimator. Call transform of each transformer in the pipeline. The transformed data are finally passed to the final estimator that calls predict method. Only valid if the final estimator implements predict . Parameters \u00b6 X : iterable Data to predict on. Must fulfill input requirements of first step of the pipeline. **predict_params : dict of string -> object Parameters to the predict called at the end of all transformations in the pipeline. Note that while this may be used to return uncertainties from some models with return_std or return_cov, uncertainties that are generated by the transformations in the pipeline are not propagated to the final estimator. .. versionadded:: 0.20 Returns \u00b6 y_pred : ndarray Result of calling predict on the final estimator. predict_log_proba ( self , X , ** predict_log_proba_params ) inherited \u00b6 Transform the data, and apply predict_log_proba with the final estimator. Call transform of each transformer in the pipeline. The transformed data are finally passed to the final estimator that calls predict_log_proba method. Only valid if the final estimator implements predict_log_proba . Parameters \u00b6 X : iterable Data to predict on. Must fulfill input requirements of first step of the pipeline. **predict_log_proba_params : dict of string -> object Parameters to the predict_log_proba called at the end of all transformations in the pipeline. Returns \u00b6 y_log_proba : ndarray of shape (n_samples, n_classes) Result of calling predict_log_proba on the final estimator. predict_proba ( self , X , ** predict_proba_params ) inherited \u00b6 Transform the data, and apply predict_proba with the final estimator. Call transform of each transformer in the pipeline. The transformed data are finally passed to the final estimator that calls predict_proba method. Only valid if the final estimator implements predict_proba . Parameters \u00b6 X : iterable Data to predict on. Must fulfill input requirements of first step of the pipeline. **predict_proba_params : dict of string -> object Parameters to the predict_proba called at the end of all transformations in the pipeline. Returns \u00b6 y_proba : ndarray of shape (n_samples, n_classes) Result of calling predict_proba on the final estimator. save_yml ( self , fout ) \u00b6 Save the features bucket to a yaml file. Parameters: Name Type Description Default fout file output required score ( self , X , y = None , sample_weight = None ) inherited \u00b6 Transform the data, and apply score with the final estimator. Call transform of each transformer in the pipeline. The transformed data are finally passed to the final estimator that calls score method. Only valid if the final estimator implements score . Parameters \u00b6 X : iterable Data to predict on. Must fulfill input requirements of first step of the pipeline. y : iterable, default=None Targets used for scoring. Must fulfill label requirements for all steps of the pipeline. sample_weight : array-like, default=None If not None, this argument is passed as sample_weight keyword argument to the score method of the final estimator. Returns \u00b6 score : float Result of calling score on the final estimator. score_samples ( self , X ) inherited \u00b6 Transform the data, and apply score_samples with the final estimator. Call transform of each transformer in the pipeline. The transformed data are finally passed to the final estimator that calls score_samples method. Only valid if the final estimator implements score_samples . Parameters \u00b6 X : iterable Data to predict on. Must fulfill input requirements of first step of the pipeline. Returns \u00b6 y_score : ndarray of shape (n_samples,) Result of calling score_samples on the final estimator. set_params ( self , ** kwargs ) inherited \u00b6 Set the parameters of this estimator. Valid parameter keys can be listed with get_params() . Note that you can directly set the parameters of the estimators contained in steps . Parameters \u00b6 **kwargs : dict Parameters of this estimator or parameters of estimators contained in steps . Parameters of the steps may be set using its name and the parameter name separated by a '__'. Returns \u00b6 self : object Pipeline class instance. summary ( self ) inherited \u00b6 Display a summary table for columns passed to .fit() . The format is the following: column num_prebuckets num_buckets dtype LIMIT_BAL 15 10 float64 BILL_AMT1 15 6 float64 transform ( self , X ) inherited \u00b6 Transform the data, and apply transform with the final estimator. Call transform of each transformer in the pipeline. The transformed data are finally passed to the final estimator that calls transform method. Only valid if the final estimator implements transform . This also works where final estimator is None in which case all prior transformations are applied. Parameters \u00b6 X : iterable Data to transform. Must fulfill input requirements of first step of the pipeline. Returns \u00b6 Xt : ndarray of shape (n_samples, n_transformed_features) Transformed data.","title":"SkorecardPipeline"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.bucket_tables_","text":"Retrieve bucket tables. Used by .bucket_table()","title":"bucket_tables_"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.classes_","text":"The classes labels. Only exist if the last step is a classifier.","title":"classes_"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.feature_names_in_","text":"Names of features seen during first step fit method.","title":"feature_names_in_"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.features_bucket_mapping_","text":"Retrieve features bucket mapping.","title":"features_bucket_mapping_"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.n_features_in_","text":"Number of features seen during first step fit method.","title":"n_features_in_"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.named_steps","text":"Access the steps by name. Read-only attribute to access any step by given name. Keys are steps names and values are the steps objects.","title":"named_steps"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.summary_dict_","text":"Retrieve summary_dicts and combine. Used by .summary()","title":"summary_dict_"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.__class__","text":"Metaclass for defining Abstract Base Classes (ABCs). Use this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as 'virtual subclasses' -- these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won't show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()).","title":"__class__"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.__class__.__instancecheck__","text":"Override for isinstance(instance, cls).","title":"__instancecheck__()"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.__class__.__new__","text":"Create and return a new object. See help(type) for accurate signature.","title":"__new__()"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.__class__.__subclasscheck__","text":"Override for issubclass(subclass, cls).","title":"__subclasscheck__()"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.__class__.register","text":"Register a virtual subclass of an ABC. Returns the subclass, to allow usage as a class decorator.","title":"register()"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.__init__","text":"Wraps sklearn Pipeline.","title":"__init__()"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.bucket_table","text":"Generates the statistics for the buckets of a particular column. The pre-buckets are matched to the post-buckets, so that the user has a much clearer understanding of how the BucketingProcess ends up with the final buckets. An example: bucket label Count Count (%) Non-event Event % Event % Non-event Event Rate WoE IV 0 (-inf, 25.0) 61.0 1.36 57.0 4.0 0.41 1.62 0.066 1.380 0.017 1 [25.0, 45.0) 2024.0 44.98 1536.0 488.0 49.64 43.67 0.241 -0.128 0.008 Parameters: Name Type Description Default column The column we wish to analyse required Returns: Type Description df (pd.DataFrame) A pandas dataframe of the format above","title":"bucket_table()"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.decision_function","text":"Transform the data, and apply decision_function with the final estimator. Call transform of each transformer in the pipeline. The transformed data are finally passed to the final estimator that calls decision_function method. Only valid if the final estimator implements decision_function .","title":"decision_function()"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.decision_function--parameters","text":"X : iterable Data to predict on. Must fulfill input requirements of first step of the pipeline.","title":"Parameters"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.decision_function--returns","text":"y_score : ndarray of shape (n_samples, n_classes) Result of calling decision_function on the final estimator.","title":"Returns"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.fit","text":"Fit the model. Fit all the transformers one after the other and transform the data. Finally, fit the transformed data using the final estimator.","title":"fit()"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.fit--parameters","text":"X : iterable Training data. Must fulfill input requirements of first step of the pipeline. y : iterable, default=None Training targets. Must fulfill label requirements for all steps of the pipeline. **fit_params : dict of string -> object Parameters passed to the fit method of each step, where each parameter name is prefixed such that parameter p for step s has key s__p .","title":"Parameters"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.fit--returns","text":"self : object Pipeline with fitted steps.","title":"Returns"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.fit_interactive","text":"Fit a bucketer and then interactively edit the fit using a dash app. Note we are using a jupyterdash app, which supports 3 different modes: 'external' (default): Start dash server and print URL 'inline': Start dash app inside an Iframe in the jupyter notebook 'jupyterlab': Start dash app as a new tab inside jupyterlab","title":"fit_interactive()"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.fit_predict","text":"Transform the data, and apply fit_predict with the final estimator. Call fit_transform of each transformer in the pipeline. The transformed data are finally passed to the final estimator that calls fit_predict method. Only valid if the final estimator implements fit_predict .","title":"fit_predict()"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.fit_predict--parameters","text":"X : iterable Training data. Must fulfill input requirements of first step of the pipeline. y : iterable, default=None Training targets. Must fulfill label requirements for all steps of the pipeline. **fit_params : dict of string -> object Parameters passed to the fit method of each step, where each parameter name is prefixed such that parameter p for step s has key s__p .","title":"Parameters"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.fit_predict--returns","text":"y_pred : ndarray Result of calling fit_predict on the final estimator.","title":"Returns"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.fit_transform","text":"Fit the model and transform with the final estimator. Fits all the transformers one after the other and transform the data. Then uses fit_transform on transformed data with the final estimator.","title":"fit_transform()"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.fit_transform--parameters","text":"X : iterable Training data. Must fulfill input requirements of first step of the pipeline. y : iterable, default=None Training targets. Must fulfill label requirements for all steps of the pipeline. **fit_params : dict of string -> object Parameters passed to the fit method of each step, where each parameter name is prefixed such that parameter p for step s has key s__p .","title":"Parameters"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.fit_transform--returns","text":"Xt : ndarray of shape (n_samples, n_transformed_features) Transformed samples.","title":"Returns"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.get_feature_names_out","text":"Get output feature names for transformation. Transform input features using the pipeline.","title":"get_feature_names_out()"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.get_feature_names_out--parameters","text":"input_features : array-like of str or None, default=None Input features.","title":"Parameters"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.get_feature_names_out--returns","text":"feature_names_out : ndarray of str objects Transformed feature names.","title":"Returns"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.get_params","text":"Get parameters for this estimator. Returns the parameters given in the constructor as well as the estimators contained within the steps of the Pipeline .","title":"get_params()"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.get_params--parameters","text":"deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators.","title":"Parameters"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.get_params--returns","text":"params : mapping of string to any Parameter names mapped to their values.","title":"Returns"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.inverse_transform","text":"Apply inverse_transform for each step in a reverse order. All estimators in the pipeline must support inverse_transform .","title":"inverse_transform()"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.inverse_transform--parameters","text":"Xt : array-like of shape (n_samples, n_transformed_features) Data samples, where n_samples is the number of samples and n_features is the number of features. Must fulfill input requirements of last step of pipeline's inverse_transform method.","title":"Parameters"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.inverse_transform--returns","text":"Xt : ndarray of shape (n_samples, n_features) Inverse transformed data, that is, data in the original feature space.","title":"Returns"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.plot_bucket","text":"Plot the buckets. Parameters: Name Type Description Default column The column we want to visualise required line The line to plot on the secondary axis. Default is Event Rate. 'event_rate' format The format of the image, such as 'png'. The default None returns a plotly image. None scale If format is specified, the scale of the image None width If format is specified, the width of the image None height If format is specified, the image of the image None Returns: Type Description plot plotly fig","title":"plot_bucket()"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.predict","text":"Transform the data, and apply predict with the final estimator. Call transform of each transformer in the pipeline. The transformed data are finally passed to the final estimator that calls predict method. Only valid if the final estimator implements predict .","title":"predict()"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.predict--parameters","text":"X : iterable Data to predict on. Must fulfill input requirements of first step of the pipeline. **predict_params : dict of string -> object Parameters to the predict called at the end of all transformations in the pipeline. Note that while this may be used to return uncertainties from some models with return_std or return_cov, uncertainties that are generated by the transformations in the pipeline are not propagated to the final estimator. .. versionadded:: 0.20","title":"Parameters"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.predict--returns","text":"y_pred : ndarray Result of calling predict on the final estimator.","title":"Returns"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.predict_log_proba","text":"Transform the data, and apply predict_log_proba with the final estimator. Call transform of each transformer in the pipeline. The transformed data are finally passed to the final estimator that calls predict_log_proba method. Only valid if the final estimator implements predict_log_proba .","title":"predict_log_proba()"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.predict_log_proba--parameters","text":"X : iterable Data to predict on. Must fulfill input requirements of first step of the pipeline. **predict_log_proba_params : dict of string -> object Parameters to the predict_log_proba called at the end of all transformations in the pipeline.","title":"Parameters"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.predict_log_proba--returns","text":"y_log_proba : ndarray of shape (n_samples, n_classes) Result of calling predict_log_proba on the final estimator.","title":"Returns"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.predict_proba","text":"Transform the data, and apply predict_proba with the final estimator. Call transform of each transformer in the pipeline. The transformed data are finally passed to the final estimator that calls predict_proba method. Only valid if the final estimator implements predict_proba .","title":"predict_proba()"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.predict_proba--parameters","text":"X : iterable Data to predict on. Must fulfill input requirements of first step of the pipeline. **predict_proba_params : dict of string -> object Parameters to the predict_proba called at the end of all transformations in the pipeline.","title":"Parameters"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.predict_proba--returns","text":"y_proba : ndarray of shape (n_samples, n_classes) Result of calling predict_proba on the final estimator.","title":"Returns"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.save_yml","text":"Save the features bucket to a yaml file. Parameters: Name Type Description Default fout file output required","title":"save_yml()"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.score","text":"Transform the data, and apply score with the final estimator. Call transform of each transformer in the pipeline. The transformed data are finally passed to the final estimator that calls score method. Only valid if the final estimator implements score .","title":"score()"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.score--parameters","text":"X : iterable Data to predict on. Must fulfill input requirements of first step of the pipeline. y : iterable, default=None Targets used for scoring. Must fulfill label requirements for all steps of the pipeline. sample_weight : array-like, default=None If not None, this argument is passed as sample_weight keyword argument to the score method of the final estimator.","title":"Parameters"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.score--returns","text":"score : float Result of calling score on the final estimator.","title":"Returns"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.score_samples","text":"Transform the data, and apply score_samples with the final estimator. Call transform of each transformer in the pipeline. The transformed data are finally passed to the final estimator that calls score_samples method. Only valid if the final estimator implements score_samples .","title":"score_samples()"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.score_samples--parameters","text":"X : iterable Data to predict on. Must fulfill input requirements of first step of the pipeline.","title":"Parameters"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.score_samples--returns","text":"y_score : ndarray of shape (n_samples,) Result of calling score_samples on the final estimator.","title":"Returns"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.set_params","text":"Set the parameters of this estimator. Valid parameter keys can be listed with get_params() . Note that you can directly set the parameters of the estimators contained in steps .","title":"set_params()"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.set_params--parameters","text":"**kwargs : dict Parameters of this estimator or parameters of estimators contained in steps . Parameters of the steps may be set using its name and the parameter name separated by a '__'.","title":"Parameters"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.set_params--returns","text":"self : object Pipeline class instance.","title":"Returns"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.summary","text":"Display a summary table for columns passed to .fit() . The format is the following: column num_prebuckets num_buckets dtype LIMIT_BAL 15 10 float64 BILL_AMT1 15 6 float64","title":"summary()"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.transform","text":"Transform the data, and apply transform with the final estimator. Call transform of each transformer in the pipeline. The transformed data are finally passed to the final estimator that calls transform method. Only valid if the final estimator implements transform . This also works where final estimator is None in which case all prior transformations are applied.","title":"transform()"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.transform--parameters","text":"X : iterable Data to transform. Must fulfill input requirements of first step of the pipeline.","title":"Parameters"},{"location":"api/pipeline/SkorecardPipeline/#skorecard.pipeline.pipeline.SkorecardPipeline.transform--returns","text":"Xt : ndarray of shape (n_samples, n_transformed_features) Transformed data.","title":"Returns"},{"location":"api/pipeline/to_skorecard_pipeline/","text":"Transform a scikit-learn Pipeline to a SkorecardPipeline. A SkorecardPipeline is a normal scikit-learn pipeline with some extra methods and attributes. Examples: from skorecard.pipeline.pipeline import SkorecardPipeline , to_skorecard_pipeline from skorecard.bucketers import DecisionTreeBucketer , OrdinalCategoricalBucketer from skorecard import datasets from sklearn.pipeline import make_pipeline pipe = make_pipeline ( DecisionTreeBucketer ( variables = [ \"LIMIT_BAL\" , \"BILL_AMT1\" ], max_n_bins = 5 ), OrdinalCategoricalBucketer ( variables = [ \"EDUCATION\" , \"MARRIAGE\" ], tol = 0.05 ) ) sk_pipe = to_skorecard_pipeline ( pipe ) df = datasets . load_uci_credit_card ( as_frame = True ) features = [ \"LIMIT_BAL\" , \"BILL_AMT1\" , \"EDUCATION\" , \"MARRIAGE\" ] X = df [ features ] y = df [ \"default\" ] . values Parameters: Name Type Description Default pipeline Pipeline scikit-learn pipeline instance. required Returns: Type Description pipeline (skorecard.pipeline.SkorecardPipeline) modified pipeline instance.","title":"To skorecard pipeline"},{"location":"api/preprocessing/ColumnSelector/","text":"Transformer that performs selection of variables from a pandas dataframe. Useful in pipelines, where we require a step that selects feautures. Examples: from skorecard import datasets from skorecard.preprocessing import ColumnSelector X , y = datasets . load_uci_credit_card ( return_X_y = True ) cs = ColumnSelector ( variables = [ 'EDUCATION' ]) assert cs . fit_transform ( X , y ) . columns == [ 'EDUCATION' ] __init__ ( self , variables = []) special \u00b6 Transformer constructor. Parameters: Name Type Description Default variables List list of columns to select. Default value is set to None - in this case, there is no selection of columns. [] fit ( self , X , y = None ) \u00b6 Fit the transformer. Here to be compliant with the sklearn API, does not fit anything. fit_transform ( self , X , y = None , ** fit_params ) inherited \u00b6 Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X . Parameters \u00b6 X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters. Returns \u00b6 X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params ( self , deep = True ) inherited \u00b6 Get parameters for this estimator. Parameters \u00b6 deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns \u00b6 params : dict Parameter names mapped to their values. set_params ( self , ** params ) inherited \u00b6 Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class: ~sklearn.pipeline.Pipeline ). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters \u00b6 **params : dict Estimator parameters. Returns \u00b6 self : estimator instance Estimator instance. transform ( self , X ) \u00b6 Selects the columns. Parameters: Name Type Description Default X pd.DataFrame Dataset required","title":"ColumnSelector"},{"location":"api/preprocessing/ColumnSelector/#skorecard.preprocessing.preprocessing.ColumnSelector.__init__","text":"Transformer constructor. Parameters: Name Type Description Default variables List list of columns to select. Default value is set to None - in this case, there is no selection of columns. []","title":"__init__()"},{"location":"api/preprocessing/ColumnSelector/#skorecard.preprocessing.preprocessing.ColumnSelector.fit","text":"Fit the transformer. Here to be compliant with the sklearn API, does not fit anything.","title":"fit()"},{"location":"api/preprocessing/ColumnSelector/#skorecard.preprocessing.preprocessing.ColumnSelector.fit_transform","text":"Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X .","title":"fit_transform()"},{"location":"api/preprocessing/ColumnSelector/#skorecard.preprocessing.preprocessing.ColumnSelector.fit_transform--parameters","text":"X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters.","title":"Parameters"},{"location":"api/preprocessing/ColumnSelector/#skorecard.preprocessing.preprocessing.ColumnSelector.fit_transform--returns","text":"X_new : ndarray array of shape (n_samples, n_features_new) Transformed array.","title":"Returns"},{"location":"api/preprocessing/ColumnSelector/#skorecard.preprocessing.preprocessing.ColumnSelector.get_params","text":"Get parameters for this estimator.","title":"get_params()"},{"location":"api/preprocessing/ColumnSelector/#skorecard.preprocessing.preprocessing.ColumnSelector.get_params--parameters","text":"deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators.","title":"Parameters"},{"location":"api/preprocessing/ColumnSelector/#skorecard.preprocessing.preprocessing.ColumnSelector.get_params--returns","text":"params : dict Parameter names mapped to their values.","title":"Returns"},{"location":"api/preprocessing/ColumnSelector/#skorecard.preprocessing.preprocessing.ColumnSelector.set_params","text":"Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class: ~sklearn.pipeline.Pipeline ). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object.","title":"set_params()"},{"location":"api/preprocessing/ColumnSelector/#skorecard.preprocessing.preprocessing.ColumnSelector.set_params--parameters","text":"**params : dict Estimator parameters.","title":"Parameters"},{"location":"api/preprocessing/ColumnSelector/#skorecard.preprocessing.preprocessing.ColumnSelector.set_params--returns","text":"self : estimator instance Estimator instance.","title":"Returns"},{"location":"api/preprocessing/ColumnSelector/#skorecard.preprocessing.preprocessing.ColumnSelector.transform","text":"Selects the columns. Parameters: Name Type Description Default X pd.DataFrame Dataset required","title":"transform()"},{"location":"api/preprocessing/WoeEncoder/","text":"Transformer that encodes unique values in features to their Weight of Evidence estimation. This class has been deprecated in favor of category_encoders.woe.WOEEncoder Only works for binary classification (target y has 0 and 1 values). The weight of evidence is given by: np.log( p(1) / p(0) ) The target probability ratio is given by: p(1) / p(0) For example in the variable colour, if the mean of the target = 1 for blue is 0.8 and the mean of the target = 0 is 0.2, blue will be replaced by: np.log(0.8/0.2) = 1.386 if log_ratio is selected. Alternatively, blue will be replaced by 0.8 / 0.2 = 4 if ratio is selected. More formally: for each unique value \ud835\udc65, consider the corresponding rows in the training set compute what percentage of positives is in these rows, compared to the whole set compute what percentage of negatives is in these rows, compared to the whole set take the ratio of these percentages take the natural logarithm of that ratio to get the weight of evidence corresponding to \ud835\udc65, so that \ud835\udc4a\ud835\udc42\ud835\udc38(\ud835\udc65) is either positive or negative according to whether \ud835\udc65 is more representative of positives or negatives More details: blogpost on weight of evidence Examples: from skorecard import datasets from skorecard.preprocessing import WoeEncoder X , y = datasets . load_uci_credit_card ( return_X_y = True ) we = WoeEncoder ( variables = [ 'EDUCATION' ]) we . fit_transform ( X , y ) we . fit_transform ( X , y )[ 'EDUCATION' ] . value_counts () Credits: Some inspiration taken from feature_engine.categorical_encoders . __init__ ( self , epsilon = 0.0001 , variables = [], handle_unknown = 'value' ) special \u00b6 Constructor for WoEEncoder. Parameters: Name Type Description Default epsilon float Amount to be added to relative counts in order to avoid division by zero in the WOE calculation. 0.0001 variables list The features to bucket. Uses all features if not defined. [] handle_unknown str How to handle any new values encountered in X on transform(). options are 'return_nan', 'error' and 'value', defaults to 'value', which will assume WOE=0. 'value' fit ( self , X , y ) \u00b6 Calculate the WOE for every column. Parameters: Name Type Description Default X np.array (binned) features required y np.array target required fit_transform ( self , X , y = None , ** fit_params ) inherited \u00b6 Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X . Parameters \u00b6 X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters. Returns \u00b6 X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params ( self , deep = True ) inherited \u00b6 Get parameters for this estimator. Parameters \u00b6 deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns \u00b6 params : dict Parameter names mapped to their values. set_params ( self , ** params ) inherited \u00b6 Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class: ~sklearn.pipeline.Pipeline ). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters \u00b6 **params : dict Estimator parameters. Returns \u00b6 self : estimator instance Estimator instance. transform ( self , X ) \u00b6 Transform X to weight of evidence encoding. Parameters: Name Type Description Default X pd.DataFrame dataset required","title":"WoeEncoder"},{"location":"api/preprocessing/WoeEncoder/#skorecard.preprocessing._WoEEncoder.WoeEncoder.__init__","text":"Constructor for WoEEncoder. Parameters: Name Type Description Default epsilon float Amount to be added to relative counts in order to avoid division by zero in the WOE calculation. 0.0001 variables list The features to bucket. Uses all features if not defined. [] handle_unknown str How to handle any new values encountered in X on transform(). options are 'return_nan', 'error' and 'value', defaults to 'value', which will assume WOE=0. 'value'","title":"__init__()"},{"location":"api/preprocessing/WoeEncoder/#skorecard.preprocessing._WoEEncoder.WoeEncoder.fit","text":"Calculate the WOE for every column. Parameters: Name Type Description Default X np.array (binned) features required y np.array target required","title":"fit()"},{"location":"api/preprocessing/WoeEncoder/#skorecard.preprocessing._WoEEncoder.WoeEncoder.fit_transform","text":"Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X .","title":"fit_transform()"},{"location":"api/preprocessing/WoeEncoder/#skorecard.preprocessing._WoEEncoder.WoeEncoder.fit_transform--parameters","text":"X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters.","title":"Parameters"},{"location":"api/preprocessing/WoeEncoder/#skorecard.preprocessing._WoEEncoder.WoeEncoder.fit_transform--returns","text":"X_new : ndarray array of shape (n_samples, n_features_new) Transformed array.","title":"Returns"},{"location":"api/preprocessing/WoeEncoder/#skorecard.preprocessing._WoEEncoder.WoeEncoder.get_params","text":"Get parameters for this estimator.","title":"get_params()"},{"location":"api/preprocessing/WoeEncoder/#skorecard.preprocessing._WoEEncoder.WoeEncoder.get_params--parameters","text":"deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators.","title":"Parameters"},{"location":"api/preprocessing/WoeEncoder/#skorecard.preprocessing._WoEEncoder.WoeEncoder.get_params--returns","text":"params : dict Parameter names mapped to their values.","title":"Returns"},{"location":"api/preprocessing/WoeEncoder/#skorecard.preprocessing._WoEEncoder.WoeEncoder.set_params","text":"Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class: ~sklearn.pipeline.Pipeline ). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object.","title":"set_params()"},{"location":"api/preprocessing/WoeEncoder/#skorecard.preprocessing._WoEEncoder.WoeEncoder.set_params--parameters","text":"**params : dict Estimator parameters.","title":"Parameters"},{"location":"api/preprocessing/WoeEncoder/#skorecard.preprocessing._WoEEncoder.WoeEncoder.set_params--returns","text":"self : estimator instance Estimator instance.","title":"Returns"},{"location":"api/preprocessing/WoeEncoder/#skorecard.preprocessing._WoEEncoder.WoeEncoder.transform","text":"Transform X to weight of evidence encoding. Parameters: Name Type Description Default X pd.DataFrame dataset required","title":"transform()"},{"location":"api/reporting/iv/","text":"Information Value \u00b6 Calculate the Information Value (IV) of the features in X . X must be the output of fitted bucketers. \\[ IV = \\sum { (\\% goods - \\% bads) } * { WOE } \\] \\[ WOE=\\ln (\\% { goods } / \\% { bads }) \\] Examples: from skorecard import datasets from sklearn.model_selection import train_test_split from skorecard.bucketers import DecisionTreeBucketer from skorecard.reporting import iv X , y = datasets . load_uci_credit_card ( return_X_y = True ) dbt = DecisionTreeBucketer () X_bins = dbt . fit_transform ( X , y ) iv_dict = iv ( X_bins , y ) Parameters: Name Type Description Default X DataFrame pd.DataFrame (bucketed) features required y Series pd.Series: target values required epsilon float Amount to be added to relative counts in order to avoid division by zero in the WOE calculation. 0.0001 digits int number of significant decimal digits in the IV calculation None Returns: Type Description IVs (dict) Keys are feature names, values are the IV values","title":"Information Value"},{"location":"api/reporting/iv/#information-value","text":"Calculate the Information Value (IV) of the features in X . X must be the output of fitted bucketers. \\[ IV = \\sum { (\\% goods - \\% bads) } * { WOE } \\] \\[ WOE=\\ln (\\% { goods } / \\% { bads }) \\] Examples: from skorecard import datasets from sklearn.model_selection import train_test_split from skorecard.bucketers import DecisionTreeBucketer from skorecard.reporting import iv X , y = datasets . load_uci_credit_card ( return_X_y = True ) dbt = DecisionTreeBucketer () X_bins = dbt . fit_transform ( X , y ) iv_dict = iv ( X_bins , y ) Parameters: Name Type Description Default X DataFrame pd.DataFrame (bucketed) features required y Series pd.Series: target values required epsilon float Amount to be added to relative counts in order to avoid division by zero in the WOE calculation. 0.0001 digits int number of significant decimal digits in the IV calculation None Returns: Type Description IVs (dict) Keys are feature names, values are the IV values","title":"Information Value"},{"location":"api/reporting/psi/","text":"Population Stability Index \u00b6 Calculate the PSI between the features in two dataframes, X1 and X2 . X1 and X2 should be bucketed (outputs of fitted bucketers). \\[ PSI = \\sum((\\%{ Good } - \\%{ Bad }) imes \\ln rac{\\%{ Good }}{\\%{ Bad }}) \\] Parameters: Name Type Description Default X1 pd.DataFrame bucketed features, expected required X2 pd.DataFrame bucketed features, actual data required epsilon float Amount to be added to relative counts in order to avoid division by zero in the WOE calculation. 0.0001 digits (int): number of significant decimal digits in the IV calculation None Examples: from skorecard import datasets from sklearn.model_selection import train_test_split from skorecard.bucketers import DecisionTreeBucketer from skorecard.reporting import psi X , y = datasets . load_uci_credit_card ( return_X_y = True ) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.25 , random_state = 42 ) dbt = DecisionTreeBucketer () X_train_bins = dbt . fit_transform ( X_train , y_train ) X_test_bins = dbt . transform ( X_test ) psi_dict = psi ( X_train_bins , X_test_bins )","title":"Population Stability Index"},{"location":"api/reporting/psi/#population-stability-index","text":"Calculate the PSI between the features in two dataframes, X1 and X2 . X1 and X2 should be bucketed (outputs of fitted bucketers). \\[ PSI = \\sum((\\%{ Good } - \\%{ Bad }) imes \\ln rac{\\%{ Good }}{\\%{ Bad }}) \\] Parameters: Name Type Description Default X1 pd.DataFrame bucketed features, expected required X2 pd.DataFrame bucketed features, actual data required epsilon float Amount to be added to relative counts in order to avoid division by zero in the WOE calculation. 0.0001 digits (int): number of significant decimal digits in the IV calculation None Examples: from skorecard import datasets from sklearn.model_selection import train_test_split from skorecard.bucketers import DecisionTreeBucketer from skorecard.reporting import psi X , y = datasets . load_uci_credit_card ( return_X_y = True ) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.25 , random_state = 42 ) dbt = DecisionTreeBucketer () X_train_bins = dbt . fit_transform ( X_train , y_train ) X_test_bins = dbt . transform ( X_test ) psi_dict = psi ( X_train_bins , X_test_bins )","title":"Population Stability Index"},{"location":"api/rescale/rescale/","text":"ScoreCardPoints \u00b6 Transformer to map the the buckets from the skorecard model and maps them to the rescaled points. Examples: from skorecard import Skorecard from skorecard.rescale import ScoreCardPoints from skorecard.datasets import load_uci_credit_card X , y = load_uci_credit_card ( return_X_y = True ) model = Skorecard ( variables = [ \"LIMIT_BAL\" , \"BILL_AMT1\" , \"EDUCATION\" , \"MARRIAGE\" ]) model . fit ( X , y ) scp = ScoreCardPoints ( model ) scp . transform ( X ) __init__ ( self , skorecard_model , * , pdo = 20 , ref_score = 100 , ref_odds = 1 ) special \u00b6 Parameters: Name Type Description Default skorecard_model the fitted Skorecard class required pdo number of points necessary to double the odds 20 ref_score reference score set for the reference odds 100 ref_odds odds that correspond to the ref_score 1 fit_transform ( self , X , y = None , ** fit_params ) inherited \u00b6 Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X . Parameters \u00b6 X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters. Returns \u00b6 X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params ( self , deep = True ) inherited \u00b6 Get parameters for this estimator. Parameters \u00b6 deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns \u00b6 params : dict Parameter names mapped to their values. get_scorecard_points ( self ) \u00b6 Get the scorecard points. set_params ( self , ** params ) inherited \u00b6 Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class: ~sklearn.pipeline.Pipeline ). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object. Parameters \u00b6 **params : dict Estimator parameters. Returns \u00b6 self : estimator instance Estimator instance. transform ( self , X ) \u00b6 Transform the features to the points.","title":"ScoreCardPoints"},{"location":"api/rescale/rescale/#scorecardpoints","text":"Transformer to map the the buckets from the skorecard model and maps them to the rescaled points. Examples: from skorecard import Skorecard from skorecard.rescale import ScoreCardPoints from skorecard.datasets import load_uci_credit_card X , y = load_uci_credit_card ( return_X_y = True ) model = Skorecard ( variables = [ \"LIMIT_BAL\" , \"BILL_AMT1\" , \"EDUCATION\" , \"MARRIAGE\" ]) model . fit ( X , y ) scp = ScoreCardPoints ( model ) scp . transform ( X )","title":"ScoreCardPoints"},{"location":"api/rescale/rescale/#skorecard.rescale.rescale.ScoreCardPoints.__init__","text":"Parameters: Name Type Description Default skorecard_model the fitted Skorecard class required pdo number of points necessary to double the odds 20 ref_score reference score set for the reference odds 100 ref_odds odds that correspond to the ref_score 1","title":"__init__()"},{"location":"api/rescale/rescale/#skorecard.rescale.rescale.ScoreCardPoints.fit_transform","text":"Fit to data, then transform it. Fits transformer to X and y with optional parameters fit_params and returns a transformed version of X .","title":"fit_transform()"},{"location":"api/rescale/rescale/#skorecard.rescale.rescale.ScoreCardPoints.fit_transform--parameters","text":"X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters.","title":"Parameters"},{"location":"api/rescale/rescale/#skorecard.rescale.rescale.ScoreCardPoints.fit_transform--returns","text":"X_new : ndarray array of shape (n_samples, n_features_new) Transformed array.","title":"Returns"},{"location":"api/rescale/rescale/#skorecard.rescale.rescale.ScoreCardPoints.get_params","text":"Get parameters for this estimator.","title":"get_params()"},{"location":"api/rescale/rescale/#skorecard.rescale.rescale.ScoreCardPoints.get_params--parameters","text":"deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators.","title":"Parameters"},{"location":"api/rescale/rescale/#skorecard.rescale.rescale.ScoreCardPoints.get_params--returns","text":"params : dict Parameter names mapped to their values.","title":"Returns"},{"location":"api/rescale/rescale/#skorecard.rescale.rescale.ScoreCardPoints.get_scorecard_points","text":"Get the scorecard points.","title":"get_scorecard_points()"},{"location":"api/rescale/rescale/#skorecard.rescale.rescale.ScoreCardPoints.set_params","text":"Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class: ~sklearn.pipeline.Pipeline ). The latter have parameters of the form <component>__<parameter> so that it's possible to update each component of a nested object.","title":"set_params()"},{"location":"api/rescale/rescale/#skorecard.rescale.rescale.ScoreCardPoints.set_params--parameters","text":"**params : dict Estimator parameters.","title":"Parameters"},{"location":"api/rescale/rescale/#skorecard.rescale.rescale.ScoreCardPoints.set_params--returns","text":"self : estimator instance Estimator instance.","title":"Returns"},{"location":"api/rescale/rescale/#skorecard.rescale.rescale.ScoreCardPoints.transform","text":"Transform the features to the points.","title":"transform()"},{"location":"discussion/benchmark_with_EBM/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); EBM benchmark with skorecard \u00b6 This benchmark was adjusted from this notebook # To run benchmark script, you will need to install XGBoost # (pip install XGBoost) import numpy as np import pandas as pd from sklearn.datasets import load_breast_cancer import warnings warnings . filterwarnings ( \"ignore\" ) def load_breast_data (): breast = load_breast_cancer () feature_names = list ( breast . feature_names ) X , y = pd . DataFrame ( breast . data , columns = feature_names ), breast . target dataset = { 'problem' : 'classification' , 'full' : { 'X' : X , 'y' : y , }, } return dataset def load_adult_data (): df = pd . read_csv ( \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\" , header = None ) df . columns = [ \"Age\" , \"WorkClass\" , \"fnlwgt\" , \"Education\" , \"EducationNum\" , \"MaritalStatus\" , \"Occupation\" , \"Relationship\" , \"Race\" , \"Gender\" , \"CapitalGain\" , \"CapitalLoss\" , \"HoursPerWeek\" , \"NativeCountry\" , \"Income\" ] train_cols = df . columns [ 0 : - 1 ] label = df . columns [ - 1 ] X_df = df [ train_cols ] y_df = df [ label ] dataset = { 'problem' : 'classification' , 'full' : { 'X' : X_df , 'y' : y_df , }, } return dataset def load_heart_data (): # https://www.kaggle.com/ronitf/heart-disease-uci df = pd . read_csv ( r 'heart.csv' ) train_cols = df . columns [ 0 : - 1 ] label = df . columns [ - 1 ] X_df = df [ train_cols ] y_df = df [ label ] dataset = { 'problem' : 'classification' , 'full' : { 'X' : X_df , 'y' : y_df , }, } return dataset def load_credit_data (): # https://www.kaggle.com/mlg-ulb/creditcardfraud df = pd . read_csv ( r 'creditcard.csv' ) train_cols = df . columns [ 0 : - 1 ] label = df . columns [ - 1 ] X_df = df [ train_cols ] y_df = df [ label ] dataset = { 'problem' : 'classification' , 'full' : { 'X' : X_df , 'y' : y_df , }, } return dataset def load_telco_churn_data (): # https://www.kaggle.com/blastchar/telco-customer-churn df = pd . read_csv ( r 'WA_Fn-UseC_-Telco-Customer-Churn.csv' ) train_cols = df . columns [ 1 : - 1 ] # First column is an ID label = df . columns [ - 1 ] X_df = df [ train_cols ] y_df = df [ label ] # 'Yes, No' dataset = { 'problem' : 'classification' , 'full' : { 'X' : X_df , 'y' : y_df , }, } return dataset from sklearn.preprocessing import OneHotEncoder , FunctionTransformer , StandardScaler from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.model_selection import StratifiedShuffleSplit , cross_validate from sklearn.ensemble import RandomForestClassifier from sklearn.preprocessing import OrdinalEncoder from xgboost import XGBClassifier from sklearn.linear_model import LogisticRegression from interpret.glassbox import ExplainableBoostingClassifier from skorecard import Skorecard from optbinning import BinningProcess from optbinning import Scorecard def format_n ( x ): return \" {0:.3f} \" . format ( x ) def process_model ( clf , name , X , y , n_splits = 3 ): # Evaluate model ss = StratifiedShuffleSplit ( n_splits = n_splits , test_size = 0.25 , random_state = 1337 ) scores = cross_validate ( clf , X , y , scoring = 'roc_auc' , cv = ss , n_jobs =- 1 , return_estimator = True ) record = dict () record [ 'model_name' ] = name record [ 'fit_time_mean' ] = format_n ( np . mean ( scores [ 'fit_time' ])) record [ 'fit_time_std' ] = format_n ( np . std ( scores [ 'fit_time' ])) record [ 'test_score_mean' ] = format_n ( np . mean ( scores [ 'test_score' ])) record [ 'test_score_std' ] = format_n ( np . std ( scores [ 'test_score' ])) return record def benchmark_models ( dataset_name , X , y , ct = None , n_splits = 3 , random_state = 1337 ): if ct is None : is_cat = np . array ([ dt . kind == 'O' for dt in X . dtypes ]) cat_cols = X . columns . values [ is_cat ] num_cols = X . columns . values [ ~ is_cat ] cat_ohe_step = ( 'ohe' , OneHotEncoder ( sparse = False , handle_unknown = 'ignore' )) cat_pipe = Pipeline ([ cat_ohe_step ]) num_pipe = Pipeline ([( 'identity' , FunctionTransformer ())]) transformers = [ ( 'cat' , cat_pipe , cat_cols ), ( 'num' , num_pipe , num_cols ) ] ct = ColumnTransformer ( transformers = transformers ) cat_ord_step = ( 'ord_enc' , OrdinalEncoder ()) cat_pipe = Pipeline ([ cat_ord_step ]) transformers = [ ( 'cat' , cat_pipe , cat_cols ), ( 'num' , num_pipe , num_cols ) ] ot = ColumnTransformer ( transformers = transformers ) records = [] summary_record = {} summary_record [ 'dataset_name' ] = dataset_name print () print ( '-' * 78 ) print ( dataset_name ) print ( '-' * 78 ) print ( summary_record ) print () pipe = Pipeline ([ ( 'ct' , ct ), ( 'std' , StandardScaler ()), ( 'lr' , LogisticRegression ( random_state = random_state )), ]) record = process_model ( pipe , 'lr_ohe' , X , y , n_splits = n_splits ) print ( record ) record . update ( summary_record ) records . append ( record ) pipe = Pipeline ([ ( 'ot' , ot ), ( 'std' , StandardScaler ()), ( 'lr' , LogisticRegression ( max_iter = 7000 , random_state = random_state )), ]) record = process_model ( pipe , 'lr_ordinal' , X , y , n_splits = n_splits ) print ( record ) record . update ( summary_record ) records . append ( record ) # Skorecard skorecard = Skorecard () record = process_model ( skorecard , 'skorecard' , X , y , n_splits = n_splits ) print ( record ) record . update ( summary_record ) records . append ( record ) pipe = Pipeline ([ ( 'ct' , ct ), # n_estimators updated from 10 to 100 due to sci-kit defaults changing in future versions ( 'rf-100' , RandomForestClassifier ( n_estimators = 100 , n_jobs =- 1 , random_state = random_state )), ]) record = process_model ( pipe , 'rf-100' , X , y , n_splits = n_splits ) print ( record ) record . update ( summary_record ) records . append ( record ) pipe = Pipeline ([ ( 'ct' , ct ), ( 'xgb' , XGBClassifier ( random_state = random_state , eval_metric = 'logloss' )), ]) record = process_model ( pipe , 'xgb' , X , y , n_splits = n_splits ) print ( record ) record . update ( summary_record ) records . append ( record ) # No pipeline needed due to EBM handling string datatypes ebm_inter = ExplainableBoostingClassifier ( n_jobs =- 1 , random_state = random_state ) record = process_model ( ebm_inter , 'ebm' , X , y , n_splits = n_splits ) print ( record ) record . update ( summary_record ) records . append ( record ) return records results = [] n_splits = 3 from skorecard.datasets import load_uci_credit_card X , y = load_uci_credit_card ( return_X_y = True ) result = benchmark_models ( 'UCI-creditcard' , X , y , n_splits = n_splits ) results . append ( result ) ------------------------------------------------------------------------------ UCI-creditcard ------------------------------------------------------------------------------ {'dataset_name': 'UCI-creditcard'} {'model_name': 'lr_ohe', 'fit_time_mean': '0.009', 'fit_time_std': '0.000', 'test_score_mean': '0.621', 'test_score_std': '0.023'} {'model_name': 'lr_ordinal', 'fit_time_mean': '0.008', 'fit_time_std': '0.001', 'test_score_mean': '0.621', 'test_score_std': '0.023'} {'model_name': 'skorecard', 'fit_time_mean': '1.489', 'fit_time_std': '0.044', 'test_score_mean': '0.627', 'test_score_std': '0.018'} {'model_name': 'rf-100', 'fit_time_mean': '0.326', 'fit_time_std': '0.051', 'test_score_mean': '0.588', 'test_score_std': '0.013'} {'model_name': 'xgb', 'fit_time_mean': '0.957', 'fit_time_std': '0.114', 'test_score_mean': '0.596', 'test_score_std': '0.005'} {'model_name': 'ebm', 'fit_time_mean': '1.219', 'fit_time_std': '0.151', 'test_score_mean': '0.644', 'test_score_std': '0.012'} dataset = load_breast_data () result = benchmark_models ( 'breast-cancer' , dataset [ 'full' ][ 'X' ], dataset [ 'full' ][ 'y' ], n_splits = n_splits ) results . append ( result ) ------------------------------------------------------------------------------ breast-cancer ------------------------------------------------------------------------------ {'dataset_name': 'breast-cancer'} {'model_name': 'lr_ohe', 'fit_time_mean': '0.014', 'fit_time_std': '0.001', 'test_score_mean': '0.994', 'test_score_std': '0.006'} {'model_name': 'lr_ordinal', 'fit_time_mean': '0.015', 'fit_time_std': '0.001', 'test_score_mean': '0.994', 'test_score_std': '0.006'} {'model_name': 'skorecard', 'fit_time_mean': '13.034', 'fit_time_std': '0.868', 'test_score_mean': '0.996', 'test_score_std': '0.004'} {'model_name': 'rf-100', 'fit_time_mean': '0.253', 'fit_time_std': '0.002', 'test_score_mean': '0.992', 'test_score_std': '0.009'} {'model_name': 'xgb', 'fit_time_mean': '0.196', 'fit_time_std': '0.014', 'test_score_mean': '0.992', 'test_score_std': '0.010'} {'model_name': 'ebm', 'fit_time_mean': '5.866', 'fit_time_std': '1.070', 'test_score_mean': '0.995', 'test_score_std': '0.006'} dataset = load_adult_data () result = benchmark_models ( 'adult' , dataset [ 'full' ][ 'X' ], dataset [ 'full' ][ 'y' ], n_splits = n_splits ) results . append ( result ) # 0.888 ------------------------------------------------------------------------------ adult ------------------------------------------------------------------------------ {'dataset_name': 'adult'} {'model_name': 'lr_ohe', 'fit_time_mean': '0.527', 'fit_time_std': '0.032', 'test_score_mean': '0.906', 'test_score_std': '0.003'} {'model_name': 'lr_ordinal', 'fit_time_mean': '0.131', 'fit_time_std': '0.003', 'test_score_mean': '0.855', 'test_score_std': '0.002'} {'model_name': 'skorecard', 'fit_time_mean': '8.047', 'fit_time_std': '0.025', 'test_score_mean': '0.888', 'test_score_std': '0.004'} {'model_name': 'rf-100', 'fit_time_mean': '2.445', 'fit_time_std': '0.008', 'test_score_mean': '0.903', 'test_score_std': '0.002'} {'model_name': 'xgb', 'fit_time_mean': '14.403', 'fit_time_std': '0.403', 'test_score_mean': '0.927', 'test_score_std': '0.001'} {'model_name': 'ebm', 'fit_time_mean': '51.093', 'fit_time_std': '1.137', 'test_score_mean': '0.928', 'test_score_std': '0.002'} dataset = load_telco_churn_data () result = benchmark_models ( 'telco_churn' , dataset [ 'full' ][ 'X' ], dataset [ 'full' ][ 'y' ], n_splits = n_splits ) results . append ( result ) ------------------------------------------------------------------------------ telco_churn ------------------------------------------------------------------------------ {'dataset_name': 'telco_churn'} {'model_name': 'lr_ohe', 'fit_time_mean': '9.091', 'fit_time_std': '0.030', 'test_score_mean': '0.809', 'test_score_std': '0.014'} {'model_name': 'lr_ordinal', 'fit_time_mean': '0.064', 'fit_time_std': '0.002', 'test_score_mean': 'nan', 'test_score_std': 'nan'} {'model_name': 'skorecard', 'fit_time_mean': '4.672', 'fit_time_std': '0.069', 'test_score_mean': '0.764', 'test_score_std': '0.014'} {'model_name': 'rf-100', 'fit_time_mean': '7.118', 'fit_time_std': '0.016', 'test_score_mean': '0.824', 'test_score_std': '0.002'} {'model_name': 'xgb', 'fit_time_mean': '104.738', 'fit_time_std': '0.969', 'test_score_mean': '0.825', 'test_score_std': '0.003'} {'model_name': 'ebm', 'fit_time_mean': '36.052', 'fit_time_std': '3.779', 'test_score_mean': '0.852', 'test_score_std': '0.004'} dataset = load_heart_data () result = benchmark_models ( 'heart' , dataset [ 'full' ][ 'X' ], dataset [ 'full' ][ 'y' ], n_splits = n_splits ) results . append ( result ) ------------------------------------------------------------------------------ heart ------------------------------------------------------------------------------ {'dataset_name': 'heart'} {'model_name': 'lr_ohe', 'fit_time_mean': '0.007', 'fit_time_std': '0.001', 'test_score_mean': '0.895', 'test_score_std': '0.030'} {'model_name': 'lr_ordinal', 'fit_time_mean': '0.007', 'fit_time_std': '0.000', 'test_score_mean': '0.895', 'test_score_std': '0.030'} {'model_name': 'skorecard', 'fit_time_mean': '2.148', 'fit_time_std': '0.013', 'test_score_mean': '0.911', 'test_score_std': '0.015'} {'model_name': 'rf-100', 'fit_time_mean': '0.241', 'fit_time_std': '0.003', 'test_score_mean': '0.890', 'test_score_std': '0.008'} {'model_name': 'xgb', 'fit_time_mean': '0.318', 'fit_time_std': '0.035', 'test_score_mean': '0.851', 'test_score_std': '0.018'} {'model_name': 'ebm', 'fit_time_mean': '1.925', 'fit_time_std': '0.392', 'test_score_mean': '0.906', 'test_score_std': '0.011'} records = [ item for result in results for item in result ] record_df = pd . DataFrame . from_records ( records )[[ 'dataset_name' , 'model_name' , 'test_score_mean' , 'test_score_std' ]] record_df = record_df . sort_values ([ 'dataset_name' , 'test_score_mean' ], ascending = False ) print ( record_df [ record_df [ 'model_name' ] . isin ([ 'lr_ohe' , 'lr_ordinal' , 'rf-100' , 'skorecard' , 'xgb' ])] . drop ([ 'test_score_std' ], axis = 1 ) . to_markdown ( tablefmt = \"github\" , showindex = False )) | dataset_name | model_name | test_score_mean | |----------------|--------------|-------------------| | telco_churn | lr_ordinal | nan | | telco_churn | xgb | 0.825 | | telco_churn | rf-100 | 0.824 | | telco_churn | lr_ohe | 0.809 | | telco_churn | skorecard | 0.764 | | heart | skorecard | 0.911 | | heart | lr_ohe | 0.895 | | heart | lr_ordinal | 0.895 | | heart | rf-100 | 0.89 | | heart | xgb | 0.851 | | breast-cancer | skorecard | 0.996 | | breast-cancer | lr_ohe | 0.994 | | breast-cancer | lr_ordinal | 0.994 | | breast-cancer | rf-100 | 0.992 | | breast-cancer | xgb | 0.992 | | adult | xgb | 0.927 | | adult | lr_ohe | 0.906 | | adult | rf-100 | 0.903 | | adult | skorecard | 0.888 | | adult | lr_ordinal | 0.855 | | UCI-creditcard | skorecard | 0.627 | | UCI-creditcard | lr_ohe | 0.621 | | UCI-creditcard | lr_ordinal | 0.621 | | UCI-creditcard | xgb | 0.596 | | UCI-creditcard | rf-100 | 0.588 |","title":"Benchmarks with Explainable Boosting Classifier"},{"location":"discussion/benchmark_with_EBM/#ebm-benchmark-with-skorecard","text":"This benchmark was adjusted from this notebook # To run benchmark script, you will need to install XGBoost # (pip install XGBoost) import numpy as np import pandas as pd from sklearn.datasets import load_breast_cancer import warnings warnings . filterwarnings ( \"ignore\" ) def load_breast_data (): breast = load_breast_cancer () feature_names = list ( breast . feature_names ) X , y = pd . DataFrame ( breast . data , columns = feature_names ), breast . target dataset = { 'problem' : 'classification' , 'full' : { 'X' : X , 'y' : y , }, } return dataset def load_adult_data (): df = pd . read_csv ( \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\" , header = None ) df . columns = [ \"Age\" , \"WorkClass\" , \"fnlwgt\" , \"Education\" , \"EducationNum\" , \"MaritalStatus\" , \"Occupation\" , \"Relationship\" , \"Race\" , \"Gender\" , \"CapitalGain\" , \"CapitalLoss\" , \"HoursPerWeek\" , \"NativeCountry\" , \"Income\" ] train_cols = df . columns [ 0 : - 1 ] label = df . columns [ - 1 ] X_df = df [ train_cols ] y_df = df [ label ] dataset = { 'problem' : 'classification' , 'full' : { 'X' : X_df , 'y' : y_df , }, } return dataset def load_heart_data (): # https://www.kaggle.com/ronitf/heart-disease-uci df = pd . read_csv ( r 'heart.csv' ) train_cols = df . columns [ 0 : - 1 ] label = df . columns [ - 1 ] X_df = df [ train_cols ] y_df = df [ label ] dataset = { 'problem' : 'classification' , 'full' : { 'X' : X_df , 'y' : y_df , }, } return dataset def load_credit_data (): # https://www.kaggle.com/mlg-ulb/creditcardfraud df = pd . read_csv ( r 'creditcard.csv' ) train_cols = df . columns [ 0 : - 1 ] label = df . columns [ - 1 ] X_df = df [ train_cols ] y_df = df [ label ] dataset = { 'problem' : 'classification' , 'full' : { 'X' : X_df , 'y' : y_df , }, } return dataset def load_telco_churn_data (): # https://www.kaggle.com/blastchar/telco-customer-churn df = pd . read_csv ( r 'WA_Fn-UseC_-Telco-Customer-Churn.csv' ) train_cols = df . columns [ 1 : - 1 ] # First column is an ID label = df . columns [ - 1 ] X_df = df [ train_cols ] y_df = df [ label ] # 'Yes, No' dataset = { 'problem' : 'classification' , 'full' : { 'X' : X_df , 'y' : y_df , }, } return dataset from sklearn.preprocessing import OneHotEncoder , FunctionTransformer , StandardScaler from sklearn.compose import ColumnTransformer from sklearn.pipeline import Pipeline from sklearn.model_selection import StratifiedShuffleSplit , cross_validate from sklearn.ensemble import RandomForestClassifier from sklearn.preprocessing import OrdinalEncoder from xgboost import XGBClassifier from sklearn.linear_model import LogisticRegression from interpret.glassbox import ExplainableBoostingClassifier from skorecard import Skorecard from optbinning import BinningProcess from optbinning import Scorecard def format_n ( x ): return \" {0:.3f} \" . format ( x ) def process_model ( clf , name , X , y , n_splits = 3 ): # Evaluate model ss = StratifiedShuffleSplit ( n_splits = n_splits , test_size = 0.25 , random_state = 1337 ) scores = cross_validate ( clf , X , y , scoring = 'roc_auc' , cv = ss , n_jobs =- 1 , return_estimator = True ) record = dict () record [ 'model_name' ] = name record [ 'fit_time_mean' ] = format_n ( np . mean ( scores [ 'fit_time' ])) record [ 'fit_time_std' ] = format_n ( np . std ( scores [ 'fit_time' ])) record [ 'test_score_mean' ] = format_n ( np . mean ( scores [ 'test_score' ])) record [ 'test_score_std' ] = format_n ( np . std ( scores [ 'test_score' ])) return record def benchmark_models ( dataset_name , X , y , ct = None , n_splits = 3 , random_state = 1337 ): if ct is None : is_cat = np . array ([ dt . kind == 'O' for dt in X . dtypes ]) cat_cols = X . columns . values [ is_cat ] num_cols = X . columns . values [ ~ is_cat ] cat_ohe_step = ( 'ohe' , OneHotEncoder ( sparse = False , handle_unknown = 'ignore' )) cat_pipe = Pipeline ([ cat_ohe_step ]) num_pipe = Pipeline ([( 'identity' , FunctionTransformer ())]) transformers = [ ( 'cat' , cat_pipe , cat_cols ), ( 'num' , num_pipe , num_cols ) ] ct = ColumnTransformer ( transformers = transformers ) cat_ord_step = ( 'ord_enc' , OrdinalEncoder ()) cat_pipe = Pipeline ([ cat_ord_step ]) transformers = [ ( 'cat' , cat_pipe , cat_cols ), ( 'num' , num_pipe , num_cols ) ] ot = ColumnTransformer ( transformers = transformers ) records = [] summary_record = {} summary_record [ 'dataset_name' ] = dataset_name print () print ( '-' * 78 ) print ( dataset_name ) print ( '-' * 78 ) print ( summary_record ) print () pipe = Pipeline ([ ( 'ct' , ct ), ( 'std' , StandardScaler ()), ( 'lr' , LogisticRegression ( random_state = random_state )), ]) record = process_model ( pipe , 'lr_ohe' , X , y , n_splits = n_splits ) print ( record ) record . update ( summary_record ) records . append ( record ) pipe = Pipeline ([ ( 'ot' , ot ), ( 'std' , StandardScaler ()), ( 'lr' , LogisticRegression ( max_iter = 7000 , random_state = random_state )), ]) record = process_model ( pipe , 'lr_ordinal' , X , y , n_splits = n_splits ) print ( record ) record . update ( summary_record ) records . append ( record ) # Skorecard skorecard = Skorecard () record = process_model ( skorecard , 'skorecard' , X , y , n_splits = n_splits ) print ( record ) record . update ( summary_record ) records . append ( record ) pipe = Pipeline ([ ( 'ct' , ct ), # n_estimators updated from 10 to 100 due to sci-kit defaults changing in future versions ( 'rf-100' , RandomForestClassifier ( n_estimators = 100 , n_jobs =- 1 , random_state = random_state )), ]) record = process_model ( pipe , 'rf-100' , X , y , n_splits = n_splits ) print ( record ) record . update ( summary_record ) records . append ( record ) pipe = Pipeline ([ ( 'ct' , ct ), ( 'xgb' , XGBClassifier ( random_state = random_state , eval_metric = 'logloss' )), ]) record = process_model ( pipe , 'xgb' , X , y , n_splits = n_splits ) print ( record ) record . update ( summary_record ) records . append ( record ) # No pipeline needed due to EBM handling string datatypes ebm_inter = ExplainableBoostingClassifier ( n_jobs =- 1 , random_state = random_state ) record = process_model ( ebm_inter , 'ebm' , X , y , n_splits = n_splits ) print ( record ) record . update ( summary_record ) records . append ( record ) return records results = [] n_splits = 3 from skorecard.datasets import load_uci_credit_card X , y = load_uci_credit_card ( return_X_y = True ) result = benchmark_models ( 'UCI-creditcard' , X , y , n_splits = n_splits ) results . append ( result ) ------------------------------------------------------------------------------ UCI-creditcard ------------------------------------------------------------------------------ {'dataset_name': 'UCI-creditcard'} {'model_name': 'lr_ohe', 'fit_time_mean': '0.009', 'fit_time_std': '0.000', 'test_score_mean': '0.621', 'test_score_std': '0.023'} {'model_name': 'lr_ordinal', 'fit_time_mean': '0.008', 'fit_time_std': '0.001', 'test_score_mean': '0.621', 'test_score_std': '0.023'} {'model_name': 'skorecard', 'fit_time_mean': '1.489', 'fit_time_std': '0.044', 'test_score_mean': '0.627', 'test_score_std': '0.018'} {'model_name': 'rf-100', 'fit_time_mean': '0.326', 'fit_time_std': '0.051', 'test_score_mean': '0.588', 'test_score_std': '0.013'} {'model_name': 'xgb', 'fit_time_mean': '0.957', 'fit_time_std': '0.114', 'test_score_mean': '0.596', 'test_score_std': '0.005'} {'model_name': 'ebm', 'fit_time_mean': '1.219', 'fit_time_std': '0.151', 'test_score_mean': '0.644', 'test_score_std': '0.012'} dataset = load_breast_data () result = benchmark_models ( 'breast-cancer' , dataset [ 'full' ][ 'X' ], dataset [ 'full' ][ 'y' ], n_splits = n_splits ) results . append ( result ) ------------------------------------------------------------------------------ breast-cancer ------------------------------------------------------------------------------ {'dataset_name': 'breast-cancer'} {'model_name': 'lr_ohe', 'fit_time_mean': '0.014', 'fit_time_std': '0.001', 'test_score_mean': '0.994', 'test_score_std': '0.006'} {'model_name': 'lr_ordinal', 'fit_time_mean': '0.015', 'fit_time_std': '0.001', 'test_score_mean': '0.994', 'test_score_std': '0.006'} {'model_name': 'skorecard', 'fit_time_mean': '13.034', 'fit_time_std': '0.868', 'test_score_mean': '0.996', 'test_score_std': '0.004'} {'model_name': 'rf-100', 'fit_time_mean': '0.253', 'fit_time_std': '0.002', 'test_score_mean': '0.992', 'test_score_std': '0.009'} {'model_name': 'xgb', 'fit_time_mean': '0.196', 'fit_time_std': '0.014', 'test_score_mean': '0.992', 'test_score_std': '0.010'} {'model_name': 'ebm', 'fit_time_mean': '5.866', 'fit_time_std': '1.070', 'test_score_mean': '0.995', 'test_score_std': '0.006'} dataset = load_adult_data () result = benchmark_models ( 'adult' , dataset [ 'full' ][ 'X' ], dataset [ 'full' ][ 'y' ], n_splits = n_splits ) results . append ( result ) # 0.888 ------------------------------------------------------------------------------ adult ------------------------------------------------------------------------------ {'dataset_name': 'adult'} {'model_name': 'lr_ohe', 'fit_time_mean': '0.527', 'fit_time_std': '0.032', 'test_score_mean': '0.906', 'test_score_std': '0.003'} {'model_name': 'lr_ordinal', 'fit_time_mean': '0.131', 'fit_time_std': '0.003', 'test_score_mean': '0.855', 'test_score_std': '0.002'} {'model_name': 'skorecard', 'fit_time_mean': '8.047', 'fit_time_std': '0.025', 'test_score_mean': '0.888', 'test_score_std': '0.004'} {'model_name': 'rf-100', 'fit_time_mean': '2.445', 'fit_time_std': '0.008', 'test_score_mean': '0.903', 'test_score_std': '0.002'} {'model_name': 'xgb', 'fit_time_mean': '14.403', 'fit_time_std': '0.403', 'test_score_mean': '0.927', 'test_score_std': '0.001'} {'model_name': 'ebm', 'fit_time_mean': '51.093', 'fit_time_std': '1.137', 'test_score_mean': '0.928', 'test_score_std': '0.002'} dataset = load_telco_churn_data () result = benchmark_models ( 'telco_churn' , dataset [ 'full' ][ 'X' ], dataset [ 'full' ][ 'y' ], n_splits = n_splits ) results . append ( result ) ------------------------------------------------------------------------------ telco_churn ------------------------------------------------------------------------------ {'dataset_name': 'telco_churn'} {'model_name': 'lr_ohe', 'fit_time_mean': '9.091', 'fit_time_std': '0.030', 'test_score_mean': '0.809', 'test_score_std': '0.014'} {'model_name': 'lr_ordinal', 'fit_time_mean': '0.064', 'fit_time_std': '0.002', 'test_score_mean': 'nan', 'test_score_std': 'nan'} {'model_name': 'skorecard', 'fit_time_mean': '4.672', 'fit_time_std': '0.069', 'test_score_mean': '0.764', 'test_score_std': '0.014'} {'model_name': 'rf-100', 'fit_time_mean': '7.118', 'fit_time_std': '0.016', 'test_score_mean': '0.824', 'test_score_std': '0.002'} {'model_name': 'xgb', 'fit_time_mean': '104.738', 'fit_time_std': '0.969', 'test_score_mean': '0.825', 'test_score_std': '0.003'} {'model_name': 'ebm', 'fit_time_mean': '36.052', 'fit_time_std': '3.779', 'test_score_mean': '0.852', 'test_score_std': '0.004'} dataset = load_heart_data () result = benchmark_models ( 'heart' , dataset [ 'full' ][ 'X' ], dataset [ 'full' ][ 'y' ], n_splits = n_splits ) results . append ( result ) ------------------------------------------------------------------------------ heart ------------------------------------------------------------------------------ {'dataset_name': 'heart'} {'model_name': 'lr_ohe', 'fit_time_mean': '0.007', 'fit_time_std': '0.001', 'test_score_mean': '0.895', 'test_score_std': '0.030'} {'model_name': 'lr_ordinal', 'fit_time_mean': '0.007', 'fit_time_std': '0.000', 'test_score_mean': '0.895', 'test_score_std': '0.030'} {'model_name': 'skorecard', 'fit_time_mean': '2.148', 'fit_time_std': '0.013', 'test_score_mean': '0.911', 'test_score_std': '0.015'} {'model_name': 'rf-100', 'fit_time_mean': '0.241', 'fit_time_std': '0.003', 'test_score_mean': '0.890', 'test_score_std': '0.008'} {'model_name': 'xgb', 'fit_time_mean': '0.318', 'fit_time_std': '0.035', 'test_score_mean': '0.851', 'test_score_std': '0.018'} {'model_name': 'ebm', 'fit_time_mean': '1.925', 'fit_time_std': '0.392', 'test_score_mean': '0.906', 'test_score_std': '0.011'} records = [ item for result in results for item in result ] record_df = pd . DataFrame . from_records ( records )[[ 'dataset_name' , 'model_name' , 'test_score_mean' , 'test_score_std' ]] record_df = record_df . sort_values ([ 'dataset_name' , 'test_score_mean' ], ascending = False ) print ( record_df [ record_df [ 'model_name' ] . isin ([ 'lr_ohe' , 'lr_ordinal' , 'rf-100' , 'skorecard' , 'xgb' ])] . drop ([ 'test_score_std' ], axis = 1 ) . to_markdown ( tablefmt = \"github\" , showindex = False )) | dataset_name | model_name | test_score_mean | |----------------|--------------|-------------------| | telco_churn | lr_ordinal | nan | | telco_churn | xgb | 0.825 | | telco_churn | rf-100 | 0.824 | | telco_churn | lr_ohe | 0.809 | | telco_churn | skorecard | 0.764 | | heart | skorecard | 0.911 | | heart | lr_ohe | 0.895 | | heart | lr_ordinal | 0.895 | | heart | rf-100 | 0.89 | | heart | xgb | 0.851 | | breast-cancer | skorecard | 0.996 | | breast-cancer | lr_ohe | 0.994 | | breast-cancer | lr_ordinal | 0.994 | | breast-cancer | rf-100 | 0.992 | | breast-cancer | xgb | 0.992 | | adult | xgb | 0.927 | | adult | lr_ohe | 0.906 | | adult | rf-100 | 0.903 | | adult | skorecard | 0.888 | | adult | lr_ordinal | 0.855 | | UCI-creditcard | skorecard | 0.627 | | UCI-creditcard | lr_ohe | 0.621 | | UCI-creditcard | lr_ordinal | 0.621 | | UCI-creditcard | xgb | 0.596 | | UCI-creditcard | rf-100 | 0.588 |","title":"EBM benchmark with skorecard"},{"location":"discussion/benchmarks/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Benchmarks \u00b6 Here we will demonstrate some benchmarks against some alternatives. Data \u00b6 UCI Credit card dataset with 30k rows and 23 features. import pandas as pd from skorecard.datasets import load_credit_card from sklearn.model_selection import train_test_split data = load_credit_card ( as_frame = True ) print ( f \"data shape: { data . shape } \" ) X_train , X_test , y_train , y_test = train_test_split ( data . drop ([ 'y' ], axis = 1 ), data [[ 'y' ]], test_size = 0.25 , random_state = 42 ) data_train_opt , data_test_opt = train_test_split ( data , test_size = 0.25 , random_state = 42 ) data shape: (30000, 24) Experiment setup \u00b6 from sklearn.metrics import roc_auc_score def report_auc ( clf , X_train , y_train , X_test , y_test ): proba_train = clf . predict_proba ( X_train )[:, 1 ] proba_test = clf . predict_proba ( X_test )[:, 1 ] auc_train = round ( roc_auc_score ( y_train , proba_train ), 4 ) auc_test = round ( roc_auc_score ( y_test , proba_test ), 4 ) return auc_train , auc_test from memo import memlist , time_taken data = [] @memlist ( data = data ) @time_taken () def fit_eval_record ( clf , name , opt = False ): if opt : clf . fit ( data_train_opt ) proba_train = clf . predict_proba ( data_train_opt )[:, 1 ] proba_test = clf . predict_proba ( data_test_opt )[:, 1 ] auc_train = round ( roc_auc_score ( y_train , proba_train ), 4 ) auc_test = round ( roc_auc_score ( y_test , proba_test ), 4 ) else : clf . fit ( X_train , y_train ) auc_train , auc_test = report_auc ( clf , X_train , y_train , X_test , y_test ) return { 'auc_train' : auc_train , 'auc_test' : auc_test } Baseline \u00b6 from skorecard import Skorecard scorecard = Skorecard () fit_eval_record ( scorecard , name = \"skorecard.Scorecard\" ) {'auc_train': 0.7727, 'auc_test': 0.766, 'time_taken': 16.73} # from sklearn.pipeline import make_pipeline # from sklearn.linear_model import LogisticRegression # from skorecard.preprocessing import WoeEncoder # from skorecard.bucketers import DecisionTreeBucketer, OptimalBucketer # from category_encoders.woe import WOEEncoder # pipe = make_pipeline( # DecisionTreeBucketer(), # OptimalBucketer(), # #WoeEncoder(), # WOEEncoder(cols=X_train.columns), # LogisticRegression(solver=\"lbfgs\", max_iter=400) # ) # fit_eval_record(pipe, name=\"pipeline\") # # .7166 with skorecard woe in 3.7s # # 0.758 with no WOE in 3.9s # # 0.7661 with WOE on all cols. Optbinning \u00b6 See the excellent package Optbinning . from optbinning import BinningProcess from optbinning import Scorecard from sklearn.linear_model import LogisticRegression import pandas as pd selection_criteria = { \"iv\" : { \"min\" : 0.02 , \"max\" : 1 }, \"quality_score\" : { \"min\" : 0.01 } } binning_process = BinningProcess ( variable_names = list ( X_train . columns ), selection_criteria = selection_criteria ) estimator = LogisticRegression ( solver = \"lbfgs\" ) opt_scorecard = Scorecard ( target = \"y\" , binning_process = binning_process , estimator = estimator , scaling_method = \"min_max\" , scaling_method_params = { \"min\" : 300 , \"max\" : 850 }, ) opt_scorecard . fit ( data_train_opt ) fit_eval_record ( opt_scorecard , name = \"optbinning.Scorecard\" , opt = True ) {'auc_train': 0.7719, 'auc_test': 0.7628, 'time_taken': 1.88} Basic LR \u00b6 from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import StandardScaler from sklearn.pipeline import make_pipeline pipe = make_pipeline ( StandardScaler (), LogisticRegression ( random_state = 42 , solver = \"lbfgs\" ) ) fit_eval_record ( pipe , name = \"sklearn.LogisticRegression\" ) /Users/iv58uq/miniconda3/envs/dancard_py37/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). return f(*args, **kwargs) {'auc_train': 0.724, 'auc_test': 0.7232, 'time_taken': 0.11} LightGBM model \u00b6 The LightGBM Classifier documentation can be found here from lightgbm import LGBMClassifier clf = LGBMClassifier ( random_state = 42 , max_depth = 10 , learning_rate = 0.01 ) fit_eval_record ( clf , name = \"LightGBM\" ) /Users/iv58uq/miniconda3/envs/dancard_py37/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). return f(*args, **kwargs) {'auc_train': 0.8038, 'auc_test': 0.7778, 'time_taken': 0.33} Results \u00b6 pd . DataFrame ( data ) . sort_values ( 'auc_test' , ascending = False ) . drop ( \"opt\" , axis = 1 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name auc_train auc_test time_taken 3 LightGBM 0.8038 0.7778 0.33 0 skorecard.Scorecard 0.7727 0.7660 16.73 1 optbinning.Scorecard 0.7719 0.7628 1.88 2 sklearn.LogisticRegression 0.7240 0.7232 0.11","title":"Benchmarks"},{"location":"discussion/benchmarks/#benchmarks","text":"Here we will demonstrate some benchmarks against some alternatives.","title":"Benchmarks"},{"location":"discussion/benchmarks/#data","text":"UCI Credit card dataset with 30k rows and 23 features. import pandas as pd from skorecard.datasets import load_credit_card from sklearn.model_selection import train_test_split data = load_credit_card ( as_frame = True ) print ( f \"data shape: { data . shape } \" ) X_train , X_test , y_train , y_test = train_test_split ( data . drop ([ 'y' ], axis = 1 ), data [[ 'y' ]], test_size = 0.25 , random_state = 42 ) data_train_opt , data_test_opt = train_test_split ( data , test_size = 0.25 , random_state = 42 ) data shape: (30000, 24)","title":"Data"},{"location":"discussion/benchmarks/#experiment-setup","text":"from sklearn.metrics import roc_auc_score def report_auc ( clf , X_train , y_train , X_test , y_test ): proba_train = clf . predict_proba ( X_train )[:, 1 ] proba_test = clf . predict_proba ( X_test )[:, 1 ] auc_train = round ( roc_auc_score ( y_train , proba_train ), 4 ) auc_test = round ( roc_auc_score ( y_test , proba_test ), 4 ) return auc_train , auc_test from memo import memlist , time_taken data = [] @memlist ( data = data ) @time_taken () def fit_eval_record ( clf , name , opt = False ): if opt : clf . fit ( data_train_opt ) proba_train = clf . predict_proba ( data_train_opt )[:, 1 ] proba_test = clf . predict_proba ( data_test_opt )[:, 1 ] auc_train = round ( roc_auc_score ( y_train , proba_train ), 4 ) auc_test = round ( roc_auc_score ( y_test , proba_test ), 4 ) else : clf . fit ( X_train , y_train ) auc_train , auc_test = report_auc ( clf , X_train , y_train , X_test , y_test ) return { 'auc_train' : auc_train , 'auc_test' : auc_test }","title":"Experiment setup"},{"location":"discussion/benchmarks/#baseline","text":"from skorecard import Skorecard scorecard = Skorecard () fit_eval_record ( scorecard , name = \"skorecard.Scorecard\" ) {'auc_train': 0.7727, 'auc_test': 0.766, 'time_taken': 16.73} # from sklearn.pipeline import make_pipeline # from sklearn.linear_model import LogisticRegression # from skorecard.preprocessing import WoeEncoder # from skorecard.bucketers import DecisionTreeBucketer, OptimalBucketer # from category_encoders.woe import WOEEncoder # pipe = make_pipeline( # DecisionTreeBucketer(), # OptimalBucketer(), # #WoeEncoder(), # WOEEncoder(cols=X_train.columns), # LogisticRegression(solver=\"lbfgs\", max_iter=400) # ) # fit_eval_record(pipe, name=\"pipeline\") # # .7166 with skorecard woe in 3.7s # # 0.758 with no WOE in 3.9s # # 0.7661 with WOE on all cols.","title":"Baseline"},{"location":"discussion/benchmarks/#optbinning","text":"See the excellent package Optbinning . from optbinning import BinningProcess from optbinning import Scorecard from sklearn.linear_model import LogisticRegression import pandas as pd selection_criteria = { \"iv\" : { \"min\" : 0.02 , \"max\" : 1 }, \"quality_score\" : { \"min\" : 0.01 } } binning_process = BinningProcess ( variable_names = list ( X_train . columns ), selection_criteria = selection_criteria ) estimator = LogisticRegression ( solver = \"lbfgs\" ) opt_scorecard = Scorecard ( target = \"y\" , binning_process = binning_process , estimator = estimator , scaling_method = \"min_max\" , scaling_method_params = { \"min\" : 300 , \"max\" : 850 }, ) opt_scorecard . fit ( data_train_opt ) fit_eval_record ( opt_scorecard , name = \"optbinning.Scorecard\" , opt = True ) {'auc_train': 0.7719, 'auc_test': 0.7628, 'time_taken': 1.88}","title":"Optbinning"},{"location":"discussion/benchmarks/#basic-lr","text":"from sklearn.linear_model import LogisticRegression from sklearn.preprocessing import StandardScaler from sklearn.pipeline import make_pipeline pipe = make_pipeline ( StandardScaler (), LogisticRegression ( random_state = 42 , solver = \"lbfgs\" ) ) fit_eval_record ( pipe , name = \"sklearn.LogisticRegression\" ) /Users/iv58uq/miniconda3/envs/dancard_py37/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). return f(*args, **kwargs) {'auc_train': 0.724, 'auc_test': 0.7232, 'time_taken': 0.11}","title":"Basic LR"},{"location":"discussion/benchmarks/#lightgbm-model","text":"The LightGBM Classifier documentation can be found here from lightgbm import LGBMClassifier clf = LGBMClassifier ( random_state = 42 , max_depth = 10 , learning_rate = 0.01 ) fit_eval_record ( clf , name = \"LightGBM\" ) /Users/iv58uq/miniconda3/envs/dancard_py37/lib/python3.7/site-packages/sklearn/utils/validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). return f(*args, **kwargs) {'auc_train': 0.8038, 'auc_test': 0.7778, 'time_taken': 0.33}","title":"LightGBM model"},{"location":"discussion/benchmarks/#results","text":"pd . DataFrame ( data ) . sort_values ( 'auc_test' , ascending = False ) . drop ( \"opt\" , axis = 1 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } name auc_train auc_test time_taken 3 LightGBM 0.8038 0.7778 0.33 0 skorecard.Scorecard 0.7727 0.7660 16.73 1 optbinning.Scorecard 0.7719 0.7628 1.88 2 sklearn.LogisticRegression 0.7240 0.7232 0.11","title":"Results"},{"location":"howto/Optimizations/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Optimizing the bucketing process \u00b6 import pandas as pd from skorecard.datasets import load_credit_card df = load_credit_card ( as_frame = True ) # Show display ( df . head ( 4 )) num_feats = [ 'x1' , 'x15' , 'x16' ] X = df [ num_feats ] y = df [ 'y' ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 ... x15 x16 x17 x18 x19 x20 x21 x22 x23 y 0 20000.0 2.0 2.0 1.0 24.0 2.0 2.0 -1.0 -1.0 -2.0 ... 0.0 0.0 0.0 0.0 689.0 0.0 0.0 0.0 0.0 1 1 120000.0 2.0 2.0 2.0 26.0 -1.0 2.0 0.0 0.0 0.0 ... 3272.0 3455.0 3261.0 0.0 1000.0 1000.0 1000.0 0.0 2000.0 1 2 90000.0 2.0 2.0 2.0 34.0 0.0 0.0 0.0 0.0 0.0 ... 14331.0 14948.0 15549.0 1518.0 1500.0 1000.0 1000.0 1000.0 5000.0 0 3 50000.0 2.0 2.0 1.0 37.0 0.0 0.0 0.0 0.0 0.0 ... 28314.0 28959.0 29547.0 2000.0 2019.0 1200.0 1100.0 1069.0 1000.0 0 4 rows \u00d7 24 columns Finding the best bucketing \u00b6 The art of building a good scorecard model lies in finding the best bucketing strategy. Good buckets improve the predicitve power of the model, as well as guarantee stability of the predictions. This is normally a very manual, labour intensive process (and for a good reason). A good bucketing strategy follows the following principles: - maximizes the Information Values, defined as \\[IV = \\sum_{i}(\\%G_{i}-\\%B_{i}) \\cdot \\log(\\frac{\\%G_{i}}{\\%B_{i}})\\] avoids buckets that contain a very large or very small fraction of the population wherever the business sense requires it, The skorecard package provides some tooling to automate part of the process, namely: Grid search the hyper-parameters of the bucketers in order to maximise the information value Run the optimal bucketer within the bucketing process Grid search the bucketers to maximise the information value \u00b6 skorecard implements an IV_scorer , that can be used as a custom scoring function for grid searching. The following snippets of code show how to integrate it in the grid search. The DecisionTreeBucketer applied on numerical features is the best use case, as there are some hyper-parameters that influence the bucketing quality. from skorecard.metrics import IV_scorer from skorecard.bucketers import DecisionTreeBucketer from sklearn.model_selection import GridSearchCV The DecisionTreeBucketer has two main hyperparameters to grid-search: - max_n_bins , maximum number of bins allowed for the bucketing - min_bin_size minimum fraction of data in the buckets gs_params = { \"max_n_bins\" : [ 3 , 4 , 5 , 6 ], \"min_bin_size\" : [ 0.05 , 0.06 , 0.07 , 0.08 ], #, 0.12] } The optimization has to be done for every feature indipendently, therefore we need a loop, and all the parameters are best stored in a data collector, like a dictionary # Define the specials best_params = dict () max_iv = dict () cv_results = dict () # Add a special for demo purposes specials = { 'x1' :{ 'special 0' :[ '50000.0' ]}} for feat in num_feats : # This snippet illustrates what to do with special values if feat in specials . keys (): # This construct is needed to remap the specials, because skorecard validates that the key # of the dictionary is present in the variables special = { feat : specials [ feat ]} else : special = {} bucketer = DecisionTreeBucketer ( variables = [ feat ], specials = special ) gs = GridSearchCV ( bucketer , gs_params , scoring = IV_scorer , cv = 3 , return_train_score = True ) gs . fit ( X [[ feat ]], y ) best_params [ feat ] = gs . best_params_ max_iv [ feat ] = gs . best_score_ cv_results [ feat ] = gs . cv_results_ Checking the best parameters per feature best_params {'x1': {'max_n_bins': 3, 'min_bin_size': 0.05}, 'x15': {'max_n_bins': 3, 'min_bin_size': 0.05}, 'x16': {'max_n_bins': 3, 'min_bin_size': 0.05}} Because of its additive nature, IV is likely to be maximal for the highest max_n_bins . Therefore it is worth looking analysing the CV results! cv_results [ 'x1' ] {'mean_fit_time': array([0.14118997, 0.13273303, 0.13474902, 0.15843304, 0.17114846, 0.1259594 , 0.12854441, 0.13791513, 0.14939396, 0.12906257, 0.15454125, 0.11709793, 0.1234947 , 0.11326059, 0.11524073, 0.11928709]), 'std_fit_time': array([0.01615798, 0.00538481, 0.00918157, 0.02513249, 0.02130305, 0.0088008 , 0.0078898 , 0.00226504, 0.01736914, 0.00537724, 0.04489044, 0.00418452, 0.00750423, 0.00055744, 0.00241629, 0.01126566]), 'mean_score_time': array([0.03244432, 0.03500628, 0.03295326, 0.04452038, 0.04895496, 0.03155041, 0.03200722, 0.03328069, 0.0405368 , 0.03386513, 0.02966809, 0.03014151, 0.03117593, 0.02836776, 0.02895562, 0.02856787]), 'std_score_time': array([0.00520814, 0.00130717, 0.00347241, 0.00365442, 0.01206228, 0.00173939, 0.00279055, 0.0009404 , 0.01539335, 0.00089022, 0.00115331, 0.00201206, 0.00159311, 0.00142928, 0.0013334 , 0.00086222]), 'param_max_n_bins': masked_array(data=[3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6], mask=[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False], fill_value='?', dtype=object), 'param_min_bin_size': masked_array(data=[0.05, 0.06, 0.07, 0.08, 0.05, 0.06, 0.07, 0.08, 0.05, 0.06, 0.07, 0.08, 0.05, 0.06, 0.07, 0.08], mask=[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False], fill_value='?', dtype=object), 'params': [{'max_n_bins': 3, 'min_bin_size': 0.05}, {'max_n_bins': 3, 'min_bin_size': 0.06}, {'max_n_bins': 3, 'min_bin_size': 0.07}, {'max_n_bins': 3, 'min_bin_size': 0.08}, {'max_n_bins': 4, 'min_bin_size': 0.05}, {'max_n_bins': 4, 'min_bin_size': 0.06}, {'max_n_bins': 4, 'min_bin_size': 0.07}, {'max_n_bins': 4, 'min_bin_size': 0.08}, {'max_n_bins': 5, 'min_bin_size': 0.05}, {'max_n_bins': 5, 'min_bin_size': 0.06}, {'max_n_bins': 5, 'min_bin_size': 0.07}, {'max_n_bins': 5, 'min_bin_size': 0.08}, {'max_n_bins': 6, 'min_bin_size': 0.05}, {'max_n_bins': 6, 'min_bin_size': 0.06}, {'max_n_bins': 6, 'min_bin_size': 0.07}, {'max_n_bins': 6, 'min_bin_size': 0.08}], 'split0_test_score': array([0.079, 0.079, 0.079, 0.079, 0.097, 0.097, 0.097, 0.097, 0.106, 0.106, 0.106, 0.106, 0.107, 0.107, 0.107, 0.107]), 'split1_test_score': array([4.491, 4.491, 4.491, 4.491, 4.308, 4.308, 4.308, 4.308, 4.19 , 4.19 , 4.19 , 4.19 , 4.043, 4.043, 4.043, 4.043]), 'split2_test_score': array([4.442, 4.442, 4.442, 4.442, 4.305, 4.305, 4.305, 4.305, 4.07 , 4.07 , 4.07 , 4.07 , 3.975, 3.975, 3.975, 3.975]), 'mean_test_score': array([3.004 , 3.004 , 3.004 , 3.004 , 2.90333333, 2.90333333, 2.90333333, 2.90333333, 2.78866667, 2.78866667, 2.78866667, 2.78866667, 2.70833333, 2.70833333, 2.70833333, 2.70833333]), 'std_test_score': array([2.06838407, 2.06838407, 2.06838407, 2.06838407, 1.98437771, 1.98437771, 1.98437771, 1.98437771, 1.89756429, 1.89756429, 1.89756429, 1.89756429, 1.83962991, 1.83962991, 1.83962991, 1.83962991]), 'rank_test_score': array([ 1, 1, 1, 1, 5, 5, 5, 5, 9, 9, 9, 9, 13, 13, 13, 13], dtype=int32), 'split0_train_score': array([0.049, 0.049, 0.049, 0.049, 0.049, 0.049, 0.049, 0.049, 0.05 , 0.05 , 0.05 , 0.05 , 0.05 , 0.05 , 0.05 , 0.05 ]), 'split1_train_score': array([0.102, 0.102, 0.102, 0.102, 0.112, 0.112, 0.112, 0.112, 0.116, 0.116, 0.116, 0.116, 0.119, 0.119, 0.119, 0.119]), 'split2_train_score': array([0.119, 0.119, 0.119, 0.119, 0.144, 0.144, 0.144, 0.144, 0.156, 0.156, 0.156, 0.156, 0.159, 0.159, 0.159, 0.159]), 'mean_train_score': array([0.09 , 0.09 , 0.09 , 0.09 , 0.10166667, 0.10166667, 0.10166667, 0.10166667, 0.10733333, 0.10733333, 0.10733333, 0.10733333, 0.10933333, 0.10933333, 0.10933333, 0.10933333]), 'std_train_score': array([0.02981051, 0.02981051, 0.02981051, 0.02981051, 0.03946588, 0.03946588, 0.03946588, 0.03946588, 0.04370609, 0.04370609, 0.04370609, 0.04370609, 0.04502098, 0.04502098, 0.04502098, 0.04502098])} RandomizedSearchCV to maximise AUC \u00b6 As Skorecard is scikit-learn compatibile we can use scikit-learn methods such as RandomizedSearchCV to maximise the AUC of our model. Shown below is one such example from skorecard.bucketers import DecisionTreeBucketer , OptimalBucketer from skorecard.pipeline import BucketingProcess from skorecard.linear_model import LogisticRegression from skorecard.preprocessing import WoeEncoder from sklearn.model_selection import RandomizedSearchCV from sklearn.pipeline import make_pipeline from scipy.stats import uniform def get_pipeline (): bucketing_process = BucketingProcess ( prebucketing_pipeline = make_pipeline ( DecisionTreeBucketer ( max_n_bins = 100 , min_bin_size = 0.05 ), ), bucketing_pipeline = make_pipeline ( OptimalBucketer ( max_n_bins = 10 , min_bin_size = 0.04 ), ), ) return make_pipeline ( bucketing_process , WoeEncoder (), LogisticRegression ( solver = \"liblinear\" , C = 1.7 , max_iter = 150 , random_state = 0 ) ) pipe = get_pipeline () param_grid = [ { 'logisticregression__C' : uniform ( loc = 0 , scale = 4 ), 'logisticregression__solver' : [ 'liblinear' ] }, ] search_cv = RandomizedSearchCV ( pipe , param_distributions = param_grid , cv = 5 , verbose = True , scoring = 'roc_auc' , n_jobs =- 1 , random_state = 0 , refit = True ) search_cv . fit ( X , y ) Fitting 5 folds for each of 10 candidates, totalling 50 fits RandomizedSearchCV(cv=5, estimator=Pipeline(steps=[('bucketingprocess', BucketingProcess(bucketing_pipeline=Pipeline(steps=[('optimalbucketer', OptimalBucketer(min_bin_size=0.04))]), prebucketing_pipeline=Pipeline(steps=[('decisiontreebucketer', DecisionTreeBucketer())]))), ('woeencoder', WoeEncoder()), ('logisticregression', LogisticRegression(C=1.7, max_iter=150, random_state=0, solver='liblinear'))]), n_jobs=-1, param_distributions=[{'logisticregression__C': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f97de25c5d0>, 'logisticregression__solver': ['liblinear']}], random_state=0, scoring='roc_auc', verbose=True) search_cv . best_params_ , search_cv . best_score_ ({'logisticregression__C': 2.860757465489678, 'logisticregression__solver': 'liblinear'}, 0.6187444445104318)","title":"Optimizations in the bucketing process"},{"location":"howto/Optimizations/#optimizing-the-bucketing-process","text":"import pandas as pd from skorecard.datasets import load_credit_card df = load_credit_card ( as_frame = True ) # Show display ( df . head ( 4 )) num_feats = [ 'x1' , 'x15' , 'x16' ] X = df [ num_feats ] y = df [ 'y' ] .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 ... x15 x16 x17 x18 x19 x20 x21 x22 x23 y 0 20000.0 2.0 2.0 1.0 24.0 2.0 2.0 -1.0 -1.0 -2.0 ... 0.0 0.0 0.0 0.0 689.0 0.0 0.0 0.0 0.0 1 1 120000.0 2.0 2.0 2.0 26.0 -1.0 2.0 0.0 0.0 0.0 ... 3272.0 3455.0 3261.0 0.0 1000.0 1000.0 1000.0 0.0 2000.0 1 2 90000.0 2.0 2.0 2.0 34.0 0.0 0.0 0.0 0.0 0.0 ... 14331.0 14948.0 15549.0 1518.0 1500.0 1000.0 1000.0 1000.0 5000.0 0 3 50000.0 2.0 2.0 1.0 37.0 0.0 0.0 0.0 0.0 0.0 ... 28314.0 28959.0 29547.0 2000.0 2019.0 1200.0 1100.0 1069.0 1000.0 0 4 rows \u00d7 24 columns","title":"Optimizing the bucketing process"},{"location":"howto/Optimizations/#finding-the-best-bucketing","text":"The art of building a good scorecard model lies in finding the best bucketing strategy. Good buckets improve the predicitve power of the model, as well as guarantee stability of the predictions. This is normally a very manual, labour intensive process (and for a good reason). A good bucketing strategy follows the following principles: - maximizes the Information Values, defined as \\[IV = \\sum_{i}(\\%G_{i}-\\%B_{i}) \\cdot \\log(\\frac{\\%G_{i}}{\\%B_{i}})\\] avoids buckets that contain a very large or very small fraction of the population wherever the business sense requires it, The skorecard package provides some tooling to automate part of the process, namely: Grid search the hyper-parameters of the bucketers in order to maximise the information value Run the optimal bucketer within the bucketing process","title":"Finding the best bucketing"},{"location":"howto/Optimizations/#grid-search-the-bucketers-to-maximise-the-information-value","text":"skorecard implements an IV_scorer , that can be used as a custom scoring function for grid searching. The following snippets of code show how to integrate it in the grid search. The DecisionTreeBucketer applied on numerical features is the best use case, as there are some hyper-parameters that influence the bucketing quality. from skorecard.metrics import IV_scorer from skorecard.bucketers import DecisionTreeBucketer from sklearn.model_selection import GridSearchCV The DecisionTreeBucketer has two main hyperparameters to grid-search: - max_n_bins , maximum number of bins allowed for the bucketing - min_bin_size minimum fraction of data in the buckets gs_params = { \"max_n_bins\" : [ 3 , 4 , 5 , 6 ], \"min_bin_size\" : [ 0.05 , 0.06 , 0.07 , 0.08 ], #, 0.12] } The optimization has to be done for every feature indipendently, therefore we need a loop, and all the parameters are best stored in a data collector, like a dictionary # Define the specials best_params = dict () max_iv = dict () cv_results = dict () # Add a special for demo purposes specials = { 'x1' :{ 'special 0' :[ '50000.0' ]}} for feat in num_feats : # This snippet illustrates what to do with special values if feat in specials . keys (): # This construct is needed to remap the specials, because skorecard validates that the key # of the dictionary is present in the variables special = { feat : specials [ feat ]} else : special = {} bucketer = DecisionTreeBucketer ( variables = [ feat ], specials = special ) gs = GridSearchCV ( bucketer , gs_params , scoring = IV_scorer , cv = 3 , return_train_score = True ) gs . fit ( X [[ feat ]], y ) best_params [ feat ] = gs . best_params_ max_iv [ feat ] = gs . best_score_ cv_results [ feat ] = gs . cv_results_ Checking the best parameters per feature best_params {'x1': {'max_n_bins': 3, 'min_bin_size': 0.05}, 'x15': {'max_n_bins': 3, 'min_bin_size': 0.05}, 'x16': {'max_n_bins': 3, 'min_bin_size': 0.05}} Because of its additive nature, IV is likely to be maximal for the highest max_n_bins . Therefore it is worth looking analysing the CV results! cv_results [ 'x1' ] {'mean_fit_time': array([0.14118997, 0.13273303, 0.13474902, 0.15843304, 0.17114846, 0.1259594 , 0.12854441, 0.13791513, 0.14939396, 0.12906257, 0.15454125, 0.11709793, 0.1234947 , 0.11326059, 0.11524073, 0.11928709]), 'std_fit_time': array([0.01615798, 0.00538481, 0.00918157, 0.02513249, 0.02130305, 0.0088008 , 0.0078898 , 0.00226504, 0.01736914, 0.00537724, 0.04489044, 0.00418452, 0.00750423, 0.00055744, 0.00241629, 0.01126566]), 'mean_score_time': array([0.03244432, 0.03500628, 0.03295326, 0.04452038, 0.04895496, 0.03155041, 0.03200722, 0.03328069, 0.0405368 , 0.03386513, 0.02966809, 0.03014151, 0.03117593, 0.02836776, 0.02895562, 0.02856787]), 'std_score_time': array([0.00520814, 0.00130717, 0.00347241, 0.00365442, 0.01206228, 0.00173939, 0.00279055, 0.0009404 , 0.01539335, 0.00089022, 0.00115331, 0.00201206, 0.00159311, 0.00142928, 0.0013334 , 0.00086222]), 'param_max_n_bins': masked_array(data=[3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6], mask=[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False], fill_value='?', dtype=object), 'param_min_bin_size': masked_array(data=[0.05, 0.06, 0.07, 0.08, 0.05, 0.06, 0.07, 0.08, 0.05, 0.06, 0.07, 0.08, 0.05, 0.06, 0.07, 0.08], mask=[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False], fill_value='?', dtype=object), 'params': [{'max_n_bins': 3, 'min_bin_size': 0.05}, {'max_n_bins': 3, 'min_bin_size': 0.06}, {'max_n_bins': 3, 'min_bin_size': 0.07}, {'max_n_bins': 3, 'min_bin_size': 0.08}, {'max_n_bins': 4, 'min_bin_size': 0.05}, {'max_n_bins': 4, 'min_bin_size': 0.06}, {'max_n_bins': 4, 'min_bin_size': 0.07}, {'max_n_bins': 4, 'min_bin_size': 0.08}, {'max_n_bins': 5, 'min_bin_size': 0.05}, {'max_n_bins': 5, 'min_bin_size': 0.06}, {'max_n_bins': 5, 'min_bin_size': 0.07}, {'max_n_bins': 5, 'min_bin_size': 0.08}, {'max_n_bins': 6, 'min_bin_size': 0.05}, {'max_n_bins': 6, 'min_bin_size': 0.06}, {'max_n_bins': 6, 'min_bin_size': 0.07}, {'max_n_bins': 6, 'min_bin_size': 0.08}], 'split0_test_score': array([0.079, 0.079, 0.079, 0.079, 0.097, 0.097, 0.097, 0.097, 0.106, 0.106, 0.106, 0.106, 0.107, 0.107, 0.107, 0.107]), 'split1_test_score': array([4.491, 4.491, 4.491, 4.491, 4.308, 4.308, 4.308, 4.308, 4.19 , 4.19 , 4.19 , 4.19 , 4.043, 4.043, 4.043, 4.043]), 'split2_test_score': array([4.442, 4.442, 4.442, 4.442, 4.305, 4.305, 4.305, 4.305, 4.07 , 4.07 , 4.07 , 4.07 , 3.975, 3.975, 3.975, 3.975]), 'mean_test_score': array([3.004 , 3.004 , 3.004 , 3.004 , 2.90333333, 2.90333333, 2.90333333, 2.90333333, 2.78866667, 2.78866667, 2.78866667, 2.78866667, 2.70833333, 2.70833333, 2.70833333, 2.70833333]), 'std_test_score': array([2.06838407, 2.06838407, 2.06838407, 2.06838407, 1.98437771, 1.98437771, 1.98437771, 1.98437771, 1.89756429, 1.89756429, 1.89756429, 1.89756429, 1.83962991, 1.83962991, 1.83962991, 1.83962991]), 'rank_test_score': array([ 1, 1, 1, 1, 5, 5, 5, 5, 9, 9, 9, 9, 13, 13, 13, 13], dtype=int32), 'split0_train_score': array([0.049, 0.049, 0.049, 0.049, 0.049, 0.049, 0.049, 0.049, 0.05 , 0.05 , 0.05 , 0.05 , 0.05 , 0.05 , 0.05 , 0.05 ]), 'split1_train_score': array([0.102, 0.102, 0.102, 0.102, 0.112, 0.112, 0.112, 0.112, 0.116, 0.116, 0.116, 0.116, 0.119, 0.119, 0.119, 0.119]), 'split2_train_score': array([0.119, 0.119, 0.119, 0.119, 0.144, 0.144, 0.144, 0.144, 0.156, 0.156, 0.156, 0.156, 0.159, 0.159, 0.159, 0.159]), 'mean_train_score': array([0.09 , 0.09 , 0.09 , 0.09 , 0.10166667, 0.10166667, 0.10166667, 0.10166667, 0.10733333, 0.10733333, 0.10733333, 0.10733333, 0.10933333, 0.10933333, 0.10933333, 0.10933333]), 'std_train_score': array([0.02981051, 0.02981051, 0.02981051, 0.02981051, 0.03946588, 0.03946588, 0.03946588, 0.03946588, 0.04370609, 0.04370609, 0.04370609, 0.04370609, 0.04502098, 0.04502098, 0.04502098, 0.04502098])}","title":"Grid search the bucketers to maximise the information value"},{"location":"howto/Optimizations/#randomizedsearchcv-to-maximise-auc","text":"As Skorecard is scikit-learn compatibile we can use scikit-learn methods such as RandomizedSearchCV to maximise the AUC of our model. Shown below is one such example from skorecard.bucketers import DecisionTreeBucketer , OptimalBucketer from skorecard.pipeline import BucketingProcess from skorecard.linear_model import LogisticRegression from skorecard.preprocessing import WoeEncoder from sklearn.model_selection import RandomizedSearchCV from sklearn.pipeline import make_pipeline from scipy.stats import uniform def get_pipeline (): bucketing_process = BucketingProcess ( prebucketing_pipeline = make_pipeline ( DecisionTreeBucketer ( max_n_bins = 100 , min_bin_size = 0.05 ), ), bucketing_pipeline = make_pipeline ( OptimalBucketer ( max_n_bins = 10 , min_bin_size = 0.04 ), ), ) return make_pipeline ( bucketing_process , WoeEncoder (), LogisticRegression ( solver = \"liblinear\" , C = 1.7 , max_iter = 150 , random_state = 0 ) ) pipe = get_pipeline () param_grid = [ { 'logisticregression__C' : uniform ( loc = 0 , scale = 4 ), 'logisticregression__solver' : [ 'liblinear' ] }, ] search_cv = RandomizedSearchCV ( pipe , param_distributions = param_grid , cv = 5 , verbose = True , scoring = 'roc_auc' , n_jobs =- 1 , random_state = 0 , refit = True ) search_cv . fit ( X , y ) Fitting 5 folds for each of 10 candidates, totalling 50 fits RandomizedSearchCV(cv=5, estimator=Pipeline(steps=[('bucketingprocess', BucketingProcess(bucketing_pipeline=Pipeline(steps=[('optimalbucketer', OptimalBucketer(min_bin_size=0.04))]), prebucketing_pipeline=Pipeline(steps=[('decisiontreebucketer', DecisionTreeBucketer())]))), ('woeencoder', WoeEncoder()), ('logisticregression', LogisticRegression(C=1.7, max_iter=150, random_state=0, solver='liblinear'))]), n_jobs=-1, param_distributions=[{'logisticregression__C': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f97de25c5d0>, 'logisticregression__solver': ['liblinear']}], random_state=0, scoring='roc_auc', verbose=True) search_cv . best_params_ , search_cv . best_score_ ({'logisticregression__C': 2.860757465489678, 'logisticregression__solver': 'liblinear'}, 0.6187444445104318)","title":"RandomizedSearchCV to maximise AUC"},{"location":"howto/mix_with_other_packages/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Mixed usage with other packages \u00b6 There are quite some excellent packages out there offering functionality around bucketing/binning/discretizing numerical variables and encoding categorical variables. Chances are you'd like to combine them in your skorecard pipelines. Here are some packages are are compatible with pandas dataframes: category_encoders from scikit-learn-contrib feature-engine categorical variable encoders feature-engine variable discretisation %% capture ! pip install category_encoders %% capture from sklearn.pipeline import make_pipeline from skorecard.datasets import load_uci_credit_card from skorecard.bucketers import OrdinalCategoricalBucketer X , y = load_uci_credit_card ( return_X_y = True ) from category_encoders import TargetEncoder pipe = make_pipeline ( TargetEncoder ( cols = [ \"EDUCATION\" ]), # category_encoders.TargetEncoder passes through other columns OrdinalCategoricalBucketer ( variables = [ \"MARRIAGE\" ]) ) pipe . fit ( X , y ) pipe . transform ( X ) . head ( 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } EDUCATION MARRIAGE LIMIT_BAL BILL_AMT1 0 0.0 2.0 400000.0 201800.0 1 1.0 2.0 80000.0 80610.0 2 0.0 2.0 500000.0 499452.0 3 0.0 1.0 140000.0 450.0 4 1.0 1.0 420000.0 56107.0 Some packages do not return pandas DataFrames, like: sklearn.preprocessing.KBinsDiscretizer You can wrap the class in skorecard.pipeline.KeepPandas to use these transformers in a pipeline: from sklearn.preprocessing import KBinsDiscretizer from skorecard.pipeline import KeepPandas from sklearn.compose import ColumnTransformer ct = ColumnTransformer ( [ ( \"binner\" , KBinsDiscretizer ( n_bins = 3 , encode = 'ordinal' , strategy = 'uniform' ), [ 'EDUCATION' ]) ], remainder = \"passthrough\" ) pipe = make_pipeline ( KeepPandas ( ct ), OrdinalCategoricalBucketer ( variables = [ \"MARRIAGE\" ]) ) pipe . fit_transform ( X , y ) . head ( 5 ) WARNING:root:sklearn.compose.ColumnTransformer can change the order of columns, be very careful when using with KeepPandas() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } EDUCATION MARRIAGE LIMIT_BAL BILL_AMT1 0 0.0 2.0 400000.0 201800.0 1 1.0 2.0 80000.0 80610.0 2 0.0 2.0 500000.0 499452.0 3 0.0 1.0 140000.0 450.0 4 1.0 1.0 420000.0 56107.0","title":"Mixed usage with other packages"},{"location":"howto/mix_with_other_packages/#mixed-usage-with-other-packages","text":"There are quite some excellent packages out there offering functionality around bucketing/binning/discretizing numerical variables and encoding categorical variables. Chances are you'd like to combine them in your skorecard pipelines. Here are some packages are are compatible with pandas dataframes: category_encoders from scikit-learn-contrib feature-engine categorical variable encoders feature-engine variable discretisation %% capture ! pip install category_encoders %% capture from sklearn.pipeline import make_pipeline from skorecard.datasets import load_uci_credit_card from skorecard.bucketers import OrdinalCategoricalBucketer X , y = load_uci_credit_card ( return_X_y = True ) from category_encoders import TargetEncoder pipe = make_pipeline ( TargetEncoder ( cols = [ \"EDUCATION\" ]), # category_encoders.TargetEncoder passes through other columns OrdinalCategoricalBucketer ( variables = [ \"MARRIAGE\" ]) ) pipe . fit ( X , y ) pipe . transform ( X ) . head ( 5 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } EDUCATION MARRIAGE LIMIT_BAL BILL_AMT1 0 0.0 2.0 400000.0 201800.0 1 1.0 2.0 80000.0 80610.0 2 0.0 2.0 500000.0 499452.0 3 0.0 1.0 140000.0 450.0 4 1.0 1.0 420000.0 56107.0 Some packages do not return pandas DataFrames, like: sklearn.preprocessing.KBinsDiscretizer You can wrap the class in skorecard.pipeline.KeepPandas to use these transformers in a pipeline: from sklearn.preprocessing import KBinsDiscretizer from skorecard.pipeline import KeepPandas from sklearn.compose import ColumnTransformer ct = ColumnTransformer ( [ ( \"binner\" , KBinsDiscretizer ( n_bins = 3 , encode = 'ordinal' , strategy = 'uniform' ), [ 'EDUCATION' ]) ], remainder = \"passthrough\" ) pipe = make_pipeline ( KeepPandas ( ct ), OrdinalCategoricalBucketer ( variables = [ \"MARRIAGE\" ]) ) pipe . fit_transform ( X , y ) . head ( 5 ) WARNING:root:sklearn.compose.ColumnTransformer can change the order of columns, be very careful when using with KeepPandas() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } EDUCATION MARRIAGE LIMIT_BAL BILL_AMT1 0 0.0 2.0 400000.0 201800.0 1 1.0 2.0 80000.0 80610.0 2 0.0 2.0 500000.0 499452.0 3 0.0 1.0 140000.0 450.0 4 1.0 1.0 420000.0 56107.0","title":"Mixed usage with other packages"},{"location":"howto/psi_and_iv/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Measuring bucketed distribution shifts. \u00b6 Population staibility index - PSI \u00b6 The PSI (population stability index), is a common measure to evaluate how similar two univariate distributions are. It's given by the following formula \\[PSI=\\sum_{i}^{N_{bins}} (\\%x_{i}^{actual} - \\%x_{i}^{expected}) log\\frac{\\%x_{i}^{actual}}{\\%x_{i}^{expected}}\\] where the sum runs over all the buckets of the feature x . skorecard implements a simple functionality to calculate the PSI between two datasets. As two datasets are needed, we split the X and y into a train and test set. from skorecard import datasets from sklearn.model_selection import train_test_split from skorecard.bucketers import DecisionTreeBucketer X , y = datasets . load_uci_credit_card ( return_X_y = True ) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.25 , random_state = 42 ) By definition, the PSI acts on bucketed features. Failing to bucket the features would still yield a value of the PSI. However, in this case the PSI will be computed over all the unique values. For numerical features, this will return artifically high and meaningless values. dbt = DecisionTreeBucketer () X_train_bins = dbt . fit_transform ( X_train , y_train ) X_test_bins = dbt . transform ( X_test ) Calculating the PSI from skorecard.reporting import psi psi_dict = psi ( X_train_bins , X_test_bins ) psi_dict {'EDUCATION': 0.0005202506508081382, 'MARRIAGE': 0.0003497580712116056, 'LIMIT_BAL': 0.013577676978376134, 'BILL_AMT1': 0.017027519474734677} Univariate predictive power \u00b6 Information value (IV) \u00b6 The information value is nothing else than the PSI, but it's computed between the features set defined by the target y=0 and y=1. In other words, it can be summarized by the formula. \\[IV=\\sum_{i}^{N_{bins}} (\\%x_{i}^{y=0} - \\%x_{i}^{y=1}) log\\frac{\\%x_{i}^{y=0}}{\\%x_{i}^{y=1}}\\] dbt = DecisionTreeBucketer () X_bins = dbt . fit_transform ( X , y ) To compute the iv, skorecard implements a handy function. The function consumes the (binned) feature set X, and the target y from skorecard.reporting import iv iv = iv ( X_bins , y ) iv {'EDUCATION': 0.036451028950383324, 'MARRIAGE': 0.009494315565036299, 'LIMIT_BAL': 0.17922043483265943, 'BILL_AMT1': 0.05239237644085838}","title":"Assessing bucket quality"},{"location":"howto/psi_and_iv/#measuring-bucketed-distribution-shifts","text":"","title":"Measuring bucketed distribution shifts."},{"location":"howto/psi_and_iv/#population-staibility-index-psi","text":"The PSI (population stability index), is a common measure to evaluate how similar two univariate distributions are. It's given by the following formula \\[PSI=\\sum_{i}^{N_{bins}} (\\%x_{i}^{actual} - \\%x_{i}^{expected}) log\\frac{\\%x_{i}^{actual}}{\\%x_{i}^{expected}}\\] where the sum runs over all the buckets of the feature x . skorecard implements a simple functionality to calculate the PSI between two datasets. As two datasets are needed, we split the X and y into a train and test set. from skorecard import datasets from sklearn.model_selection import train_test_split from skorecard.bucketers import DecisionTreeBucketer X , y = datasets . load_uci_credit_card ( return_X_y = True ) X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = 0.25 , random_state = 42 ) By definition, the PSI acts on bucketed features. Failing to bucket the features would still yield a value of the PSI. However, in this case the PSI will be computed over all the unique values. For numerical features, this will return artifically high and meaningless values. dbt = DecisionTreeBucketer () X_train_bins = dbt . fit_transform ( X_train , y_train ) X_test_bins = dbt . transform ( X_test ) Calculating the PSI from skorecard.reporting import psi psi_dict = psi ( X_train_bins , X_test_bins ) psi_dict {'EDUCATION': 0.0005202506508081382, 'MARRIAGE': 0.0003497580712116056, 'LIMIT_BAL': 0.013577676978376134, 'BILL_AMT1': 0.017027519474734677}","title":"Population staibility index - PSI"},{"location":"howto/psi_and_iv/#univariate-predictive-power","text":"","title":"Univariate predictive power"},{"location":"howto/psi_and_iv/#information-value-iv","text":"The information value is nothing else than the PSI, but it's computed between the features set defined by the target y=0 and y=1. In other words, it can be summarized by the formula. \\[IV=\\sum_{i}^{N_{bins}} (\\%x_{i}^{y=0} - \\%x_{i}^{y=1}) log\\frac{\\%x_{i}^{y=0}}{\\%x_{i}^{y=1}}\\] dbt = DecisionTreeBucketer () X_bins = dbt . fit_transform ( X , y ) To compute the iv, skorecard implements a handy function. The function consumes the (binned) feature set X, and the target y from skorecard.reporting import iv iv = iv ( X_bins , y ) iv {'EDUCATION': 0.036451028950383324, 'MARRIAGE': 0.009494315565036299, 'LIMIT_BAL': 0.17922043483265943, 'BILL_AMT1': 0.05239237644085838}","title":"Information value (IV)"},{"location":"howto/save_buckets_to_file/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Saving bucket information to a file \u00b6 If you have a specific set of bucketing boundaries you are satisfied with, it's useful to save them to a file. You might want to save the bucketing information as configuration files along with your code. All skorecard bucketers, the BucketingProcess and Skorecard model support saving to yaml files with save_yml() . The special UserInputBucketer can read in these configuration files and can be used in the final model pipeline. Example with a bucketer \u00b6 from skorecard.datasets import load_uci_credit_card from skorecard.bucketers import DecisionTreeBucketer , UserInputBucketer X , y = load_uci_credit_card ( return_X_y = True ) bucketer = DecisionTreeBucketer ( max_n_bins = 10 ) bucketer = bucketer . fit ( X , y ) bucketer . save_yml ( \"bucketer.yml\" ) uib = UserInputBucketer ( \"bucketer.yml\" ) uib . transform ( X ) . head ( 4 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } EDUCATION MARRIAGE LIMIT_BAL BILL_AMT1 0 0 1 9 9 1 1 1 3 7 2 0 1 9 9 3 0 0 5 0 Example with BucketingProcess \u00b6 A bucketing process works in exactly the same way. Because there is a prebucketing pipeline and a bucketing pipeline, skorecard makes sure that the buckets are the transformation from raw data to final bucket. from skorecard.pipeline import BucketingProcess from skorecard.bucketers import EqualFrequencyBucketer , OptimalBucketer , AsIsCategoricalBucketer from sklearn.pipeline import make_pipeline num_cols = [ \"LIMIT_BAL\" , \"BILL_AMT1\" ] cat_cols = [ \"EDUCATION\" , \"MARRIAGE\" ] bucketing_process = BucketingProcess ( prebucketing_pipeline = make_pipeline ( DecisionTreeBucketer ( variables = num_cols , max_n_bins = 100 , min_bin_size = 0.05 ), AsIsCategoricalBucketer ( variables = cat_cols ) ), bucketing_pipeline = make_pipeline ( OptimalBucketer ( variables = num_cols , max_n_bins = 10 , min_bin_size = 0.05 ), OptimalBucketer ( variables = cat_cols , variables_type = 'categorical' , max_n_bins = 10 , min_bin_size = 0.05 ), ) ) bucketing_process . fit ( X , y ) bucketing_process . save_yml ( \"bucket_process.yml\" ) uib = UserInputBucketer ( \"bucket_process.yml\" ) uib . transform ( X ) . head ( 4 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } EDUCATION MARRIAGE LIMIT_BAL BILL_AMT1 0 0 0 8 5 1 2 0 3 4 2 0 0 8 5 3 0 1 4 0 Example with ScorecardPipelines \u00b6 skorecard supports converting scikit-learn pipelines to a SkorecardPipeline using to_skorecard_pipeline . This will add support for .save_yml() : from sklearn.pipeline import make_pipeline from skorecard.bucketers import EqualFrequencyBucketer from skorecard.pipeline.pipeline import to_skorecard_pipeline pipe = make_pipeline ( EqualFrequencyBucketer ( n_bins = 10 , variables = [ \"BILL_AMT1\" ]), DecisionTreeBucketer ( max_n_bins = 5 , variables = [ \"LIMIT_BAL\" ]) ) pipe . fit ( X , y ) sk_pipe = to_skorecard_pipeline ( pipe ) sk_pipe . save_yml ( \"pipe.yml\" ) uib = UserInputBucketer ( \"pipe.yml\" ) uib . transform ( X ) . head ( 4 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } EDUCATION MARRIAGE LIMIT_BAL BILL_AMT1 0 1 2 4 9 1 2 2 2 7 2 1 2 4 9 3 1 1 3 1","title":"Read/write buckets to file"},{"location":"howto/save_buckets_to_file/#saving-bucket-information-to-a-file","text":"If you have a specific set of bucketing boundaries you are satisfied with, it's useful to save them to a file. You might want to save the bucketing information as configuration files along with your code. All skorecard bucketers, the BucketingProcess and Skorecard model support saving to yaml files with save_yml() . The special UserInputBucketer can read in these configuration files and can be used in the final model pipeline.","title":"Saving bucket information to a file"},{"location":"howto/save_buckets_to_file/#example-with-a-bucketer","text":"from skorecard.datasets import load_uci_credit_card from skorecard.bucketers import DecisionTreeBucketer , UserInputBucketer X , y = load_uci_credit_card ( return_X_y = True ) bucketer = DecisionTreeBucketer ( max_n_bins = 10 ) bucketer = bucketer . fit ( X , y ) bucketer . save_yml ( \"bucketer.yml\" ) uib = UserInputBucketer ( \"bucketer.yml\" ) uib . transform ( X ) . head ( 4 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } EDUCATION MARRIAGE LIMIT_BAL BILL_AMT1 0 0 1 9 9 1 1 1 3 7 2 0 1 9 9 3 0 0 5 0","title":"Example with a bucketer"},{"location":"howto/save_buckets_to_file/#example-with-bucketingprocess","text":"A bucketing process works in exactly the same way. Because there is a prebucketing pipeline and a bucketing pipeline, skorecard makes sure that the buckets are the transformation from raw data to final bucket. from skorecard.pipeline import BucketingProcess from skorecard.bucketers import EqualFrequencyBucketer , OptimalBucketer , AsIsCategoricalBucketer from sklearn.pipeline import make_pipeline num_cols = [ \"LIMIT_BAL\" , \"BILL_AMT1\" ] cat_cols = [ \"EDUCATION\" , \"MARRIAGE\" ] bucketing_process = BucketingProcess ( prebucketing_pipeline = make_pipeline ( DecisionTreeBucketer ( variables = num_cols , max_n_bins = 100 , min_bin_size = 0.05 ), AsIsCategoricalBucketer ( variables = cat_cols ) ), bucketing_pipeline = make_pipeline ( OptimalBucketer ( variables = num_cols , max_n_bins = 10 , min_bin_size = 0.05 ), OptimalBucketer ( variables = cat_cols , variables_type = 'categorical' , max_n_bins = 10 , min_bin_size = 0.05 ), ) ) bucketing_process . fit ( X , y ) bucketing_process . save_yml ( \"bucket_process.yml\" ) uib = UserInputBucketer ( \"bucket_process.yml\" ) uib . transform ( X ) . head ( 4 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } EDUCATION MARRIAGE LIMIT_BAL BILL_AMT1 0 0 0 8 5 1 2 0 3 4 2 0 0 8 5 3 0 1 4 0","title":"Example with BucketingProcess"},{"location":"howto/save_buckets_to_file/#example-with-scorecardpipelines","text":"skorecard supports converting scikit-learn pipelines to a SkorecardPipeline using to_skorecard_pipeline . This will add support for .save_yml() : from sklearn.pipeline import make_pipeline from skorecard.bucketers import EqualFrequencyBucketer from skorecard.pipeline.pipeline import to_skorecard_pipeline pipe = make_pipeline ( EqualFrequencyBucketer ( n_bins = 10 , variables = [ \"BILL_AMT1\" ]), DecisionTreeBucketer ( max_n_bins = 5 , variables = [ \"LIMIT_BAL\" ]) ) pipe . fit ( X , y ) sk_pipe = to_skorecard_pipeline ( pipe ) sk_pipe . save_yml ( \"pipe.yml\" ) uib = UserInputBucketer ( \"pipe.yml\" ) uib . transform ( X ) . head ( 4 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } EDUCATION MARRIAGE LIMIT_BAL BILL_AMT1 0 1 2 4 9 1 2 2 2 7 2 1 2 4 9 3 1 1 3 1","title":"Example with ScorecardPipelines"},{"location":"howto/using_manually_defined_buckets/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Working with manually defined buckets \u00b6 Often bucketing is tweaked manually to incorporate domain expertise. Skorecard offers good support for manually defining buckets. From a bucketer \u00b6 If you've used .fit_interactive() (see interactive bucketing ), you can choose to explicitly use the updated bucket mapping in a UserInputBucketer : from skorecard.datasets import load_uci_credit_card , load_credit_card from skorecard.bucketers import DecisionTreeBucketer , UserInputBucketer X , y = load_uci_credit_card ( return_X_y = True ) bucketer = DecisionTreeBucketer ( variables = [ 'EDUCATION' ]) bucketer . fit ( X , y ) # can also be .fit_interactive() bucketer . features_bucket_mapping_ FeaturesBucketMapping([BucketMapping(feature_name='EDUCATION', type='numerical', missing_bucket=None, other_bucket=None, map=[1.5, 2.5], right=False, specials={})]) uib = UserInputBucketer ( bucketer . features_bucket_mapping_ ) uib . transform ( X ) . head ( 1 ) # note uib does not require a .fit() step .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } EDUCATION MARRIAGE LIMIT_BAL BILL_AMT1 0 0 2 400000.0 201800.0 From a dictionary \u00b6 You can manually define the buckets in a python dictionary. For every feature, the following keys must be present. feature_name (mandatory): must match the column name in the dataframe type (mandatory): type of feature (categorical or numerical) map (mandatory): contains the actual mapping for the bins. categorical features: expect a dictionary {value:bin_index} numerical features: expect a list of boundaries [value, value] right (optional, defaults to True ): flag that indicates if to include the upper bound (True) or lower bound (False) in the bucket definition. Applicable only to numerical bucketers specials (optional, defaults to {} ): dictionary of special values that will be put in their own bucket. Here's an example: bucket_maps = { 'EDUCATION' :{ \"feature_name\" : 'EDUCATION' , \"type\" : 'categorical' , \"map\" : { 2 : 0 , 1 : 1 , 3 : 2 }, \"right\" : True , \"specials\" : {} }, 'LIMIT_BAL' :{ \"feature_name\" : 'LIMIT_BAL' , \"type\" : 'numerical' , \"map\" : [ 25000. , 55000. , 105000. , 225000. , 275000. , 325000. ], \"right\" : True , \"specials\" : {} }, 'BILL_AMT1' :{ \"feature_name\" : 'BILL_AMT1' , \"type\" : 'numerical' , \"map\" : [ 800. , 12500 , 50000 , 77800 , 195000. ], \"right\" : True , \"specials\" : {} } } You can create a bucketer using the input dictionary using UserInputBucketer : from skorecard.bucketers import UserInputBucketer uib = UserInputBucketer ( bucket_maps ) From a file \u00b6 You can also work with manually defined buckets that have saved in a .yml file. See the how to on Read/write buckets to file .","title":"Manually defining buckets"},{"location":"howto/using_manually_defined_buckets/#working-with-manually-defined-buckets","text":"Often bucketing is tweaked manually to incorporate domain expertise. Skorecard offers good support for manually defining buckets.","title":"Working with manually defined buckets"},{"location":"howto/using_manually_defined_buckets/#from-a-bucketer","text":"If you've used .fit_interactive() (see interactive bucketing ), you can choose to explicitly use the updated bucket mapping in a UserInputBucketer : from skorecard.datasets import load_uci_credit_card , load_credit_card from skorecard.bucketers import DecisionTreeBucketer , UserInputBucketer X , y = load_uci_credit_card ( return_X_y = True ) bucketer = DecisionTreeBucketer ( variables = [ 'EDUCATION' ]) bucketer . fit ( X , y ) # can also be .fit_interactive() bucketer . features_bucket_mapping_ FeaturesBucketMapping([BucketMapping(feature_name='EDUCATION', type='numerical', missing_bucket=None, other_bucket=None, map=[1.5, 2.5], right=False, specials={})]) uib = UserInputBucketer ( bucketer . features_bucket_mapping_ ) uib . transform ( X ) . head ( 1 ) # note uib does not require a .fit() step .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } EDUCATION MARRIAGE LIMIT_BAL BILL_AMT1 0 0 2 400000.0 201800.0","title":"From a bucketer"},{"location":"howto/using_manually_defined_buckets/#from-a-dictionary","text":"You can manually define the buckets in a python dictionary. For every feature, the following keys must be present. feature_name (mandatory): must match the column name in the dataframe type (mandatory): type of feature (categorical or numerical) map (mandatory): contains the actual mapping for the bins. categorical features: expect a dictionary {value:bin_index} numerical features: expect a list of boundaries [value, value] right (optional, defaults to True ): flag that indicates if to include the upper bound (True) or lower bound (False) in the bucket definition. Applicable only to numerical bucketers specials (optional, defaults to {} ): dictionary of special values that will be put in their own bucket. Here's an example: bucket_maps = { 'EDUCATION' :{ \"feature_name\" : 'EDUCATION' , \"type\" : 'categorical' , \"map\" : { 2 : 0 , 1 : 1 , 3 : 2 }, \"right\" : True , \"specials\" : {} }, 'LIMIT_BAL' :{ \"feature_name\" : 'LIMIT_BAL' , \"type\" : 'numerical' , \"map\" : [ 25000. , 55000. , 105000. , 225000. , 275000. , 325000. ], \"right\" : True , \"specials\" : {} }, 'BILL_AMT1' :{ \"feature_name\" : 'BILL_AMT1' , \"type\" : 'numerical' , \"map\" : [ 800. , 12500 , 50000 , 77800 , 195000. ], \"right\" : True , \"specials\" : {} } } You can create a bucketer using the input dictionary using UserInputBucketer : from skorecard.bucketers import UserInputBucketer uib = UserInputBucketer ( bucket_maps )","title":"From a dictionary"},{"location":"howto/using_manually_defined_buckets/#from-a-file","text":"You can also work with manually defined buckets that have saved in a .yml file. See the how to on Read/write buckets to file .","title":"From a file"},{"location":"tutorials/1_bucketing/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Bucketing \u00b6 The core of a good skorecard model is to bucket the features. This section showcases how to use skorecard to achieve this. Let's start by loading the demo data from skorecard.datasets import load_credit_card data = load_credit_card ( as_frame = True ) data . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 ... x15 x16 x17 x18 x19 x20 x21 x22 x23 y 0 20000.0 2.0 2.0 1.0 24.0 2.0 2.0 -1.0 -1.0 -2.0 ... 0.0 0.0 0.0 0.0 689.0 0.0 0.0 0.0 0.0 1 1 120000.0 2.0 2.0 2.0 26.0 -1.0 2.0 0.0 0.0 0.0 ... 3272.0 3455.0 3261.0 0.0 1000.0 1000.0 1000.0 0.0 2000.0 1 2 90000.0 2.0 2.0 2.0 34.0 0.0 0.0 0.0 0.0 0.0 ... 14331.0 14948.0 15549.0 1518.0 1500.0 1000.0 1000.0 1000.0 5000.0 0 3 50000.0 2.0 2.0 1.0 37.0 0.0 0.0 0.0 0.0 0.0 ... 28314.0 28959.0 29547.0 2000.0 2019.0 1200.0 1100.0 1069.0 1000.0 0 4 50000.0 1.0 2.0 1.0 57.0 -1.0 0.0 -1.0 0.0 0.0 ... 20940.0 19146.0 19131.0 2000.0 36681.0 10000.0 9000.0 689.0 679.0 0 5 rows \u00d7 24 columns The dataset used contains 30K rows, 23 features and a binary target. Let's start by splitting the data in train and test sample, as per common practice when building a model. from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( data . drop ([ 'y' ], axis = 1 ), data [ 'y' ], test_size = 0.25 , random_state = 42 ) Define the numerical and categorical features \u00b6 The features x2 , x3 and x4 are of a categorical nature cat_cols = [ 'x2' , 'x3' , 'x4' ] num_cols = [ col for col in X_train . columns if col not in cat_cols ] print ( f \"Total categorical columns: { len ( cat_cols ) } \" ) print ( f \"Total numerical columns: { len ( num_cols ) } \" ) Total categorical columns: 3 Total numerical columns: 20 Bucketing features \u00b6 Skorecard implements different bucketers, but they are not applicable to all the features. Categorical features : In most of the cases, there is no numerical relationship between categories. Therefore automatic bucketing is very difficult to perform. skorecard implements the OrdinalCategoricalBucketer that orders the buckets either by the count or by the target rate. It includes a tolerance ( tol ) input, which represents the lower bound of the fraction of data allowed to keep a category in the same bucket. Numerical features : differently from categorical features, algorithmic bucketing can be applied to numerical features. skorecard implements different bucketers that can be used for numerical features: DecisionTreeBucketer : fits a univariate decision tree to find the optimal splits (requires the target y ) EqualWidthBucketer : generates buckets with equal spacing in the bucket boundaries (i.e. historgrams) EqualFrequencyBucketer : generates buckets with equal counts in the buckets (i.e. quantiles) AgglomerativeClusteringBucketer : generates bucketes by applying AgglomerativeClustering (density-based bucketer) Manual correction \u00b6 Note: to build a high quality scorecard, it's highly recommended to manually assess every bucket. Algorithms implemented in skorecard are very helpful, but are obscure to the business sense. This is especially true for categorical features, where business sense should prevail. Default bucketers \u00b6 Categorical features \u00b6 Due to (generally speaking) no relationship between categories, it's not possible to implement an algorithmic way of bucketing the values (in the same way as it is possible for numerical features). The only suitable bucketer for categorical features in skorecard is the OrdinalCategoricalBucketer , which groups together low-frequency categorical variables (all variables with a frequency below the tol threshold are put in the other bucket) Let's fix the tol to 5% , as this is the recommended minimum. # Start with categorical features from skorecard.bucketers import OrdinalCategoricalBucketer cat_bucketer = OrdinalCategoricalBucketer ( variables = cat_cols , tol = 0.05 , encoding_method = 'ordered' ) cat_bucketer . fit ( X_train , y_train ) OrdinalCategoricalBucketer(encoding_method='ordered', variables=['x2', 'x3', 'x4']) Example: Look up the feature 'x3' The feature 'x3' has a few categories that are sparsely-populated X_train [ 'x3' ] . value_counts ( normalize = True ) 2.0 0.466311 1.0 0.354089 3.0 0.163911 5.0 0.009333 4.0 0.004089 6.0 0.001822 0.0 0.000444 Name: x3, dtype: float64 The OrdinalCategorical bucketer populates the other category with sparse values cat_bucketer . bucket_table ( 'x3' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bucket label Count Count (%) Non-event Event Event Rate WoE IV 0 -2 Other 353.0 1.57 326.0 27.0 0.076487 1.239 0.016 1 -1 Missing 0.0 0.00 0.0 0.0 NaN 0.000 0.000 2 0 3.0 3688.0 16.39 2755.0 933.0 0.252983 -0.168 0.005 3 1 1.0 7967.0 35.41 6445.0 1522.0 0.191038 0.193 0.012 4 2 2.0 10492.0 46.63 7965.0 2527.0 0.240850 -0.102 0.005 Note There might be a different strategy applied to different bucketers. This is addressed by defining a pipeline of bucketers (see the numerical features section for details) Numerical features \u00b6 Numerical features allow for different bucketers (as described above). However, the recommended approach for bucketing is to use either the DecisionTreeBucketer or the BucketingProcess . X_train [ 'x1' ] . value_counts () 50000.0 2510 20000.0 1470 30000.0 1218 80000.0 1173 200000.0 1151 ... 650000.0 1 1000000.0 1 730000.0 1 690000.0 1 327680.0 1 Name: x1, Length: 79, dtype: int64 from skorecard.bucketers import DecisionTreeBucketer specials = { 'x1' :{ 'special_demo' :[ 50000 ]}} num_bucketer = DecisionTreeBucketer ( variables = num_cols , max_n_bins = 5 , #max number of bins allowed min_bin_size = 0.06 , # min fraction of data allowed in the bin dt_kwargs = { 'criterion' : \"entropy\" , 'min_impurity_decrease' : 0.0005 , #as in sklearn. Helps to decide how to split the buckets }, specials = specials ) num_bucketer . fit ( X_train , y_train ) DecisionTreeBucketer(dt_kwargs={'criterion': 'entropy', 'min_impurity_decrease': 0.0005, 'random_state': None}, max_n_bins=5, min_bin_size=0.06, specials={'x1': {'special_demo': [50000]}}, variables=['x1', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10', 'x11', 'x12', 'x13', 'x14', 'x15', 'x16', 'x17', 'x18', 'x19', 'x20', 'x21', 'x22', 'x23']) num_bucketer . bucket_table ( 'x1' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bucket label Count Count (%) Non-event Event Event Rate WoE IV 0 -3 Special: special_demo 2510.0 11.16 1848.0 662.0 0.263745 -0.224 0.006 1 -1 Missing 0.0 0.00 0.0 0.0 NaN 0.000 0.000 2 0 [-inf, 75000.0) 4399.0 19.55 2880.0 1519.0 0.345306 -0.611 0.085 3 1 [75000.0, 145000.0) 4567.0 20.30 3494.0 1073.0 0.234946 -0.070 0.001 4 2 [145000.0, 375000.0) 9230.0 41.02 7685.0 1545.0 0.167389 0.354 0.046 5 3 [375000.0, inf) 1794.0 7.97 1584.0 210.0 0.117057 0.770 0.037 See the bucket outputs (for the first three features) \u00b6 for x in num_cols [: 3 ]: display ( num_bucketer . fit ( X_train , y_train ) . bucket_table ( x )) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bucket label Count Count (%) Non-event Event Event Rate WoE IV 0 -3 Special: special_demo 2510.0 11.16 1848.0 662.0 0.263745 -0.224 0.006 1 -1 Missing 0.0 0.00 0.0 0.0 NaN 0.000 0.000 2 0 [-inf, 75000.0) 4399.0 19.55 2880.0 1519.0 0.345306 -0.611 0.085 3 1 [75000.0, 145000.0) 4567.0 20.30 3494.0 1073.0 0.234946 -0.070 0.001 4 2 [145000.0, 375000.0) 9230.0 41.02 7685.0 1545.0 0.167389 0.354 0.046 5 3 [375000.0, inf) 1794.0 7.97 1584.0 210.0 0.117057 0.770 0.037 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bucket label Count Count (%) Non-event Event Event Rate WoE IV 0 -1 Missing 0.0 0.00 0.0 0.0 NaN 0.000 0.000 1 0 [-inf, 25.5) 2927.0 13.01 2143.0 784.0 0.267851 -0.245 0.008 2 1 [25.5, 35.5) 9664.0 42.95 7750.0 1914.0 0.198055 0.148 0.009 3 2 [35.5, inf) 9909.0 44.04 7598.0 2311.0 0.233222 -0.060 0.002 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bucket label Count Count (%) Non-event Event Event Rate WoE IV 0 -1 Missing 0.0 0.00 0.0 0.0 NaN 0.000 0.000 1 0 [-inf, -0.5) 6370.0 28.31 5361.0 1009.0 0.158399 0.420 0.044 2 1 [-0.5, 0.5) 11032.0 49.03 9620.0 1412.0 0.127991 0.668 0.179 3 2 [0.5, 1.5) 2720.0 12.09 1796.0 924.0 0.339706 -0.586 0.048 4 3 [1.5, inf) 2378.0 10.57 714.0 1664.0 0.699748 -2.096 0.611 With different bucketers for different features in one go \u00b6 Note that below a warning is given to alert you that there are too many unique values in the numerical features. It is good to pay attention to these warnings, as the quantiles are approximate. from sklearn.pipeline import make_pipeline from skorecard.bucketers import EqualFrequencyBucketer , DecisionTreeBucketer pipe = make_pipeline ( EqualFrequencyBucketer ( n_bins = 5 , variables = num_cols [: 5 ]), DecisionTreeBucketer ( max_n_bins = 5 , variables = num_cols [ 5 :]) ) pipe . fit ( X_train , y_train ) /Users/iv58uq/Documents/open_source/skorecard/skorecard/bucketers/bucketers.py:502: ApproximationWarning: Approximated quantiles - too many unique values warnings.warn(ApproximationWarning(\"Approximated quantiles - too many unique values\")) /Users/iv58uq/Documents/open_source/skorecard/skorecard/bucketers/bucketers.py:502: ApproximationWarning: Approximated quantiles - too many unique values warnings.warn(ApproximationWarning(\"Approximated quantiles - too many unique values\")) /Users/iv58uq/Documents/open_source/skorecard/skorecard/bucketers/bucketers.py:502: ApproximationWarning: Approximated quantiles - too many unique values warnings.warn(ApproximationWarning(\"Approximated quantiles - too many unique values\")) Pipeline(steps=[('equalfrequencybucketer', EqualFrequencyBucketer(variables=['x1', 'x5', 'x6', 'x7', 'x8'])), ('decisiontreebucketer', DecisionTreeBucketer(max_n_bins=5, variables=['x9', 'x10', 'x11', 'x12', 'x13', 'x14', 'x15', 'x16', 'x17', 'x18', 'x19', 'x20', 'x21', 'x22', 'x23']))]) pipe . transform ( X_test ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 ... x14 x15 x16 x17 x18 x19 x20 x21 x22 x23 2308 0 1.0 2.0 2.0 0 1 1 1 2 2 ... 2 1 3 3 1 2 1 1 1 1 22404 2 2.0 1.0 2.0 0 1 1 1 2 2 ... 4 4 4 4 2 2 2 3 2 2 23397 1 2.0 3.0 1.0 2 1 1 1 2 2 ... 3 4 4 4 1 2 2 2 1 2 25058 2 1.0 3.0 2.0 4 1 1 1 2 2 ... 2 2 2 1 1 2 3 1 3 2 2664 0 2.0 2.0 2.0 2 1 1 1 2 2 ... 3 2 3 3 1 1 1 2 0 1 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 3211 0 2.0 3.0 1.0 4 2 0 0 0 0 ... 0 0 0 0 0 0 0 0 1 1 9355 4 2.0 2.0 1.0 3 1 1 1 2 2 ... 2 3 3 4 3 3 3 3 3 2 28201 4 2.0 3.0 2.0 4 2 2 2 1 2 ... 0 0 1 0 1 0 1 1 1 0 19705 1 2.0 2.0 1.0 4 0 0 0 1 1 ... 0 1 0 0 3 0 3 0 0 0 28313 2 2.0 3.0 1.0 2 0 0 0 1 1 ... 0 1 0 0 1 1 1 0 1 0 7500 rows \u00d7 23 columns Parenthesis: compare the buckets from two different algorithms \u00b6 By comparing the DecisionTreeBucketer in the first example, and the EqualFrequencyBucketer from the pipeline example, here comes a quick preview on assessing the two bucketing results. The first case results in the higher IV, with less buckets, hence it's definetely a better result! print ( f \" \\n DecisionTreeBucketer, with IV: { num_bucketer . bucket_table ( num_cols [ 0 ])[ 'IV' ] . sum () } \" ) display ( num_bucketer . bucket_table ( num_cols [ 0 ])) print ( f \" \\n EqualFrequencyBucketer, with IV: { pipe . steps [ 0 ][ 1 ] . bucket_table ( num_cols [ 0 ])[ 'IV' ] . sum () } \" ) display ( pipe . steps [ 0 ][ 1 ] . bucket_table ( num_cols [ 0 ])) DecisionTreeBucketer, with IV: 0.17500000000000002 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bucket label Count Count (%) Non-event Event Event Rate WoE IV 0 -3 Special: special_demo 2510.0 11.16 1848.0 662.0 0.263745 -0.224 0.006 1 -1 Missing 0.0 0.00 0.0 0.0 NaN 0.000 0.000 2 0 [-inf, 75000.0) 4399.0 19.55 2880.0 1519.0 0.345306 -0.611 0.085 3 1 [75000.0, 145000.0) 4567.0 20.30 3494.0 1073.0 0.234946 -0.070 0.001 4 2 [145000.0, 375000.0) 9230.0 41.02 7685.0 1545.0 0.167389 0.354 0.046 5 3 [375000.0, inf) 1794.0 7.97 1584.0 210.0 0.117057 0.770 0.037 EqualFrequencyBucketer, with IV: 0.159 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bucket label Count Count (%) Non-event Event Event Rate WoE IV 0 -1 Missing 0.0 0.00 0.0 0.0 NaN 0.000 0.000 1 0 (-inf, 50000.0] 5741.0 25.52 3885.0 1856.0 0.323289 -0.512 0.076 2 1 (50000.0, 100000.0] 3613.0 16.06 2691.0 922.0 0.255190 -0.179 0.005 3 2 (100000.0, 180000.0] 4629.0 20.57 3711.0 918.0 0.198315 0.146 0.004 4 3 (180000.0, 270000.0] 4062.0 18.05 3362.0 700.0 0.172329 0.319 0.017 5 4 (270000.0, inf] 4455.0 19.80 3842.0 613.0 0.137598 0.585 0.057 Make a pipeline for all the features \u00b6 So far we have shown how to deal with bucketers for categoricals/numericals. The whole process can be put together as in a scikit-learn pipeline. bucketing_pipe = make_pipeline ( num_bucketer , cat_bucketer ) bucketing_pipe . fit ( X_train , y_train ) Pipeline(steps=[('decisiontreebucketer', DecisionTreeBucketer(dt_kwargs={'criterion': 'entropy', 'min_impurity_decrease': 0.0005, 'random_state': None}, max_n_bins=5, min_bin_size=0.06, specials={'x1': {'special_demo': [50000]}}, variables=['x1', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10', 'x11', 'x12', 'x13', 'x14', 'x15', 'x16', 'x17', 'x18', 'x19', 'x20', 'x21', 'x22', 'x23'])), ('ordinalcategoricalbucketer', OrdinalCategoricalBucketer(encoding_method='ordered', variables=['x2', 'x3', 'x4']))]) Save the bucketers to file \u00b6 Once the buckets are satisfactory, save the ouputs to a yaml file from skorecard.pipeline import to_skorecard_pipeline to_skorecard_pipeline ( bucketing_pipe ) . save_yml ( open ( \"buckets.yml\" , \"w\" )) Using the bucketing process \u00b6 The most common approach in bucketing is to perform what is known as fine-coarse classing . In less fancy words: - you start with very loose bucketing requirements (many buckets, where some minimal (hopefully significant) aggregations and statistics can be computed - this is known as fine classing - for numerical features, it starts by merging together adjacent buckets with similar default rate/WoE - for categorical features, one should merge together categories with similar default rate/WoE, but only when it makes sense - the last two steps (or merging together buckets) is known as coarse classing In skorecard, this process is known as Bucketing Process , as shown below: The bucketing process starts by defining the loose (fine) buckets (prebucketing pipeline) It then runs an optimization algorithm, that merges the buckets together according to an optimization algorithm (bucketing pipeline) from skorecard.bucketers import DecisionTreeBucketer , OrdinalCategoricalBucketer , OptimalBucketer from skorecard.pipeline import BucketingProcess from sklearn.pipeline import make_pipeline specials = { 'x1' :{ 'special_demo' :[ 50000 ]}} prebucketing_pipeline = make_pipeline ( DecisionTreeBucketer ( variables = num_cols , max_n_bins = 40 , #loose requirements min_bin_size = 0.03 ), OrdinalCategoricalBucketer ( variables = cat_cols , tol = 0.02 ) ) bucketing_pipeline = make_pipeline ( OptimalBucketer ( variables = num_cols , max_n_bins = 6 , min_bin_size = 0.05 ), OptimalBucketer ( variables = cat_cols , variables_type = 'categorical' , max_n_bins = 10 , min_bin_size = 0.05 ) ) bucketing_process = BucketingProcess ( prebucketing_pipeline = prebucketing_pipeline , bucketing_pipeline = bucketing_pipeline , specials = specials , ) bucketing_process = bucketing_process . fit ( X_train , y_train ) Let's see the output of this optimization step \u00b6 print ( f \"Prebucketing step (fine classing), IV { bucketing_process . prebucket_table ( 'x1' )[ 'IV' ] . sum () } \" ) display ( bucketing_process . prebucket_table ( 'x1' )) print ( f \" \\n Bucketing step (coarse classing), IV { bucketing_process . bucket_table ( 'x1' )[ 'IV' ] . sum () } \" ) display ( bucketing_process . bucket_table ( 'x1' )) Prebucketing step (fine classing), IV 0.192 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pre-bucket label Count Count (%) Non-event Event Event Rate WoE IV bucket 0 -3 Special: special_demo 2510.0 11.16 1848.0 662.0 0.263745 -0.224 0.006 -3 1 -1 Missing 0.0 0.00 0.0 0.0 NaN 0.000 0.000 -1 2 0 [-inf, 25000.0) 1830.0 8.13 1153.0 677.0 0.369945 -0.718 0.050 0 3 1 [25000.0, 50000.0) 1401.0 6.23 884.0 517.0 0.369022 -0.714 0.038 0 4 2 [50000.0, 75000.0) 1168.0 5.19 843.0 325.0 0.278253 -0.297 0.005 1 5 3 [75000.0, 85000.0) 1173.0 5.21 907.0 266.0 0.226769 -0.024 0.000 2 6 4 [85000.0, 105000.0) 1272.0 5.65 941.0 331.0 0.260220 -0.206 0.003 2 7 5 [105000.0, 125000.0) 995.0 4.42 770.0 225.0 0.226131 -0.020 0.000 2 8 6 [125000.0, 145000.0) 1127.0 5.01 876.0 251.0 0.222715 -0.001 0.000 2 9 7 [145000.0, 155000.0) 821.0 3.65 683.0 138.0 0.168088 0.349 0.004 3 10 8 [155000.0, 175000.0) 937.0 4.16 768.0 169.0 0.180363 0.263 0.003 3 11 9 [175000.0, 185000.0) 749.0 3.33 614.0 135.0 0.180240 0.264 0.002 3 12 10 [185000.0, 205000.0) 1321.0 5.87 1070.0 251.0 0.190008 0.199 0.002 3 13 11 [205000.0, 225000.0) 877.0 3.90 729.0 148.0 0.168757 0.344 0.004 3 14 12 [225000.0, 245000.0) 1037.0 4.61 859.0 178.0 0.171649 0.323 0.004 3 15 13 [245000.0, 285000.0) 1193.0 5.30 1018.0 175.0 0.146689 0.510 0.012 4 16 14 [285000.0, 305000.0) 680.0 3.02 573.0 107.0 0.157353 0.427 0.005 4 17 15 [305000.0, 355000.0) 908.0 4.04 791.0 117.0 0.128855 0.660 0.014 4 18 16 [355000.0, 375000.0) 707.0 3.14 580.0 127.0 0.179632 0.268 0.002 4 19 17 [375000.0, 495000.0) 1098.0 4.88 965.0 133.0 0.121129 0.731 0.021 5 20 18 [495000.0, inf) 696.0 3.09 619.0 77.0 0.110632 0.833 0.017 5 Bucketing step (coarse classing), IV 0.186 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bucket label Count Count (%) Non-event Event Event Rate WoE IV 0 -3 Special: special_demo 2510.0 11.16 1848.0 662.0 0.263745 -0.224 0.006 1 -1 Missing 0.0 0.00 0.0 0.0 NaN 0.000 0.000 2 0 [-inf, 2.0) 3231.0 14.36 2037.0 1194.0 0.369545 -0.716 0.087 3 1 [2.0, 3.0) 1168.0 5.19 843.0 325.0 0.278253 -0.297 0.005 4 2 [3.0, 7.0) 4567.0 20.30 3494.0 1073.0 0.234946 -0.070 0.001 5 3 [7.0, 13.0) 5742.0 25.52 4723.0 1019.0 0.177464 0.283 0.019 6 4 [13.0, 17.0) 3488.0 15.50 2962.0 526.0 0.150803 0.478 0.031 7 5 [17.0, inf) 1794.0 7.97 1584.0 210.0 0.117057 0.770 0.037 Notice in the first table there is an additional column that is the optimized buckets (those indexes are aggregated in the second table). Because of the additive nature of the IV, by reducing the number of buckets, the IV will normally decrease. The goal is to reduce the number of buckets as much as possible, by keeping a high IV value, and check for monotonicity wherever needed. y_train . value_counts ( normalize = True ) 0 0.777378 1 0.222622 Name: y, dtype: float64 bucketing_process . plot_prebucket ( 'x1' , format = \"png\" , scale = 2 , width = 1050 , height = 525 ) bucketing_process . plot_bucket ( 'x1' , format = \"png\" , scale = 2 , width = 1050 , height = 525 ) bucketing_process . summary () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } column num_prebuckets num_buckets IV_score dtype 0 x1 21 8 0.002778 float64 1 x2 4 4 0.001921 float64 2 x3 5 5 0.002564 float64 3 x4 4 4 0.001909 float64 4 x5 24 3 0.001914 float64 5 x6 6 5 0.002351 float64 6 x7 5 3 0.002112 float64 7 x8 5 4 0.002129 float64 8 x9 5 4 0.002809 float64 9 x10 5 4 0.001997 float64 10 x11 5 4 0.002106 float64 11 x12 26 7 0.002408 float64 12 x13 27 6 0.002798 float64 13 x14 25 5 0.002571 float64 14 x15 22 6 0.002321 float64 15 x16 24 4 0.002236 float64 16 x17 25 4 0.002109 float64 17 x18 21 7 0.003035 float64 18 x19 21 7 0.003631 float64 19 x20 21 7 0.001999 float64 20 x21 22 7 0.002747 float64 21 x22 19 7 0.002383 float64 22 x23 20 7 0.002647 float64 bucketing_process . save_yml ( open ( 'best_bucketing.yml' , 'w' )) Manual bucket refinement \u00b6 Besides manually inspecting, it's often necessary to manually refine the buckets. skorecard implements a handy dash web-app that allows the user to redefine the bucket allocation. bucketing_process . fit ( X_train , y_train ) BucketingProcess(bucketing_pipeline=Pipeline(steps=[('optimalbucketer-1', OptimalBucketer(max_n_bins=6, variables=['x1', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10', 'x11', 'x12', 'x13', 'x14', 'x15', 'x16', 'x17', 'x18', 'x19', 'x20', 'x21', 'x22', 'x23'])), ('optimalbucketer-2', OptimalBucketer(variables=['x2', 'x3', 'x4'], variables_type='categorical'))]), prebucketing_pipeline=P...steps=[('decisiontreebucketer', DecisionTreeBucketer(max_n_bins=40, min_bin_size=0.03, variables=['x1', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10', 'x11', 'x12', 'x13', 'x14', 'x15', 'x16', 'x17', 'x18', 'x19', 'x20', 'x21', 'x22', 'x23'])), ('ordinalcategoricalbucketer', OrdinalCategoricalBucketer(tol=0.02, variables=['x2', 'x3', 'x4']))]), specials={'x1': {'special_demo': [50000]}}) Up next \u00b6 How to perform feature selection in a skorecard model","title":"Bucketing features"},{"location":"tutorials/1_bucketing/#bucketing","text":"The core of a good skorecard model is to bucket the features. This section showcases how to use skorecard to achieve this. Let's start by loading the demo data from skorecard.datasets import load_credit_card data = load_credit_card ( as_frame = True ) data . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 ... x15 x16 x17 x18 x19 x20 x21 x22 x23 y 0 20000.0 2.0 2.0 1.0 24.0 2.0 2.0 -1.0 -1.0 -2.0 ... 0.0 0.0 0.0 0.0 689.0 0.0 0.0 0.0 0.0 1 1 120000.0 2.0 2.0 2.0 26.0 -1.0 2.0 0.0 0.0 0.0 ... 3272.0 3455.0 3261.0 0.0 1000.0 1000.0 1000.0 0.0 2000.0 1 2 90000.0 2.0 2.0 2.0 34.0 0.0 0.0 0.0 0.0 0.0 ... 14331.0 14948.0 15549.0 1518.0 1500.0 1000.0 1000.0 1000.0 5000.0 0 3 50000.0 2.0 2.0 1.0 37.0 0.0 0.0 0.0 0.0 0.0 ... 28314.0 28959.0 29547.0 2000.0 2019.0 1200.0 1100.0 1069.0 1000.0 0 4 50000.0 1.0 2.0 1.0 57.0 -1.0 0.0 -1.0 0.0 0.0 ... 20940.0 19146.0 19131.0 2000.0 36681.0 10000.0 9000.0 689.0 679.0 0 5 rows \u00d7 24 columns The dataset used contains 30K rows, 23 features and a binary target. Let's start by splitting the data in train and test sample, as per common practice when building a model. from sklearn.model_selection import train_test_split X_train , X_test , y_train , y_test = train_test_split ( data . drop ([ 'y' ], axis = 1 ), data [ 'y' ], test_size = 0.25 , random_state = 42 )","title":"Bucketing"},{"location":"tutorials/1_bucketing/#define-the-numerical-and-categorical-features","text":"The features x2 , x3 and x4 are of a categorical nature cat_cols = [ 'x2' , 'x3' , 'x4' ] num_cols = [ col for col in X_train . columns if col not in cat_cols ] print ( f \"Total categorical columns: { len ( cat_cols ) } \" ) print ( f \"Total numerical columns: { len ( num_cols ) } \" ) Total categorical columns: 3 Total numerical columns: 20","title":"Define the numerical and categorical features"},{"location":"tutorials/1_bucketing/#bucketing-features","text":"Skorecard implements different bucketers, but they are not applicable to all the features. Categorical features : In most of the cases, there is no numerical relationship between categories. Therefore automatic bucketing is very difficult to perform. skorecard implements the OrdinalCategoricalBucketer that orders the buckets either by the count or by the target rate. It includes a tolerance ( tol ) input, which represents the lower bound of the fraction of data allowed to keep a category in the same bucket. Numerical features : differently from categorical features, algorithmic bucketing can be applied to numerical features. skorecard implements different bucketers that can be used for numerical features: DecisionTreeBucketer : fits a univariate decision tree to find the optimal splits (requires the target y ) EqualWidthBucketer : generates buckets with equal spacing in the bucket boundaries (i.e. historgrams) EqualFrequencyBucketer : generates buckets with equal counts in the buckets (i.e. quantiles) AgglomerativeClusteringBucketer : generates bucketes by applying AgglomerativeClustering (density-based bucketer)","title":"Bucketing features"},{"location":"tutorials/1_bucketing/#manual-correction","text":"Note: to build a high quality scorecard, it's highly recommended to manually assess every bucket. Algorithms implemented in skorecard are very helpful, but are obscure to the business sense. This is especially true for categorical features, where business sense should prevail.","title":"Manual correction"},{"location":"tutorials/1_bucketing/#default-bucketers","text":"","title":"Default bucketers"},{"location":"tutorials/1_bucketing/#categorical-features","text":"Due to (generally speaking) no relationship between categories, it's not possible to implement an algorithmic way of bucketing the values (in the same way as it is possible for numerical features). The only suitable bucketer for categorical features in skorecard is the OrdinalCategoricalBucketer , which groups together low-frequency categorical variables (all variables with a frequency below the tol threshold are put in the other bucket) Let's fix the tol to 5% , as this is the recommended minimum. # Start with categorical features from skorecard.bucketers import OrdinalCategoricalBucketer cat_bucketer = OrdinalCategoricalBucketer ( variables = cat_cols , tol = 0.05 , encoding_method = 'ordered' ) cat_bucketer . fit ( X_train , y_train ) OrdinalCategoricalBucketer(encoding_method='ordered', variables=['x2', 'x3', 'x4']) Example: Look up the feature 'x3' The feature 'x3' has a few categories that are sparsely-populated X_train [ 'x3' ] . value_counts ( normalize = True ) 2.0 0.466311 1.0 0.354089 3.0 0.163911 5.0 0.009333 4.0 0.004089 6.0 0.001822 0.0 0.000444 Name: x3, dtype: float64 The OrdinalCategorical bucketer populates the other category with sparse values cat_bucketer . bucket_table ( 'x3' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bucket label Count Count (%) Non-event Event Event Rate WoE IV 0 -2 Other 353.0 1.57 326.0 27.0 0.076487 1.239 0.016 1 -1 Missing 0.0 0.00 0.0 0.0 NaN 0.000 0.000 2 0 3.0 3688.0 16.39 2755.0 933.0 0.252983 -0.168 0.005 3 1 1.0 7967.0 35.41 6445.0 1522.0 0.191038 0.193 0.012 4 2 2.0 10492.0 46.63 7965.0 2527.0 0.240850 -0.102 0.005 Note There might be a different strategy applied to different bucketers. This is addressed by defining a pipeline of bucketers (see the numerical features section for details)","title":"Categorical features"},{"location":"tutorials/1_bucketing/#numerical-features","text":"Numerical features allow for different bucketers (as described above). However, the recommended approach for bucketing is to use either the DecisionTreeBucketer or the BucketingProcess . X_train [ 'x1' ] . value_counts () 50000.0 2510 20000.0 1470 30000.0 1218 80000.0 1173 200000.0 1151 ... 650000.0 1 1000000.0 1 730000.0 1 690000.0 1 327680.0 1 Name: x1, Length: 79, dtype: int64 from skorecard.bucketers import DecisionTreeBucketer specials = { 'x1' :{ 'special_demo' :[ 50000 ]}} num_bucketer = DecisionTreeBucketer ( variables = num_cols , max_n_bins = 5 , #max number of bins allowed min_bin_size = 0.06 , # min fraction of data allowed in the bin dt_kwargs = { 'criterion' : \"entropy\" , 'min_impurity_decrease' : 0.0005 , #as in sklearn. Helps to decide how to split the buckets }, specials = specials ) num_bucketer . fit ( X_train , y_train ) DecisionTreeBucketer(dt_kwargs={'criterion': 'entropy', 'min_impurity_decrease': 0.0005, 'random_state': None}, max_n_bins=5, min_bin_size=0.06, specials={'x1': {'special_demo': [50000]}}, variables=['x1', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10', 'x11', 'x12', 'x13', 'x14', 'x15', 'x16', 'x17', 'x18', 'x19', 'x20', 'x21', 'x22', 'x23']) num_bucketer . bucket_table ( 'x1' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bucket label Count Count (%) Non-event Event Event Rate WoE IV 0 -3 Special: special_demo 2510.0 11.16 1848.0 662.0 0.263745 -0.224 0.006 1 -1 Missing 0.0 0.00 0.0 0.0 NaN 0.000 0.000 2 0 [-inf, 75000.0) 4399.0 19.55 2880.0 1519.0 0.345306 -0.611 0.085 3 1 [75000.0, 145000.0) 4567.0 20.30 3494.0 1073.0 0.234946 -0.070 0.001 4 2 [145000.0, 375000.0) 9230.0 41.02 7685.0 1545.0 0.167389 0.354 0.046 5 3 [375000.0, inf) 1794.0 7.97 1584.0 210.0 0.117057 0.770 0.037","title":"Numerical features"},{"location":"tutorials/1_bucketing/#see-the-bucket-outputs-for-the-first-three-features","text":"for x in num_cols [: 3 ]: display ( num_bucketer . fit ( X_train , y_train ) . bucket_table ( x )) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bucket label Count Count (%) Non-event Event Event Rate WoE IV 0 -3 Special: special_demo 2510.0 11.16 1848.0 662.0 0.263745 -0.224 0.006 1 -1 Missing 0.0 0.00 0.0 0.0 NaN 0.000 0.000 2 0 [-inf, 75000.0) 4399.0 19.55 2880.0 1519.0 0.345306 -0.611 0.085 3 1 [75000.0, 145000.0) 4567.0 20.30 3494.0 1073.0 0.234946 -0.070 0.001 4 2 [145000.0, 375000.0) 9230.0 41.02 7685.0 1545.0 0.167389 0.354 0.046 5 3 [375000.0, inf) 1794.0 7.97 1584.0 210.0 0.117057 0.770 0.037 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bucket label Count Count (%) Non-event Event Event Rate WoE IV 0 -1 Missing 0.0 0.00 0.0 0.0 NaN 0.000 0.000 1 0 [-inf, 25.5) 2927.0 13.01 2143.0 784.0 0.267851 -0.245 0.008 2 1 [25.5, 35.5) 9664.0 42.95 7750.0 1914.0 0.198055 0.148 0.009 3 2 [35.5, inf) 9909.0 44.04 7598.0 2311.0 0.233222 -0.060 0.002 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bucket label Count Count (%) Non-event Event Event Rate WoE IV 0 -1 Missing 0.0 0.00 0.0 0.0 NaN 0.000 0.000 1 0 [-inf, -0.5) 6370.0 28.31 5361.0 1009.0 0.158399 0.420 0.044 2 1 [-0.5, 0.5) 11032.0 49.03 9620.0 1412.0 0.127991 0.668 0.179 3 2 [0.5, 1.5) 2720.0 12.09 1796.0 924.0 0.339706 -0.586 0.048 4 3 [1.5, inf) 2378.0 10.57 714.0 1664.0 0.699748 -2.096 0.611","title":"See the bucket outputs (for the first three features)"},{"location":"tutorials/1_bucketing/#with-different-bucketers-for-different-features-in-one-go","text":"Note that below a warning is given to alert you that there are too many unique values in the numerical features. It is good to pay attention to these warnings, as the quantiles are approximate. from sklearn.pipeline import make_pipeline from skorecard.bucketers import EqualFrequencyBucketer , DecisionTreeBucketer pipe = make_pipeline ( EqualFrequencyBucketer ( n_bins = 5 , variables = num_cols [: 5 ]), DecisionTreeBucketer ( max_n_bins = 5 , variables = num_cols [ 5 :]) ) pipe . fit ( X_train , y_train ) /Users/iv58uq/Documents/open_source/skorecard/skorecard/bucketers/bucketers.py:502: ApproximationWarning: Approximated quantiles - too many unique values warnings.warn(ApproximationWarning(\"Approximated quantiles - too many unique values\")) /Users/iv58uq/Documents/open_source/skorecard/skorecard/bucketers/bucketers.py:502: ApproximationWarning: Approximated quantiles - too many unique values warnings.warn(ApproximationWarning(\"Approximated quantiles - too many unique values\")) /Users/iv58uq/Documents/open_source/skorecard/skorecard/bucketers/bucketers.py:502: ApproximationWarning: Approximated quantiles - too many unique values warnings.warn(ApproximationWarning(\"Approximated quantiles - too many unique values\")) Pipeline(steps=[('equalfrequencybucketer', EqualFrequencyBucketer(variables=['x1', 'x5', 'x6', 'x7', 'x8'])), ('decisiontreebucketer', DecisionTreeBucketer(max_n_bins=5, variables=['x9', 'x10', 'x11', 'x12', 'x13', 'x14', 'x15', 'x16', 'x17', 'x18', 'x19', 'x20', 'x21', 'x22', 'x23']))]) pipe . transform ( X_test ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 ... x14 x15 x16 x17 x18 x19 x20 x21 x22 x23 2308 0 1.0 2.0 2.0 0 1 1 1 2 2 ... 2 1 3 3 1 2 1 1 1 1 22404 2 2.0 1.0 2.0 0 1 1 1 2 2 ... 4 4 4 4 2 2 2 3 2 2 23397 1 2.0 3.0 1.0 2 1 1 1 2 2 ... 3 4 4 4 1 2 2 2 1 2 25058 2 1.0 3.0 2.0 4 1 1 1 2 2 ... 2 2 2 1 1 2 3 1 3 2 2664 0 2.0 2.0 2.0 2 1 1 1 2 2 ... 3 2 3 3 1 1 1 2 0 1 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 3211 0 2.0 3.0 1.0 4 2 0 0 0 0 ... 0 0 0 0 0 0 0 0 1 1 9355 4 2.0 2.0 1.0 3 1 1 1 2 2 ... 2 3 3 4 3 3 3 3 3 2 28201 4 2.0 3.0 2.0 4 2 2 2 1 2 ... 0 0 1 0 1 0 1 1 1 0 19705 1 2.0 2.0 1.0 4 0 0 0 1 1 ... 0 1 0 0 3 0 3 0 0 0 28313 2 2.0 3.0 1.0 2 0 0 0 1 1 ... 0 1 0 0 1 1 1 0 1 0 7500 rows \u00d7 23 columns","title":"With different bucketers for different features in one go"},{"location":"tutorials/1_bucketing/#parenthesis-compare-the-buckets-from-two-different-algorithms","text":"By comparing the DecisionTreeBucketer in the first example, and the EqualFrequencyBucketer from the pipeline example, here comes a quick preview on assessing the two bucketing results. The first case results in the higher IV, with less buckets, hence it's definetely a better result! print ( f \" \\n DecisionTreeBucketer, with IV: { num_bucketer . bucket_table ( num_cols [ 0 ])[ 'IV' ] . sum () } \" ) display ( num_bucketer . bucket_table ( num_cols [ 0 ])) print ( f \" \\n EqualFrequencyBucketer, with IV: { pipe . steps [ 0 ][ 1 ] . bucket_table ( num_cols [ 0 ])[ 'IV' ] . sum () } \" ) display ( pipe . steps [ 0 ][ 1 ] . bucket_table ( num_cols [ 0 ])) DecisionTreeBucketer, with IV: 0.17500000000000002 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bucket label Count Count (%) Non-event Event Event Rate WoE IV 0 -3 Special: special_demo 2510.0 11.16 1848.0 662.0 0.263745 -0.224 0.006 1 -1 Missing 0.0 0.00 0.0 0.0 NaN 0.000 0.000 2 0 [-inf, 75000.0) 4399.0 19.55 2880.0 1519.0 0.345306 -0.611 0.085 3 1 [75000.0, 145000.0) 4567.0 20.30 3494.0 1073.0 0.234946 -0.070 0.001 4 2 [145000.0, 375000.0) 9230.0 41.02 7685.0 1545.0 0.167389 0.354 0.046 5 3 [375000.0, inf) 1794.0 7.97 1584.0 210.0 0.117057 0.770 0.037 EqualFrequencyBucketer, with IV: 0.159 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bucket label Count Count (%) Non-event Event Event Rate WoE IV 0 -1 Missing 0.0 0.00 0.0 0.0 NaN 0.000 0.000 1 0 (-inf, 50000.0] 5741.0 25.52 3885.0 1856.0 0.323289 -0.512 0.076 2 1 (50000.0, 100000.0] 3613.0 16.06 2691.0 922.0 0.255190 -0.179 0.005 3 2 (100000.0, 180000.0] 4629.0 20.57 3711.0 918.0 0.198315 0.146 0.004 4 3 (180000.0, 270000.0] 4062.0 18.05 3362.0 700.0 0.172329 0.319 0.017 5 4 (270000.0, inf] 4455.0 19.80 3842.0 613.0 0.137598 0.585 0.057","title":"Parenthesis: compare the buckets from two different algorithms"},{"location":"tutorials/1_bucketing/#make-a-pipeline-for-all-the-features","text":"So far we have shown how to deal with bucketers for categoricals/numericals. The whole process can be put together as in a scikit-learn pipeline. bucketing_pipe = make_pipeline ( num_bucketer , cat_bucketer ) bucketing_pipe . fit ( X_train , y_train ) Pipeline(steps=[('decisiontreebucketer', DecisionTreeBucketer(dt_kwargs={'criterion': 'entropy', 'min_impurity_decrease': 0.0005, 'random_state': None}, max_n_bins=5, min_bin_size=0.06, specials={'x1': {'special_demo': [50000]}}, variables=['x1', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10', 'x11', 'x12', 'x13', 'x14', 'x15', 'x16', 'x17', 'x18', 'x19', 'x20', 'x21', 'x22', 'x23'])), ('ordinalcategoricalbucketer', OrdinalCategoricalBucketer(encoding_method='ordered', variables=['x2', 'x3', 'x4']))])","title":"Make a pipeline for all the features"},{"location":"tutorials/1_bucketing/#save-the-bucketers-to-file","text":"Once the buckets are satisfactory, save the ouputs to a yaml file from skorecard.pipeline import to_skorecard_pipeline to_skorecard_pipeline ( bucketing_pipe ) . save_yml ( open ( \"buckets.yml\" , \"w\" ))","title":"Save the bucketers to file"},{"location":"tutorials/1_bucketing/#using-the-bucketing-process","text":"The most common approach in bucketing is to perform what is known as fine-coarse classing . In less fancy words: - you start with very loose bucketing requirements (many buckets, where some minimal (hopefully significant) aggregations and statistics can be computed - this is known as fine classing - for numerical features, it starts by merging together adjacent buckets with similar default rate/WoE - for categorical features, one should merge together categories with similar default rate/WoE, but only when it makes sense - the last two steps (or merging together buckets) is known as coarse classing In skorecard, this process is known as Bucketing Process , as shown below: The bucketing process starts by defining the loose (fine) buckets (prebucketing pipeline) It then runs an optimization algorithm, that merges the buckets together according to an optimization algorithm (bucketing pipeline) from skorecard.bucketers import DecisionTreeBucketer , OrdinalCategoricalBucketer , OptimalBucketer from skorecard.pipeline import BucketingProcess from sklearn.pipeline import make_pipeline specials = { 'x1' :{ 'special_demo' :[ 50000 ]}} prebucketing_pipeline = make_pipeline ( DecisionTreeBucketer ( variables = num_cols , max_n_bins = 40 , #loose requirements min_bin_size = 0.03 ), OrdinalCategoricalBucketer ( variables = cat_cols , tol = 0.02 ) ) bucketing_pipeline = make_pipeline ( OptimalBucketer ( variables = num_cols , max_n_bins = 6 , min_bin_size = 0.05 ), OptimalBucketer ( variables = cat_cols , variables_type = 'categorical' , max_n_bins = 10 , min_bin_size = 0.05 ) ) bucketing_process = BucketingProcess ( prebucketing_pipeline = prebucketing_pipeline , bucketing_pipeline = bucketing_pipeline , specials = specials , ) bucketing_process = bucketing_process . fit ( X_train , y_train )","title":"Using the bucketing process"},{"location":"tutorials/1_bucketing/#lets-see-the-output-of-this-optimization-step","text":"print ( f \"Prebucketing step (fine classing), IV { bucketing_process . prebucket_table ( 'x1' )[ 'IV' ] . sum () } \" ) display ( bucketing_process . prebucket_table ( 'x1' )) print ( f \" \\n Bucketing step (coarse classing), IV { bucketing_process . bucket_table ( 'x1' )[ 'IV' ] . sum () } \" ) display ( bucketing_process . bucket_table ( 'x1' )) Prebucketing step (fine classing), IV 0.192 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pre-bucket label Count Count (%) Non-event Event Event Rate WoE IV bucket 0 -3 Special: special_demo 2510.0 11.16 1848.0 662.0 0.263745 -0.224 0.006 -3 1 -1 Missing 0.0 0.00 0.0 0.0 NaN 0.000 0.000 -1 2 0 [-inf, 25000.0) 1830.0 8.13 1153.0 677.0 0.369945 -0.718 0.050 0 3 1 [25000.0, 50000.0) 1401.0 6.23 884.0 517.0 0.369022 -0.714 0.038 0 4 2 [50000.0, 75000.0) 1168.0 5.19 843.0 325.0 0.278253 -0.297 0.005 1 5 3 [75000.0, 85000.0) 1173.0 5.21 907.0 266.0 0.226769 -0.024 0.000 2 6 4 [85000.0, 105000.0) 1272.0 5.65 941.0 331.0 0.260220 -0.206 0.003 2 7 5 [105000.0, 125000.0) 995.0 4.42 770.0 225.0 0.226131 -0.020 0.000 2 8 6 [125000.0, 145000.0) 1127.0 5.01 876.0 251.0 0.222715 -0.001 0.000 2 9 7 [145000.0, 155000.0) 821.0 3.65 683.0 138.0 0.168088 0.349 0.004 3 10 8 [155000.0, 175000.0) 937.0 4.16 768.0 169.0 0.180363 0.263 0.003 3 11 9 [175000.0, 185000.0) 749.0 3.33 614.0 135.0 0.180240 0.264 0.002 3 12 10 [185000.0, 205000.0) 1321.0 5.87 1070.0 251.0 0.190008 0.199 0.002 3 13 11 [205000.0, 225000.0) 877.0 3.90 729.0 148.0 0.168757 0.344 0.004 3 14 12 [225000.0, 245000.0) 1037.0 4.61 859.0 178.0 0.171649 0.323 0.004 3 15 13 [245000.0, 285000.0) 1193.0 5.30 1018.0 175.0 0.146689 0.510 0.012 4 16 14 [285000.0, 305000.0) 680.0 3.02 573.0 107.0 0.157353 0.427 0.005 4 17 15 [305000.0, 355000.0) 908.0 4.04 791.0 117.0 0.128855 0.660 0.014 4 18 16 [355000.0, 375000.0) 707.0 3.14 580.0 127.0 0.179632 0.268 0.002 4 19 17 [375000.0, 495000.0) 1098.0 4.88 965.0 133.0 0.121129 0.731 0.021 5 20 18 [495000.0, inf) 696.0 3.09 619.0 77.0 0.110632 0.833 0.017 5 Bucketing step (coarse classing), IV 0.186 .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bucket label Count Count (%) Non-event Event Event Rate WoE IV 0 -3 Special: special_demo 2510.0 11.16 1848.0 662.0 0.263745 -0.224 0.006 1 -1 Missing 0.0 0.00 0.0 0.0 NaN 0.000 0.000 2 0 [-inf, 2.0) 3231.0 14.36 2037.0 1194.0 0.369545 -0.716 0.087 3 1 [2.0, 3.0) 1168.0 5.19 843.0 325.0 0.278253 -0.297 0.005 4 2 [3.0, 7.0) 4567.0 20.30 3494.0 1073.0 0.234946 -0.070 0.001 5 3 [7.0, 13.0) 5742.0 25.52 4723.0 1019.0 0.177464 0.283 0.019 6 4 [13.0, 17.0) 3488.0 15.50 2962.0 526.0 0.150803 0.478 0.031 7 5 [17.0, inf) 1794.0 7.97 1584.0 210.0 0.117057 0.770 0.037 Notice in the first table there is an additional column that is the optimized buckets (those indexes are aggregated in the second table). Because of the additive nature of the IV, by reducing the number of buckets, the IV will normally decrease. The goal is to reduce the number of buckets as much as possible, by keeping a high IV value, and check for monotonicity wherever needed. y_train . value_counts ( normalize = True ) 0 0.777378 1 0.222622 Name: y, dtype: float64 bucketing_process . plot_prebucket ( 'x1' , format = \"png\" , scale = 2 , width = 1050 , height = 525 ) bucketing_process . plot_bucket ( 'x1' , format = \"png\" , scale = 2 , width = 1050 , height = 525 ) bucketing_process . summary () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } column num_prebuckets num_buckets IV_score dtype 0 x1 21 8 0.002778 float64 1 x2 4 4 0.001921 float64 2 x3 5 5 0.002564 float64 3 x4 4 4 0.001909 float64 4 x5 24 3 0.001914 float64 5 x6 6 5 0.002351 float64 6 x7 5 3 0.002112 float64 7 x8 5 4 0.002129 float64 8 x9 5 4 0.002809 float64 9 x10 5 4 0.001997 float64 10 x11 5 4 0.002106 float64 11 x12 26 7 0.002408 float64 12 x13 27 6 0.002798 float64 13 x14 25 5 0.002571 float64 14 x15 22 6 0.002321 float64 15 x16 24 4 0.002236 float64 16 x17 25 4 0.002109 float64 17 x18 21 7 0.003035 float64 18 x19 21 7 0.003631 float64 19 x20 21 7 0.001999 float64 20 x21 22 7 0.002747 float64 21 x22 19 7 0.002383 float64 22 x23 20 7 0.002647 float64 bucketing_process . save_yml ( open ( 'best_bucketing.yml' , 'w' ))","title":"Let's see the output of this optimization step"},{"location":"tutorials/1_bucketing/#manual-bucket-refinement","text":"Besides manually inspecting, it's often necessary to manually refine the buckets. skorecard implements a handy dash web-app that allows the user to redefine the bucket allocation. bucketing_process . fit ( X_train , y_train ) BucketingProcess(bucketing_pipeline=Pipeline(steps=[('optimalbucketer-1', OptimalBucketer(max_n_bins=6, variables=['x1', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10', 'x11', 'x12', 'x13', 'x14', 'x15', 'x16', 'x17', 'x18', 'x19', 'x20', 'x21', 'x22', 'x23'])), ('optimalbucketer-2', OptimalBucketer(variables=['x2', 'x3', 'x4'], variables_type='categorical'))]), prebucketing_pipeline=P...steps=[('decisiontreebucketer', DecisionTreeBucketer(max_n_bins=40, min_bin_size=0.03, variables=['x1', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10', 'x11', 'x12', 'x13', 'x14', 'x15', 'x16', 'x17', 'x18', 'x19', 'x20', 'x21', 'x22', 'x23'])), ('ordinalcategoricalbucketer', OrdinalCategoricalBucketer(tol=0.02, variables=['x2', 'x3', 'x4']))]), specials={'x1': {'special_demo': [50000]}})","title":"Manual bucket refinement"},{"location":"tutorials/1_bucketing/#up-next","text":"How to perform feature selection in a skorecard model","title":"Up next"},{"location":"tutorials/2_feature_selection/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Once the buckets are defined, the next step is to perform the feature selection. \u00b6 In building a skorecard model, there are a few recommended steps to felect the features. Calculate the information values (IV) to identify the very predictive features Calculate the Population Stability Index (PSI) to identify the unstable features Evaluate the multicollinearity of the features that pass the previous two steps The information value IV \u00b6 The information value is defined by the following equation \\[IV = \\sum_{i}(\\%n_{i}^{y=0} - \\%n_{i}^{y=1})\\frac{\\%n_{i}^{y=0}}{\\%n_{i}^{y=1}}\\] where \\(i\\) is the bucket index, \\(\\%n_{i}^{y=0}\\) represents the fraction counts of target 0 in the bucket, while \\(\\%n_{i}^{y=1}\\) represents the fraction of the counts of target 1 in the bucket \\(i\\) . The IV is a weighted sum of the Weight of Evidences (WoE) of every bin. The higher the value, the larger the separation between the classes: in other words the more predictive the feature is. As a rule of thumb: - IV < 0.02 non-predictive feature - 0.02 < IV < 0.1 predictive feature - IV > 0.1 very predictive feature The Population Stability index (PSI) \u00b6 The PSI measures the similarity between two samples. The PSI is defined as \\[PSI = \\sum_{i}(\\%n_{i}^{X1} - \\%n_{i}^{X2})\\frac{\\%n_{i}^{X1}}{\\%n_{i}^{X2}}\\] where \\(i\\) is the bucket index, \\(\\%n_{i}^{X1}\\) represents the fraction counts of the feature in the sample X1, while \\(\\%n_{i}^{y=1}\\) represents the fraction counts of the feature in the sample X2 in the bucket \\(i\\) . It's the same definition as in the IV. However, here large values indicate a difference between two samples, therefore for the selection we look at the lower values: - IV < 0.02 stable feature - 0.02 < IV < 0.1 unstable, but acceptable, feature - IV > 0.1 unstable feature Let's load the data as in the previous tutorials and split it into train and test. import pandas as pd from skorecard.datasets import load_credit_card from sklearn.model_selection import train_test_split data = load_credit_card ( as_frame = True ) X_train , X_test , y_train , y_test = train_test_split ( data . drop ([ 'y' ], axis = 1 ), data [ 'y' ], test_size = 0.25 , random_state = 42 ) Load the saved buckets \u00b6 import yaml buckets_dict = yaml . safe_load ( open ( \"buckets.yml\" , 'r' )) Define the bucketer, using the UserInputBucketer from skorecard.bucketers import UserInputBucketer uib = UserInputBucketer ( buckets_dict ) X_train_bins = uib . fit_transform ( X_train , y_train ) X_test_bins = uib . transform ( X_test ) X_train_bins .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 ... x14 x15 x16 x17 x18 x19 x20 x21 x22 x23 21177 2 0 0 0 1 1 0 0 0 0 ... 0 0 3 3 1 1 1 2 2 2 23942 0 0 2 0 0 1 0 0 0 0 ... 0 0 2 2 1 1 1 3 2 1 1247 1 1 2 0 1 0 0 0 0 0 ... 0 0 0 0 1 0 0 0 0 0 23622 2 1 2 1 2 2 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 28454 0 1 2 0 2 1 0 0 0 0 ... 0 0 2 2 1 1 1 1 1 1 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 29802 -3 0 2 0 1 1 0 0 0 0 ... 0 0 2 1 1 1 1 1 1 3 5390 2 0 1 0 2 3 1 1 1 1 ... 0 0 3 3 2 2 0 3 2 2 860 -3 0 1 0 1 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 15795 0 1 2 0 0 1 0 0 0 1 ... 0 0 2 2 1 1 1 1 0 1 23654 2 1 2 1 2 0 0 0 0 0 ... 0 0 1 2 0 1 1 2 3 1 22500 rows \u00d7 23 columns uib . bucket_table ( 'x1' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bucket label Count Count (%) Non-event Event Event Rate WoE IV 0 -3 Special: special_demo 2510.0 11.16 1848.0 662.0 0.263745 -0.224 0.006 1 -1 Missing 0.0 0.00 0.0 0.0 NaN 0.000 0.000 2 0 [-inf, 75000.0) 4399.0 19.55 2880.0 1519.0 0.345306 -0.611 0.085 3 1 [75000.0, 145000.0) 4567.0 20.30 3494.0 1073.0 0.234946 -0.070 0.001 4 2 [145000.0, 375000.0) 9230.0 41.02 7685.0 1545.0 0.167389 0.354 0.046 5 3 [375000.0, inf) 1794.0 7.97 1584.0 210.0 0.117057 0.770 0.037 Calculating the Information Value \u00b6 The information value can be calculated by the iv() function in the reporting module. from skorecard.reporting import iv iv_dict = iv ( X_train_bins , y_train ) iv_values = pd . Series ( iv_dict ) . sort_values ( ascending = False ) # sort them by predicting power iv_values . head ( 5 ) x19 0.003325 x3 0.002968 x13 0.002634 x18 0.002503 x1 0.002457 dtype: float64 As an abritrary threshold, we can select the features where the IV values are above 0.002 preselected_features = iv_values [ iv_values > 0.002 ] . index . tolist () print ( f \"Total selected features by IV: { len ( preselected_features ) } \" ) Total selected features by IV: 15 Calculate the PSI \u00b6 Similar to the IV, by using the psi function in the report module. from skorecard.reporting import psi psi_dict = psi ( X_train_bins , X_test_bins ) psi_values = pd . Series ( psi_dict ) psi_values . sort_values ( ascending = False ) . head ( 5 ) x6 0.000996 x2 0.000702 x12 0.000697 x19 0.000443 x21 0.000357 dtype: float64 In this particular case, all the features have a very low PSI, hence no instability is present and no feature selection is performed. Removing multi-collinear features \u00b6 A skorecard model is based on a logistic regression algorithm. Logistic regression suffers from multi-collinearity (i.e. highly correlated features) by design. There are multiple ways of measuring it, such as VIF and correlations Here we are showing the approach with correlations. First, let's build an easy visualization function import numpy as np import matplotlib.pyplot as plt import seaborn as sns % matplotlib inline def plot_correlations ( corr ): plt . figure ( figsize = ( 10 , 8 ), constrained_layout = True ) cmap = plt . cm . get_cmap ( \"RdBu\" ) matrix = np . triu ( corr ) sns . heatmap ( corr , vmin =- 1 , vmax = 1 , annot = True , mask = matrix , cmap = cmap , annot_kws = { \"fontsize\" : 6 }) Adding the WoE Encoder \u00b6 A default scorecard model is defined by the following steps: - bucketing - encoder (a default one is a WoE encoder) - logistic regression model As the input of the logistic regression model is the dataset transformed to its WoE, first let's build the pipline with the first two steps and transform the dataset from skorecard.preprocessing import WoeEncoder from sklearn.pipeline import make_pipeline woe_pipe = make_pipeline ( uib , WoeEncoder () ) X_train_woe = woe_pipe . fit_transform ( X_train , y_train ) preselected_features ['x19', 'x3', 'x13', 'x18', 'x1', 'x6', 'x21', 'x12', 'x16', 'x23', 'x22', 'x7', 'x17', 'x11', 'x5'] X_train_corr = X_train_woe [ preselected_features ] . corr () plot_correlations ( X_train_corr ) As a rule of thumb, correlations above 0.6 can be considered problematic for the logisitc regression model (this threshold might depend heeavily on dataset and use case). The following code snippet illustrates a recursive feature elimination step, where features are sorted by their IV importance, and correlated features with lower IV importance are removed corr_limit = 0.6 # correlation threshold drop_feats = list () # keep_feats = list() for ix , feature in enumerate ( preselected_features ): if feature in drop_feats : continue remaining_features = [ feat for feat in preselected_features [ ix :] # check the next feature in the preselected step if feat not in drop_feats and feat != feature ] if len ( remaining_features ) == 0 : continue # go to the next step if the features at step x have already been removeed # find the correlated features with the remaining preselected features # both positive and negative correlations matter, hence the abs() corr_feats = X_train_corr . loc [ remaining_features , feature ] . apply ( lambda x : abs ( x )) drop_at_step = corr_feats [ corr_feats > corr_limit ] . index . tolist () # append the new features to the list drop_feats += drop_at_step # Select thee features with low correlations good_feats = [ feat for feat in preselected_features if feat not in drop_feats ] print ( f \"Total preselected features: { len ( preselected_features ) } \" ) print ( f \"Total features dropped due too high correlations: { len ( drop_feats ) } \" ) print ( f \"Total selected features: { len ( good_feats ) } \" ) Total preselected features: 15 Total features dropped due too high correlations: 2 Total selected features: 13 Visualizing the correlation of the good features to verify that the RFE step worked plot_correlations ( X_train_woe [ good_feats ] . corr ()) and the final list of selected features is shown below good_feats ['x19', 'x3', 'x13', 'x18', 'x1', 'x6', 'x21', 'x16', 'x23', 'x22', 'x17', 'x11', 'x5'] Up next \u00b6 After performing the feature selection, it's time to build the final Skorecard model.","title":"Selecting features"},{"location":"tutorials/2_feature_selection/#once-the-buckets-are-defined-the-next-step-is-to-perform-the-feature-selection","text":"In building a skorecard model, there are a few recommended steps to felect the features. Calculate the information values (IV) to identify the very predictive features Calculate the Population Stability Index (PSI) to identify the unstable features Evaluate the multicollinearity of the features that pass the previous two steps","title":"Once the buckets are defined, the next step is to perform the feature selection."},{"location":"tutorials/2_feature_selection/#the-information-value-iv","text":"The information value is defined by the following equation \\[IV = \\sum_{i}(\\%n_{i}^{y=0} - \\%n_{i}^{y=1})\\frac{\\%n_{i}^{y=0}}{\\%n_{i}^{y=1}}\\] where \\(i\\) is the bucket index, \\(\\%n_{i}^{y=0}\\) represents the fraction counts of target 0 in the bucket, while \\(\\%n_{i}^{y=1}\\) represents the fraction of the counts of target 1 in the bucket \\(i\\) . The IV is a weighted sum of the Weight of Evidences (WoE) of every bin. The higher the value, the larger the separation between the classes: in other words the more predictive the feature is. As a rule of thumb: - IV < 0.02 non-predictive feature - 0.02 < IV < 0.1 predictive feature - IV > 0.1 very predictive feature","title":"The information value IV"},{"location":"tutorials/2_feature_selection/#the-population-stability-index-psi","text":"The PSI measures the similarity between two samples. The PSI is defined as \\[PSI = \\sum_{i}(\\%n_{i}^{X1} - \\%n_{i}^{X2})\\frac{\\%n_{i}^{X1}}{\\%n_{i}^{X2}}\\] where \\(i\\) is the bucket index, \\(\\%n_{i}^{X1}\\) represents the fraction counts of the feature in the sample X1, while \\(\\%n_{i}^{y=1}\\) represents the fraction counts of the feature in the sample X2 in the bucket \\(i\\) . It's the same definition as in the IV. However, here large values indicate a difference between two samples, therefore for the selection we look at the lower values: - IV < 0.02 stable feature - 0.02 < IV < 0.1 unstable, but acceptable, feature - IV > 0.1 unstable feature Let's load the data as in the previous tutorials and split it into train and test. import pandas as pd from skorecard.datasets import load_credit_card from sklearn.model_selection import train_test_split data = load_credit_card ( as_frame = True ) X_train , X_test , y_train , y_test = train_test_split ( data . drop ([ 'y' ], axis = 1 ), data [ 'y' ], test_size = 0.25 , random_state = 42 )","title":"The Population Stability index (PSI)"},{"location":"tutorials/2_feature_selection/#load-the-saved-buckets","text":"import yaml buckets_dict = yaml . safe_load ( open ( \"buckets.yml\" , 'r' )) Define the bucketer, using the UserInputBucketer from skorecard.bucketers import UserInputBucketer uib = UserInputBucketer ( buckets_dict ) X_train_bins = uib . fit_transform ( X_train , y_train ) X_test_bins = uib . transform ( X_test ) X_train_bins .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 ... x14 x15 x16 x17 x18 x19 x20 x21 x22 x23 21177 2 0 0 0 1 1 0 0 0 0 ... 0 0 3 3 1 1 1 2 2 2 23942 0 0 2 0 0 1 0 0 0 0 ... 0 0 2 2 1 1 1 3 2 1 1247 1 1 2 0 1 0 0 0 0 0 ... 0 0 0 0 1 0 0 0 0 0 23622 2 1 2 1 2 2 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 28454 0 1 2 0 2 1 0 0 0 0 ... 0 0 2 2 1 1 1 1 1 1 ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 29802 -3 0 2 0 1 1 0 0 0 0 ... 0 0 2 1 1 1 1 1 1 3 5390 2 0 1 0 2 3 1 1 1 1 ... 0 0 3 3 2 2 0 3 2 2 860 -3 0 1 0 1 0 0 0 0 0 ... 0 0 0 0 0 0 0 0 0 0 15795 0 1 2 0 0 1 0 0 0 1 ... 0 0 2 2 1 1 1 1 0 1 23654 2 1 2 1 2 0 0 0 0 0 ... 0 0 1 2 0 1 1 2 3 1 22500 rows \u00d7 23 columns uib . bucket_table ( 'x1' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bucket label Count Count (%) Non-event Event Event Rate WoE IV 0 -3 Special: special_demo 2510.0 11.16 1848.0 662.0 0.263745 -0.224 0.006 1 -1 Missing 0.0 0.00 0.0 0.0 NaN 0.000 0.000 2 0 [-inf, 75000.0) 4399.0 19.55 2880.0 1519.0 0.345306 -0.611 0.085 3 1 [75000.0, 145000.0) 4567.0 20.30 3494.0 1073.0 0.234946 -0.070 0.001 4 2 [145000.0, 375000.0) 9230.0 41.02 7685.0 1545.0 0.167389 0.354 0.046 5 3 [375000.0, inf) 1794.0 7.97 1584.0 210.0 0.117057 0.770 0.037","title":"Load the saved buckets"},{"location":"tutorials/2_feature_selection/#calculating-the-information-value","text":"The information value can be calculated by the iv() function in the reporting module. from skorecard.reporting import iv iv_dict = iv ( X_train_bins , y_train ) iv_values = pd . Series ( iv_dict ) . sort_values ( ascending = False ) # sort them by predicting power iv_values . head ( 5 ) x19 0.003325 x3 0.002968 x13 0.002634 x18 0.002503 x1 0.002457 dtype: float64 As an abritrary threshold, we can select the features where the IV values are above 0.002 preselected_features = iv_values [ iv_values > 0.002 ] . index . tolist () print ( f \"Total selected features by IV: { len ( preselected_features ) } \" ) Total selected features by IV: 15","title":"Calculating the Information Value"},{"location":"tutorials/2_feature_selection/#calculate-the-psi","text":"Similar to the IV, by using the psi function in the report module. from skorecard.reporting import psi psi_dict = psi ( X_train_bins , X_test_bins ) psi_values = pd . Series ( psi_dict ) psi_values . sort_values ( ascending = False ) . head ( 5 ) x6 0.000996 x2 0.000702 x12 0.000697 x19 0.000443 x21 0.000357 dtype: float64 In this particular case, all the features have a very low PSI, hence no instability is present and no feature selection is performed.","title":"Calculate the PSI"},{"location":"tutorials/2_feature_selection/#removing-multi-collinear-features","text":"A skorecard model is based on a logistic regression algorithm. Logistic regression suffers from multi-collinearity (i.e. highly correlated features) by design. There are multiple ways of measuring it, such as VIF and correlations Here we are showing the approach with correlations. First, let's build an easy visualization function import numpy as np import matplotlib.pyplot as plt import seaborn as sns % matplotlib inline def plot_correlations ( corr ): plt . figure ( figsize = ( 10 , 8 ), constrained_layout = True ) cmap = plt . cm . get_cmap ( \"RdBu\" ) matrix = np . triu ( corr ) sns . heatmap ( corr , vmin =- 1 , vmax = 1 , annot = True , mask = matrix , cmap = cmap , annot_kws = { \"fontsize\" : 6 })","title":"Removing multi-collinear features"},{"location":"tutorials/2_feature_selection/#adding-the-woe-encoder","text":"A default scorecard model is defined by the following steps: - bucketing - encoder (a default one is a WoE encoder) - logistic regression model As the input of the logistic regression model is the dataset transformed to its WoE, first let's build the pipline with the first two steps and transform the dataset from skorecard.preprocessing import WoeEncoder from sklearn.pipeline import make_pipeline woe_pipe = make_pipeline ( uib , WoeEncoder () ) X_train_woe = woe_pipe . fit_transform ( X_train , y_train ) preselected_features ['x19', 'x3', 'x13', 'x18', 'x1', 'x6', 'x21', 'x12', 'x16', 'x23', 'x22', 'x7', 'x17', 'x11', 'x5'] X_train_corr = X_train_woe [ preselected_features ] . corr () plot_correlations ( X_train_corr ) As a rule of thumb, correlations above 0.6 can be considered problematic for the logisitc regression model (this threshold might depend heeavily on dataset and use case). The following code snippet illustrates a recursive feature elimination step, where features are sorted by their IV importance, and correlated features with lower IV importance are removed corr_limit = 0.6 # correlation threshold drop_feats = list () # keep_feats = list() for ix , feature in enumerate ( preselected_features ): if feature in drop_feats : continue remaining_features = [ feat for feat in preselected_features [ ix :] # check the next feature in the preselected step if feat not in drop_feats and feat != feature ] if len ( remaining_features ) == 0 : continue # go to the next step if the features at step x have already been removeed # find the correlated features with the remaining preselected features # both positive and negative correlations matter, hence the abs() corr_feats = X_train_corr . loc [ remaining_features , feature ] . apply ( lambda x : abs ( x )) drop_at_step = corr_feats [ corr_feats > corr_limit ] . index . tolist () # append the new features to the list drop_feats += drop_at_step # Select thee features with low correlations good_feats = [ feat for feat in preselected_features if feat not in drop_feats ] print ( f \"Total preselected features: { len ( preselected_features ) } \" ) print ( f \"Total features dropped due too high correlations: { len ( drop_feats ) } \" ) print ( f \"Total selected features: { len ( good_feats ) } \" ) Total preselected features: 15 Total features dropped due too high correlations: 2 Total selected features: 13 Visualizing the correlation of the good features to verify that the RFE step worked plot_correlations ( X_train_woe [ good_feats ] . corr ()) and the final list of selected features is shown below good_feats ['x19', 'x3', 'x13', 'x18', 'x1', 'x6', 'x21', 'x16', 'x23', 'x22', 'x17', 'x11', 'x5']","title":"Adding the WoE Encoder"},{"location":"tutorials/2_feature_selection/#up-next","text":"After performing the feature selection, it's time to build the final Skorecard model.","title":"Up next"},{"location":"tutorials/3_skorecard_model/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Building a scorecard model \u00b6 This tutorial shows how to build a skorecard model. Start by loading the data and performiing the train test split: import pandas as pd import numpy as np import matplotlib.pyplot as plt % matplotlib inline from skorecard.datasets import load_credit_card from sklearn.model_selection import train_test_split data = load_credit_card ( as_frame = True ) X_train , X_test , y_train , y_test = train_test_split ( data . drop ([ 'y' ], axis = 1 ), data [ 'y' ], test_size = 0.25 , random_state = 42 ) Load the buckets and the selected features that were created in the previous tutorials. import yaml buckets_dict = yaml . safe_load ( open ( \"buckets.yml\" , 'r' )) selected_features = [ 'x6' , 'x8' , 'x10' , 'x18' , 'x1' , 'x19' , 'x20' , 'x21' , 'x23' , 'x22' , 'x3' , 'x17' , 'x16' ] Define the scorecard model \u00b6 A Skorecard class has two main components: - the bucketer - the list of selected features (if None is passed, it uses all the features defined in the bucketer) It behaves like a scikit-learn model from skorecard import Skorecard from skorecard.bucketers import UserInputBucketer scorecard = Skorecard ( bucketing = UserInputBucketer ( buckets_dict ), variables = selected_features , calculate_stats = True ) scorecard = scorecard . fit ( X_train , y_train ) The get_stats method returns the coefficients with their standard error and p-values scorecard . get_stats () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Coef. Std.Err z P>|z| const -1.242955 0.018238 -68.152266 0.000000e+00 x6 0.765416 0.020622 37.116970 1.495993e-301 x8 0.276995 0.033657 8.229831 1.874781e-16 x10 0.323640 0.036329 8.908558 5.170055e-19 x18 0.226277 0.050398 4.489820 7.128339e-06 x1 0.394922 0.048176 8.197563 2.453087e-16 x19 0.165650 0.055950 2.960659 3.069817e-03 x20 0.254235 0.062924 4.040349 5.337170e-05 x21 0.097257 0.072504 1.341396 1.797919e-01 x23 0.176958 0.074045 2.389866 1.685452e-02 x22 0.110976 0.076718 1.446541 1.480256e-01 x3 0.443555 0.096009 4.619955 3.838229e-06 x17 0.203349 0.133504 1.523175 1.277148e-01 x16 -0.166103 0.142547 -1.165254 2.439163e-01 Retrieve the model performance like in any sklearn classifier from sklearn.metrics import roc_auc_score , classification_report proba_train = scorecard . predict_proba ( X_train )[:, 1 ] proba_test = scorecard . predict_proba ( X_test )[:, 1 ] print ( f \"AUC train: { round ( roc_auc_score ( y_train , proba_train ), 4 ) } \" ) print ( f \"AUC test : { round ( roc_auc_score ( y_test , proba_test ), 4 ) } \\n \" ) print ( classification_report ( y_test , scorecard . predict ( X_test ))) AUC train:0.7714 AUC test :0.7642 precision recall f1-score support 0 0.84 0.95 0.89 5873 1 0.66 0.34 0.45 1627 accuracy 0.82 7500 macro avg 0.75 0.65 0.67 7500 weighted avg 0.80 0.82 0.80 7500 Removing features based on their statistical properties \u00b6 Features can be further removed. In a scorecard model, the coefficients are expected to be between 0 and -1. Coefficients smaller than -1 indicate that the model relies heavily on features (likely to overfit), while positive coefficients show an inverted trend. Additionally, p-values of the coefficients should be smaller that 0.05. (or 0.01). Looking at the stats table above, this would suggest removing the following features from the list ['x21','x16','x17','x22'] . Note that feature removal should be done carefully, as every time the feature is removed, the coefficients might converge elsewhere, and would hence give a different model with a different interpretation. new_feats = [ feat for feat in selected_features if feat not in [ 'x21' , 'x16' , 'x17' , 'x22' ]] scorecard = Skorecard ( UserInputBucketer ( buckets_dict ), variables = new_feats , calculate_stats = True ) scorecard = scorecard . fit ( X_train , y_train ) model_stats = scorecard . get_stats () model_stats . index = [ 'Const' ] + new_feats display ( model_stats ) proba_train = scorecard . predict_proba ( X_train )[:, 1 ] proba_test = scorecard . predict_proba ( X_test )[:, 1 ] print ( f \"AUC train: { round ( roc_auc_score ( y_train , proba_train ), 4 ) } \" ) print ( f \"AUC test : { round ( roc_auc_score ( y_test , proba_test ), 4 ) } \" ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Coef. Std.Err z P>|z| Const -1.242598 0.018229 -68.166667 0.000000e+00 x6 0.763946 0.020596 37.092613 3.695794e-301 x8 0.269057 0.033234 8.095809 5.688515e-16 x10 0.339016 0.035156 9.643180 5.253488e-22 x18 0.241832 0.049687 4.867076 1.132612e-06 x1 0.409354 0.046618 8.781019 1.619998e-18 x19 0.191910 0.053865 3.562812 3.669030e-04 x20 0.282166 0.060879 4.634866 3.571691e-06 x23 0.227794 0.069624 3.271788 1.068698e-03 x3 0.441695 0.095994 4.601264 4.199341e-06 AUC train:0.7712 AUC test :0.7648 Retrieving the transformed data \u00b6 Buckets and WoE transformations are available directly in a fitted skorecard model print ( \"Top 5 rows and the transformed buckets\" ) display ( scorecard . bucket_transform ( X_test )[ new_feats ] . head ()) print ( \" \\n Top 5 rows and the transformed WoEs\" ) display ( scorecard . woe_transform ( X_test )[ new_feats ] . head ()) Top 5 rows and the transformed buckets .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x6 x8 x10 x18 x1 x19 x20 x23 x3 2308 1 0 0 1 0 1 1 1 2 22404 1 0 0 1 2 1 1 2 1 23397 1 0 0 1 0 1 1 2 0 25058 1 0 0 1 1 1 2 2 0 2664 1 0 0 1 -3 1 1 1 2 Top 5 rows and the transformed WoEs .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x6 x8 x10 x18 x1 x19 x20 x23 x3 2308 -0.668068 -0.321161 -0.230059 -0.029168 0.610738 0.000397 -0.015444 0.105318 0.102411 22404 -0.668068 -0.321161 -0.230059 -0.029168 -0.353564 0.000397 -0.015444 -0.256887 -0.192612 23397 -0.668068 -0.321161 -0.230059 -0.029168 0.610738 0.000397 -0.015444 -0.256887 0.168106 25058 -0.668068 -0.321161 -0.230059 -0.029168 0.070222 0.000397 -0.405293 -0.256887 0.168106 2664 -0.668068 -0.321161 -0.230059 -0.029168 0.224540 0.000397 -0.015444 0.105318 0.102411 Getting the feature importance (to be integrated in the skorecard class) \u00b6 In order to talk of feature importance, we should consider both the coefficients and the IV of the single feature. The importance cab be approximated as the product of the two numbers. from skorecard.reporting import iv X_train_bins = scorecard . bucket_transform ( X_train ) iv_dict = iv ( X_train_bins , y_train ) iv_values = pd . Series ( iv_dict ) . sort_values ( ascending = False ) iv_values . name = \"IV\" feat_importance = model_stats [[ 'Coef.' ]] . join ( iv_values ) feat_importance [ 'importance' ] = - 1. * feat_importance [ 'Coef.' ] * feat_importance [ 'IV' ] feat_importance . sort_values ( by = 'importance' , ascending = False ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Coef. IV importance x23 0.227794 0.002257 -0.000514 x8 0.269057 0.001924 -0.000518 x20 0.282166 0.001998 -0.000564 x18 0.241832 0.002503 -0.000605 x19 0.191910 0.003325 -0.000638 x10 0.339016 0.001917 -0.000650 x1 0.409354 0.002457 -0.001006 x3 0.441695 0.002968 -0.001311 x6 0.763946 0.002430 -0.001857 Const -1.242598 NaN NaN Scaling the scores \u00b6 The last step of building skorecard models is the rescaling of the predictions. This is a very common practice within the Credit Risk domain, where scorecard models are widely used. Rescaling scorecards has no impact on the model performance, but rather returns the predictions on an arbitrary scale (normally from 0-1000) which are more meaningful for risk managers and underwriters in a bank than probabilities. The rescaling is a linear transfromation performed on the log-odds of the predicted probability \\(p\\) , \\[ log(\\frac{1-p}{p}) \\] Where the odds are defined as: \\[ \\frac{1-p}{p} \\] The reference for the linear transformation are commonly defined by the following values: ref_score : reference score, that should match a given reference odds (ref_odds) ref_odds : reference odds that should match a giver reference score pdo : points to double the odds, number of points to add where the odds double. An example: with the following settings: ref_score = 400 ref_odds = 20 pdo = 25 A score of 400 corresponds to the odds 20:1 of being a \"good client\" ( y=0 ). This means that the predicted probability for y=1 is in this case ~4.76% , which you can get by rearranging the equation for the odds, above. When the score increases to 425 , the odds double to 40:1 (predicted probability to be y=1 is ~2,43% ). When the score decreases to 375 , the odds are reduced by a factor 2, ie, 10:1 (predicted probability to be y=1 is ~9,09% ). In skorecard , one can use the calibrate_to_master_scale function. from skorecard.rescale import calibrate_to_master_scale proba_train = pd . Series ( proba_train , index = y_train . index ) . sort_values () # sorting for visualization purposes scores = calibrate_to_master_scale ( proba_train , pdo = 25 , ref_score = 400 , ref_odds = 20 ) Visualize the score dependencies \u00b6 fig , ( ax1 , ax2 , ax3 ) = plt . subplots ( 3 , sharex = True , figsize = ( 8 , 12 ), gridspec_kw = { 'hspace' : 0 }) ax1 . plot ( scores . values , proba_train . values ) ax1 . set_ylabel ( \"Predicted probability\" ) ax1 . set_title ( \"Rescaled scores and probabilities\" ) ax1 . grid () ax2 . plot ( scores . values , proba_train . apply ( lambda x : ( 1 - x ) / x ) . values ) ax2 . set_ylabel ( \"Odds\" ) ax2 . grid () ax3 . plot ( scores . values , proba_train . apply ( lambda x : np . log ( 1 - x ) - np . log ( x )) . values ,) ax3 . set_ylabel ( \"log-odds\" ) ax3 . grid () ax3 . set_xlabel ( \"Rescaled scores\" ) plt . show () Assigning points to every feature \u00b6 The last step of a scorecard development is to convert all the features into the rescaled model. A scorecard model is a logisitic regression fitted on the WoE values of every single bucketed feature. In other words, the following equations holds: \\[ log(odds) = log(\\frac{1-p}{p}) = \\beta_{0} + \\sum_{i} \\beta_{i} \\cdot WOE(X_{i}) \\] As the rescaling performed earlier is linear in the predicted log-odds , this means that the every feature-bucket contribution can be rescaled to an integer value (by rescaling directly the $ \\beta_{i} \\cdot WOE(X_{i})$ factors with the same calculations. This returns the final scorecard, that can be easily implemented. The functionality in skorecard to rescale the features is as follows from skorecard.rescale import ScoreCardPoints # ensure that pdo, ref_score and ref_odds are consistent scp = ScoreCardPoints ( skorecard_model = scorecard , pdo = 25 , ref_score = 400 , ref_odds = 20 ) one can extract the final scorecard as follows scp . get_scorecard_points () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bin_index map woe feature coef contribution Points 0 -2 NaN 0.000000 x6 0.763946 0.000000 37 1 -1 Missing 0.000000 x6 0.763946 0.000000 37 3 1 [-0.5, 0.5) -0.668068 x6 0.763946 -0.510368 56 4 2 [0.5, 1.5) -0.419221 x6 0.763946 -0.320262 49 5 3 [1.5, inf) 0.586085 x6 0.763946 0.447737 21 6 4 NaN 2.095463 x6 0.763946 1.600820 -20 7 -2 NaN 0.000000 x8 0.269057 0.000000 37 8 -1 Missing 0.000000 x8 0.269057 0.000000 37 10 1 [1.5, inf) -0.321161 x8 0.269057 -0.086411 41 11 2 NaN 1.392671 x8 0.269057 0.374708 24 12 -2 NaN 0.000000 x10 0.339016 0.000000 37 13 -1 Missing 0.000000 x10 0.339016 0.000000 37 15 1 [1.0, inf) -0.230059 x10 0.339016 -0.077994 40 16 2 NaN 1.491617 x10 0.339016 0.505681 19 17 -2 NaN 0.000000 x18 0.241832 0.000000 37 18 -1 Missing 0.000000 x18 0.241832 0.000000 37 20 1 [21.0, 4552.5) -0.029168 x18 0.241832 -0.007054 38 21 2 [4552.5, 15001.5) 0.692001 x18 0.241832 0.167348 31 22 3 [15001.5, inf) -0.457263 x18 0.241832 -0.110581 41 23 4 NaN -0.860845 x18 0.241832 -0.208180 45 25 -2 NaN 0.000000 x1 0.409354 0.000000 37 26 -1 Missing 0.000000 x1 0.409354 0.000000 37 28 1 [75000.0, 145000.0) -0.353564 x1 0.409354 -0.144733 43 29 2 [145000.0, 375000.0) 0.610738 x1 0.409354 0.250008 28 30 3 [375000.0, inf) 0.070222 x1 0.409354 0.028746 36 31 4 NaN -0.766316 x1 0.409354 -0.313695 49 32 5 NaN 0.224540 x1 0.409354 0.091916 34 33 -2 NaN 0.000000 x19 0.191910 0.000000 37 34 -1 Missing 0.000000 x19 0.191910 0.000000 37 36 1 [131.5, 4970.5) 0.000397 x19 0.191910 0.000076 37 37 2 [4970.5, 15001.0) 0.576300 x19 0.191910 0.110598 33 38 3 [15001.0, inf) -0.403868 x19 0.191910 -0.077506 40 39 4 NaN -1.162523 x19 0.191910 -0.223100 45 40 -2 NaN 0.000000 x20 0.282166 0.000000 37 41 -1 Missing 0.000000 x20 0.282166 0.000000 37 43 1 [16.5, 4513.5) -0.015444 x20 0.282166 -0.004358 38 44 2 [4513.5, 12490.5) 0.509437 x20 0.282166 0.143746 32 45 3 [12490.5, inf) -0.405293 x20 0.282166 -0.114360 42 46 4 NaN -0.832020 x20 0.282166 -0.234768 46 47 -2 NaN 0.000000 x23 0.227794 0.000000 37 48 -1 Missing 0.000000 x23 0.227794 0.000000 37 50 1 [1.5, 2000.5) -0.256887 x23 0.227794 -0.058517 40 51 2 [2000.5, 9849.5) 0.105318 x23 0.227794 0.023991 37 52 3 [9849.5, inf) 0.350867 x23 0.227794 0.079925 35 53 4 NaN -0.701982 x23 0.227794 -0.159907 43 54 -2 Other 0.000000 x3 0.441695 0.000000 37 55 -1 Missing 0.000000 x3 0.441695 0.000000 37 57 1 1.0 0.168106 x3 0.441695 0.074252 35 58 2 2.0 0.102411 x3 0.441695 0.045235 36 59 3 NaN -0.192612 x3 0.441695 -0.085076 40 60 4 NaN -1.207590 x3 0.441695 -0.533387 57 61 0 0 0.000000 Intercept -1.242598 -0.000000 0 Or one can apply the transformation directly on the data, by calling the transform method, in order to map each feature to its actual points. Validate the rescaling \u00b6 As the last step, in order to ensure that the rescaling was successfull, one can verify that the sum of the points of each row in the dataset matches the rescaled scores. The rescaling steps has some integer rounding, therefore small discrepancies of 1-2 points might occur due to the rounding error proba_train = pd . Series ( proba_train , index = y_train . index ) #convert to pandas and correct index in order to be able to perform the diff scores = calibrate_to_master_scale ( proba_train , pdo = 25 , ref_score = 400 , ref_odds = 20 ) # Check the distribution of the differences ( scores - scp . transform ( X_train ) . sum ( axis = 1 )) . value_counts () 111.0 878 105.0 708 146.0 560 125.0 516 130.0 442 ... 284.0 1 292.0 1 -54.0 1 261.0 1 -80.0 1 Length: 373, dtype: int64","title":"Scorecard model"},{"location":"tutorials/3_skorecard_model/#building-a-scorecard-model","text":"This tutorial shows how to build a skorecard model. Start by loading the data and performiing the train test split: import pandas as pd import numpy as np import matplotlib.pyplot as plt % matplotlib inline from skorecard.datasets import load_credit_card from sklearn.model_selection import train_test_split data = load_credit_card ( as_frame = True ) X_train , X_test , y_train , y_test = train_test_split ( data . drop ([ 'y' ], axis = 1 ), data [ 'y' ], test_size = 0.25 , random_state = 42 ) Load the buckets and the selected features that were created in the previous tutorials. import yaml buckets_dict = yaml . safe_load ( open ( \"buckets.yml\" , 'r' )) selected_features = [ 'x6' , 'x8' , 'x10' , 'x18' , 'x1' , 'x19' , 'x20' , 'x21' , 'x23' , 'x22' , 'x3' , 'x17' , 'x16' ]","title":"Building a scorecard model"},{"location":"tutorials/3_skorecard_model/#define-the-scorecard-model","text":"A Skorecard class has two main components: - the bucketer - the list of selected features (if None is passed, it uses all the features defined in the bucketer) It behaves like a scikit-learn model from skorecard import Skorecard from skorecard.bucketers import UserInputBucketer scorecard = Skorecard ( bucketing = UserInputBucketer ( buckets_dict ), variables = selected_features , calculate_stats = True ) scorecard = scorecard . fit ( X_train , y_train ) The get_stats method returns the coefficients with their standard error and p-values scorecard . get_stats () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Coef. Std.Err z P>|z| const -1.242955 0.018238 -68.152266 0.000000e+00 x6 0.765416 0.020622 37.116970 1.495993e-301 x8 0.276995 0.033657 8.229831 1.874781e-16 x10 0.323640 0.036329 8.908558 5.170055e-19 x18 0.226277 0.050398 4.489820 7.128339e-06 x1 0.394922 0.048176 8.197563 2.453087e-16 x19 0.165650 0.055950 2.960659 3.069817e-03 x20 0.254235 0.062924 4.040349 5.337170e-05 x21 0.097257 0.072504 1.341396 1.797919e-01 x23 0.176958 0.074045 2.389866 1.685452e-02 x22 0.110976 0.076718 1.446541 1.480256e-01 x3 0.443555 0.096009 4.619955 3.838229e-06 x17 0.203349 0.133504 1.523175 1.277148e-01 x16 -0.166103 0.142547 -1.165254 2.439163e-01 Retrieve the model performance like in any sklearn classifier from sklearn.metrics import roc_auc_score , classification_report proba_train = scorecard . predict_proba ( X_train )[:, 1 ] proba_test = scorecard . predict_proba ( X_test )[:, 1 ] print ( f \"AUC train: { round ( roc_auc_score ( y_train , proba_train ), 4 ) } \" ) print ( f \"AUC test : { round ( roc_auc_score ( y_test , proba_test ), 4 ) } \\n \" ) print ( classification_report ( y_test , scorecard . predict ( X_test ))) AUC train:0.7714 AUC test :0.7642 precision recall f1-score support 0 0.84 0.95 0.89 5873 1 0.66 0.34 0.45 1627 accuracy 0.82 7500 macro avg 0.75 0.65 0.67 7500 weighted avg 0.80 0.82 0.80 7500","title":"Define the scorecard model"},{"location":"tutorials/3_skorecard_model/#removing-features-based-on-their-statistical-properties","text":"Features can be further removed. In a scorecard model, the coefficients are expected to be between 0 and -1. Coefficients smaller than -1 indicate that the model relies heavily on features (likely to overfit), while positive coefficients show an inverted trend. Additionally, p-values of the coefficients should be smaller that 0.05. (or 0.01). Looking at the stats table above, this would suggest removing the following features from the list ['x21','x16','x17','x22'] . Note that feature removal should be done carefully, as every time the feature is removed, the coefficients might converge elsewhere, and would hence give a different model with a different interpretation. new_feats = [ feat for feat in selected_features if feat not in [ 'x21' , 'x16' , 'x17' , 'x22' ]] scorecard = Skorecard ( UserInputBucketer ( buckets_dict ), variables = new_feats , calculate_stats = True ) scorecard = scorecard . fit ( X_train , y_train ) model_stats = scorecard . get_stats () model_stats . index = [ 'Const' ] + new_feats display ( model_stats ) proba_train = scorecard . predict_proba ( X_train )[:, 1 ] proba_test = scorecard . predict_proba ( X_test )[:, 1 ] print ( f \"AUC train: { round ( roc_auc_score ( y_train , proba_train ), 4 ) } \" ) print ( f \"AUC test : { round ( roc_auc_score ( y_test , proba_test ), 4 ) } \" ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Coef. Std.Err z P>|z| Const -1.242598 0.018229 -68.166667 0.000000e+00 x6 0.763946 0.020596 37.092613 3.695794e-301 x8 0.269057 0.033234 8.095809 5.688515e-16 x10 0.339016 0.035156 9.643180 5.253488e-22 x18 0.241832 0.049687 4.867076 1.132612e-06 x1 0.409354 0.046618 8.781019 1.619998e-18 x19 0.191910 0.053865 3.562812 3.669030e-04 x20 0.282166 0.060879 4.634866 3.571691e-06 x23 0.227794 0.069624 3.271788 1.068698e-03 x3 0.441695 0.095994 4.601264 4.199341e-06 AUC train:0.7712 AUC test :0.7648","title":"Removing features based on their statistical properties"},{"location":"tutorials/3_skorecard_model/#retrieving-the-transformed-data","text":"Buckets and WoE transformations are available directly in a fitted skorecard model print ( \"Top 5 rows and the transformed buckets\" ) display ( scorecard . bucket_transform ( X_test )[ new_feats ] . head ()) print ( \" \\n Top 5 rows and the transformed WoEs\" ) display ( scorecard . woe_transform ( X_test )[ new_feats ] . head ()) Top 5 rows and the transformed buckets .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x6 x8 x10 x18 x1 x19 x20 x23 x3 2308 1 0 0 1 0 1 1 1 2 22404 1 0 0 1 2 1 1 2 1 23397 1 0 0 1 0 1 1 2 0 25058 1 0 0 1 1 1 2 2 0 2664 1 0 0 1 -3 1 1 1 2 Top 5 rows and the transformed WoEs .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } x6 x8 x10 x18 x1 x19 x20 x23 x3 2308 -0.668068 -0.321161 -0.230059 -0.029168 0.610738 0.000397 -0.015444 0.105318 0.102411 22404 -0.668068 -0.321161 -0.230059 -0.029168 -0.353564 0.000397 -0.015444 -0.256887 -0.192612 23397 -0.668068 -0.321161 -0.230059 -0.029168 0.610738 0.000397 -0.015444 -0.256887 0.168106 25058 -0.668068 -0.321161 -0.230059 -0.029168 0.070222 0.000397 -0.405293 -0.256887 0.168106 2664 -0.668068 -0.321161 -0.230059 -0.029168 0.224540 0.000397 -0.015444 0.105318 0.102411","title":"Retrieving the transformed data"},{"location":"tutorials/3_skorecard_model/#getting-the-feature-importance-to-be-integrated-in-the-skorecard-class","text":"In order to talk of feature importance, we should consider both the coefficients and the IV of the single feature. The importance cab be approximated as the product of the two numbers. from skorecard.reporting import iv X_train_bins = scorecard . bucket_transform ( X_train ) iv_dict = iv ( X_train_bins , y_train ) iv_values = pd . Series ( iv_dict ) . sort_values ( ascending = False ) iv_values . name = \"IV\" feat_importance = model_stats [[ 'Coef.' ]] . join ( iv_values ) feat_importance [ 'importance' ] = - 1. * feat_importance [ 'Coef.' ] * feat_importance [ 'IV' ] feat_importance . sort_values ( by = 'importance' , ascending = False ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Coef. IV importance x23 0.227794 0.002257 -0.000514 x8 0.269057 0.001924 -0.000518 x20 0.282166 0.001998 -0.000564 x18 0.241832 0.002503 -0.000605 x19 0.191910 0.003325 -0.000638 x10 0.339016 0.001917 -0.000650 x1 0.409354 0.002457 -0.001006 x3 0.441695 0.002968 -0.001311 x6 0.763946 0.002430 -0.001857 Const -1.242598 NaN NaN","title":"Getting the feature importance (to be integrated in the skorecard class)"},{"location":"tutorials/3_skorecard_model/#scaling-the-scores","text":"The last step of building skorecard models is the rescaling of the predictions. This is a very common practice within the Credit Risk domain, where scorecard models are widely used. Rescaling scorecards has no impact on the model performance, but rather returns the predictions on an arbitrary scale (normally from 0-1000) which are more meaningful for risk managers and underwriters in a bank than probabilities. The rescaling is a linear transfromation performed on the log-odds of the predicted probability \\(p\\) , \\[ log(\\frac{1-p}{p}) \\] Where the odds are defined as: \\[ \\frac{1-p}{p} \\] The reference for the linear transformation are commonly defined by the following values: ref_score : reference score, that should match a given reference odds (ref_odds) ref_odds : reference odds that should match a giver reference score pdo : points to double the odds, number of points to add where the odds double. An example: with the following settings: ref_score = 400 ref_odds = 20 pdo = 25 A score of 400 corresponds to the odds 20:1 of being a \"good client\" ( y=0 ). This means that the predicted probability for y=1 is in this case ~4.76% , which you can get by rearranging the equation for the odds, above. When the score increases to 425 , the odds double to 40:1 (predicted probability to be y=1 is ~2,43% ). When the score decreases to 375 , the odds are reduced by a factor 2, ie, 10:1 (predicted probability to be y=1 is ~9,09% ). In skorecard , one can use the calibrate_to_master_scale function. from skorecard.rescale import calibrate_to_master_scale proba_train = pd . Series ( proba_train , index = y_train . index ) . sort_values () # sorting for visualization purposes scores = calibrate_to_master_scale ( proba_train , pdo = 25 , ref_score = 400 , ref_odds = 20 )","title":"Scaling the scores"},{"location":"tutorials/3_skorecard_model/#visualize-the-score-dependencies","text":"fig , ( ax1 , ax2 , ax3 ) = plt . subplots ( 3 , sharex = True , figsize = ( 8 , 12 ), gridspec_kw = { 'hspace' : 0 }) ax1 . plot ( scores . values , proba_train . values ) ax1 . set_ylabel ( \"Predicted probability\" ) ax1 . set_title ( \"Rescaled scores and probabilities\" ) ax1 . grid () ax2 . plot ( scores . values , proba_train . apply ( lambda x : ( 1 - x ) / x ) . values ) ax2 . set_ylabel ( \"Odds\" ) ax2 . grid () ax3 . plot ( scores . values , proba_train . apply ( lambda x : np . log ( 1 - x ) - np . log ( x )) . values ,) ax3 . set_ylabel ( \"log-odds\" ) ax3 . grid () ax3 . set_xlabel ( \"Rescaled scores\" ) plt . show ()","title":"Visualize the score dependencies"},{"location":"tutorials/3_skorecard_model/#assigning-points-to-every-feature","text":"The last step of a scorecard development is to convert all the features into the rescaled model. A scorecard model is a logisitic regression fitted on the WoE values of every single bucketed feature. In other words, the following equations holds: \\[ log(odds) = log(\\frac{1-p}{p}) = \\beta_{0} + \\sum_{i} \\beta_{i} \\cdot WOE(X_{i}) \\] As the rescaling performed earlier is linear in the predicted log-odds , this means that the every feature-bucket contribution can be rescaled to an integer value (by rescaling directly the $ \\beta_{i} \\cdot WOE(X_{i})$ factors with the same calculations. This returns the final scorecard, that can be easily implemented. The functionality in skorecard to rescale the features is as follows from skorecard.rescale import ScoreCardPoints # ensure that pdo, ref_score and ref_odds are consistent scp = ScoreCardPoints ( skorecard_model = scorecard , pdo = 25 , ref_score = 400 , ref_odds = 20 ) one can extract the final scorecard as follows scp . get_scorecard_points () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bin_index map woe feature coef contribution Points 0 -2 NaN 0.000000 x6 0.763946 0.000000 37 1 -1 Missing 0.000000 x6 0.763946 0.000000 37 3 1 [-0.5, 0.5) -0.668068 x6 0.763946 -0.510368 56 4 2 [0.5, 1.5) -0.419221 x6 0.763946 -0.320262 49 5 3 [1.5, inf) 0.586085 x6 0.763946 0.447737 21 6 4 NaN 2.095463 x6 0.763946 1.600820 -20 7 -2 NaN 0.000000 x8 0.269057 0.000000 37 8 -1 Missing 0.000000 x8 0.269057 0.000000 37 10 1 [1.5, inf) -0.321161 x8 0.269057 -0.086411 41 11 2 NaN 1.392671 x8 0.269057 0.374708 24 12 -2 NaN 0.000000 x10 0.339016 0.000000 37 13 -1 Missing 0.000000 x10 0.339016 0.000000 37 15 1 [1.0, inf) -0.230059 x10 0.339016 -0.077994 40 16 2 NaN 1.491617 x10 0.339016 0.505681 19 17 -2 NaN 0.000000 x18 0.241832 0.000000 37 18 -1 Missing 0.000000 x18 0.241832 0.000000 37 20 1 [21.0, 4552.5) -0.029168 x18 0.241832 -0.007054 38 21 2 [4552.5, 15001.5) 0.692001 x18 0.241832 0.167348 31 22 3 [15001.5, inf) -0.457263 x18 0.241832 -0.110581 41 23 4 NaN -0.860845 x18 0.241832 -0.208180 45 25 -2 NaN 0.000000 x1 0.409354 0.000000 37 26 -1 Missing 0.000000 x1 0.409354 0.000000 37 28 1 [75000.0, 145000.0) -0.353564 x1 0.409354 -0.144733 43 29 2 [145000.0, 375000.0) 0.610738 x1 0.409354 0.250008 28 30 3 [375000.0, inf) 0.070222 x1 0.409354 0.028746 36 31 4 NaN -0.766316 x1 0.409354 -0.313695 49 32 5 NaN 0.224540 x1 0.409354 0.091916 34 33 -2 NaN 0.000000 x19 0.191910 0.000000 37 34 -1 Missing 0.000000 x19 0.191910 0.000000 37 36 1 [131.5, 4970.5) 0.000397 x19 0.191910 0.000076 37 37 2 [4970.5, 15001.0) 0.576300 x19 0.191910 0.110598 33 38 3 [15001.0, inf) -0.403868 x19 0.191910 -0.077506 40 39 4 NaN -1.162523 x19 0.191910 -0.223100 45 40 -2 NaN 0.000000 x20 0.282166 0.000000 37 41 -1 Missing 0.000000 x20 0.282166 0.000000 37 43 1 [16.5, 4513.5) -0.015444 x20 0.282166 -0.004358 38 44 2 [4513.5, 12490.5) 0.509437 x20 0.282166 0.143746 32 45 3 [12490.5, inf) -0.405293 x20 0.282166 -0.114360 42 46 4 NaN -0.832020 x20 0.282166 -0.234768 46 47 -2 NaN 0.000000 x23 0.227794 0.000000 37 48 -1 Missing 0.000000 x23 0.227794 0.000000 37 50 1 [1.5, 2000.5) -0.256887 x23 0.227794 -0.058517 40 51 2 [2000.5, 9849.5) 0.105318 x23 0.227794 0.023991 37 52 3 [9849.5, inf) 0.350867 x23 0.227794 0.079925 35 53 4 NaN -0.701982 x23 0.227794 -0.159907 43 54 -2 Other 0.000000 x3 0.441695 0.000000 37 55 -1 Missing 0.000000 x3 0.441695 0.000000 37 57 1 1.0 0.168106 x3 0.441695 0.074252 35 58 2 2.0 0.102411 x3 0.441695 0.045235 36 59 3 NaN -0.192612 x3 0.441695 -0.085076 40 60 4 NaN -1.207590 x3 0.441695 -0.533387 57 61 0 0 0.000000 Intercept -1.242598 -0.000000 0 Or one can apply the transformation directly on the data, by calling the transform method, in order to map each feature to its actual points.","title":"Assigning points to every feature"},{"location":"tutorials/3_skorecard_model/#validate-the-rescaling","text":"As the last step, in order to ensure that the rescaling was successfull, one can verify that the sum of the points of each row in the dataset matches the rescaled scores. The rescaling steps has some integer rounding, therefore small discrepancies of 1-2 points might occur due to the rounding error proba_train = pd . Series ( proba_train , index = y_train . index ) #convert to pandas and correct index in order to be able to perform the diff scores = calibrate_to_master_scale ( proba_train , pdo = 25 , ref_score = 400 , ref_odds = 20 ) # Check the distribution of the differences ( scores - scp . transform ( X_train ) . sum ( axis = 1 )) . value_counts () 111.0 878 105.0 708 146.0 560 125.0 516 130.0 442 ... 284.0 1 292.0 1 -54.0 1 261.0 1 -80.0 1 Length: 373, dtype: int64","title":"Validate the rescaling"},{"location":"tutorials/categoricals/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Categoricals \u00b6 skorecard also has bucketers that support categorical features (such as OptimalBucketer and OrdinalCategoricalBucketer ). If you have a categorical feature, you can bucket them directly: from skorecard.bucketers import OptimalBucketer import random from skorecard.datasets import load_uci_credit_card X , y = load_uci_credit_card ( return_X_y = True ) # Add a categorical feature pets = [ \"no pets\" ] * 3000 + [ \"cat lover\" ] * 1500 + [ \"dog lover\" ] * 1000 + [ \"rabbit\" ] * 498 + [ \"gold fish\" ] * 2 random . Random ( 42 ) . shuffle ( pets ) X [ \"pet_ownership\" ] = pets bucketer = OptimalBucketer ( max_n_bins = 3 , variables = [ \"pet_ownership\" ], variables_type = \"categorical\" , cat_cutoff = None ) bucketer . fit_transform ( X , y )[ 'pet_ownership' ] . value_counts () . sort_index () 0 1998 1 3000 2 1002 Name: pet_ownership, dtype: int64","title":"Categoricals"},{"location":"tutorials/categoricals/#categoricals","text":"skorecard also has bucketers that support categorical features (such as OptimalBucketer and OrdinalCategoricalBucketer ). If you have a categorical feature, you can bucket them directly: from skorecard.bucketers import OptimalBucketer import random from skorecard.datasets import load_uci_credit_card X , y = load_uci_credit_card ( return_X_y = True ) # Add a categorical feature pets = [ \"no pets\" ] * 3000 + [ \"cat lover\" ] * 1500 + [ \"dog lover\" ] * 1000 + [ \"rabbit\" ] * 498 + [ \"gold fish\" ] * 2 random . Random ( 42 ) . shuffle ( pets ) X [ \"pet_ownership\" ] = pets bucketer = OptimalBucketer ( max_n_bins = 3 , variables = [ \"pet_ownership\" ], variables_type = \"categorical\" , cat_cutoff = None ) bucketer . fit_transform ( X , y )[ 'pet_ownership' ] . value_counts () . sort_index () 0 1998 1 3000 2 1002 Name: pet_ownership, dtype: int64","title":"Categoricals"},{"location":"tutorials/interactive_bucketing/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Interactive bucketing \u00b6 You might want to manually edit the bucketing boundaries, for example to incorporate specific domain knowledge. You can manually define buckets , but you could also use to interactive explore and update the buckets. All skorecard.bucketers have a method called .fit_interactive() , which will call .fit() if the bucketer is not yet fitted, and then launch a dash webapp . Make sure to have the up to date dash dependencies by running pip install --upgrade skorecard[dashboard] . from skorecard.datasets import load_uci_credit_card from skorecard.bucketers import DecisionTreeBucketer X , y = load_uci_credit_card ( return_X_y = True ) bucketer = DecisionTreeBucketer ( max_n_bins = 10 ) # bucketer.fit_interactive(X, y) # not run This should look like: This also works for categorical features: from skorecard.bucketers import OrdinalCategoricalBucketer import random pets = [ \"no pets\" ] * 3000 + [ \"cat lover\" ] * 1500 + [ \"dog lover\" ] * 1000 + [ \"rabbit\" ] * 498 + [ \"gold fish\" ] * 2 random . Random ( 42 ) . shuffle ( pets ) X [ \"pet_ownership\" ] = pets bucketer = OrdinalCategoricalBucketer ( variables = [ 'pet_ownership' ]) # bucketer.fit_interactive(X, y) # not run Dash app running on http://127.0.0.1:8050/ Which should look like: Pipelines \u00b6 You can also run .fit_interactive() on a pipeline of bucketers. You'll need to convert to a SkorecardPipeline in order to have access to the method: [ feat for feat in X . columns if feat not in cat_cols ] ['LIMIT_BAL', 'BILL_AMT1', 'pet_ownership'] from skorecard.bucketers import OrdinalCategoricalBucketer from skorecard.pipeline import to_skorecard_pipeline from sklearn.pipeline import make_pipeline pipe = make_pipeline ( OrdinalCategoricalBucketer ( variables = [ \"EDUCATION\" , \"MARRIAGE\" ]), DecisionTreeBucketer ( max_n_bins = 10 , variables = [ \"LIMIT_BAL\" , \"BILL_AMT1\" ]) ) # Make this a skorecard pipeline, which adds some convenience methods pipe = to_skorecard_pipeline ( pipe ) # pipe.fit_interactive(X, y) # not run Dash app running on http://127.0.0.1:8050/ BucketingProcess and Skorecard models \u00b6 Interactively setting pre-bucketing and bucketing per column is also possible on BucketingProcess and Skorecard models from skorecard import Skorecard from skorecard.datasets import load_uci_credit_card model = Skorecard ( variables = [ \"EDUCATION\" , \"MARRIAGE\" , \"LIMIT_BAL\" , \"BILL_AMT1\" ]) # model.fit_interactive(X, y) # not run Dash app running on http://127.0.0.1:8050/","title":"Interactive"},{"location":"tutorials/interactive_bucketing/#interactive-bucketing","text":"You might want to manually edit the bucketing boundaries, for example to incorporate specific domain knowledge. You can manually define buckets , but you could also use to interactive explore and update the buckets. All skorecard.bucketers have a method called .fit_interactive() , which will call .fit() if the bucketer is not yet fitted, and then launch a dash webapp . Make sure to have the up to date dash dependencies by running pip install --upgrade skorecard[dashboard] . from skorecard.datasets import load_uci_credit_card from skorecard.bucketers import DecisionTreeBucketer X , y = load_uci_credit_card ( return_X_y = True ) bucketer = DecisionTreeBucketer ( max_n_bins = 10 ) # bucketer.fit_interactive(X, y) # not run This should look like: This also works for categorical features: from skorecard.bucketers import OrdinalCategoricalBucketer import random pets = [ \"no pets\" ] * 3000 + [ \"cat lover\" ] * 1500 + [ \"dog lover\" ] * 1000 + [ \"rabbit\" ] * 498 + [ \"gold fish\" ] * 2 random . Random ( 42 ) . shuffle ( pets ) X [ \"pet_ownership\" ] = pets bucketer = OrdinalCategoricalBucketer ( variables = [ 'pet_ownership' ]) # bucketer.fit_interactive(X, y) # not run Dash app running on http://127.0.0.1:8050/ Which should look like:","title":"Interactive bucketing"},{"location":"tutorials/interactive_bucketing/#pipelines","text":"You can also run .fit_interactive() on a pipeline of bucketers. You'll need to convert to a SkorecardPipeline in order to have access to the method: [ feat for feat in X . columns if feat not in cat_cols ] ['LIMIT_BAL', 'BILL_AMT1', 'pet_ownership'] from skorecard.bucketers import OrdinalCategoricalBucketer from skorecard.pipeline import to_skorecard_pipeline from sklearn.pipeline import make_pipeline pipe = make_pipeline ( OrdinalCategoricalBucketer ( variables = [ \"EDUCATION\" , \"MARRIAGE\" ]), DecisionTreeBucketer ( max_n_bins = 10 , variables = [ \"LIMIT_BAL\" , \"BILL_AMT1\" ]) ) # Make this a skorecard pipeline, which adds some convenience methods pipe = to_skorecard_pipeline ( pipe ) # pipe.fit_interactive(X, y) # not run Dash app running on http://127.0.0.1:8050/","title":"Pipelines"},{"location":"tutorials/interactive_bucketing/#bucketingprocess-and-skorecard-models","text":"Interactively setting pre-bucketing and bucketing per column is also possible on BucketingProcess and Skorecard models from skorecard import Skorecard from skorecard.datasets import load_uci_credit_card model = Skorecard ( variables = [ \"EDUCATION\" , \"MARRIAGE\" , \"LIMIT_BAL\" , \"BILL_AMT1\" ]) # model.fit_interactive(X, y) # not run Dash app running on http://127.0.0.1:8050/","title":"BucketingProcess and Skorecard models"},{"location":"tutorials/methods/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Methods \u00b6 The bucketers of skorecard come with a handy list of methods for you to peek under the hood of the bucketer from skorecard.datasets import load_uci_credit_card from skorecard.bucketers import DecisionTreeBucketer X , y = load_uci_credit_card ( return_X_y = True ) specials = { \"LIMIT_BAL\" : { \"=50000\" : [ 50000 ], \"in [20000,30000]\" : [ 20000 , 30000 ]}} dt_bucketer = DecisionTreeBucketer ( variables = [ 'LIMIT_BAL' ], specials = specials ) dt_bucketer . fit ( X , y ) dt_bucketer . fit_transform ( X , y ) . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } EDUCATION MARRIAGE LIMIT_BAL BILL_AMT1 0 1 2 9 201800.0 1 2 2 1 80610.0 2 1 2 9 499452.0 3 1 1 3 450.0 4 2 1 9 56107.0 .summary() \u00b6 This gives the user a simple table of the columns and number of (pre)buckets generated by the bucketer. The information value and dtypes are also given dt_bucketer . summary () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } column num_prebuckets num_buckets IV_score dtype 0 EDUCATION not_prebucketed not_bucketed 0.057606 int64 1 MARRIAGE not_prebucketed not_bucketed 0.016267 int64 2 LIMIT_BAL not_prebucketed 13 0.178036 float64 3 BILL_AMT1 not_prebucketed not_bucketed 2.915613 float64 .bucket_table() \u00b6 To look at the buckets in a more granular level, the bucket_table() method outputs, among others, a table containing the counts in each bin, the percentages, and the event rate. dt_bucketer . bucket_table ( 'LIMIT_BAL' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bucket label Count Count (%) Non-event Event Event Rate WoE IV 0 -4 Special: in [20000,30000] 723.0 12.05 453.0 270.0 0.373444 -0.724 -0.075 1 -3 Special: =50000 676.0 11.27 518.0 158.0 0.233728 -0.054 -0.000 2 -1 Missing 0.0 0.00 0.0 0.0 NaN 0.000 0.000 3 0 [-inf, 75000.0) 462.0 7.70 313.0 149.0 0.322511 -0.499 -0.022 4 1 [75000.0, 85000.0) 319.0 5.32 243.0 76.0 0.238245 -0.079 -0.000 5 2 [85000.0, 105000.0) 330.0 5.50 241.0 89.0 0.269697 -0.245 -0.004 6 3 [105000.0, 145000.0) 566.0 9.43 436.0 130.0 0.229682 -0.031 -0.000 7 4 [145000.0, 175000.0) 449.0 7.48 380.0 69.0 0.153675 0.464 -0.014 8 5 [175000.0, 225000.0) 769.0 12.82 630.0 139.0 0.180754 0.270 -0.009 9 6 [225000.0, 275000.0) 501.0 8.35 419.0 82.0 0.163673 0.390 -0.011 10 7 [275000.0, 325000.0) 379.0 6.32 326.0 53.0 0.139842 0.575 -0.018 11 8 [325000.0, 385000.0) 350.0 5.83 287.0 63.0 0.180000 0.275 -0.004 12 9 [385000.0, inf) 476.0 7.93 409.0 67.0 0.140756 0.567 -0.022 .plot_bucket() \u00b6 We have already seen that we can plot the above bucket table for a better visualisation of the buckets dt_bucketer . plot_bucket ( 'LIMIT_BAL' , format = \"png\" , scale = 2 , width = 1050 , height = 525 ) # remove format argument for an interactive plotly plot.) .save_yml() \u00b6 We can save the generated bucket to a yaml file. This yaml file can later be used to generate a bucketer as we show in the create_bucketer_from_file tutorial dt_bucketer . save_yml ( open ( \"my_output.yml\" , \"w\" )) Bucket mapping \u00b6 If you're interested into digging into the internals of the buckets, you can access the fitted attribute features_bucket_mapping_ . For example: ```python bucketer.features_bucket_mapping_.get('pet_ownership').labels # {0: 'cat lover, rabbit', # 1: 'no pets', # 2: 'dog lover', # 3: 'gold fish', # 4: 'other', # 5: 'Missing'} ```","title":"Methods"},{"location":"tutorials/methods/#methods","text":"The bucketers of skorecard come with a handy list of methods for you to peek under the hood of the bucketer from skorecard.datasets import load_uci_credit_card from skorecard.bucketers import DecisionTreeBucketer X , y = load_uci_credit_card ( return_X_y = True ) specials = { \"LIMIT_BAL\" : { \"=50000\" : [ 50000 ], \"in [20000,30000]\" : [ 20000 , 30000 ]}} dt_bucketer = DecisionTreeBucketer ( variables = [ 'LIMIT_BAL' ], specials = specials ) dt_bucketer . fit ( X , y ) dt_bucketer . fit_transform ( X , y ) . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } EDUCATION MARRIAGE LIMIT_BAL BILL_AMT1 0 1 2 9 201800.0 1 2 2 1 80610.0 2 1 2 9 499452.0 3 1 1 3 450.0 4 2 1 9 56107.0","title":"Methods"},{"location":"tutorials/methods/#summary","text":"This gives the user a simple table of the columns and number of (pre)buckets generated by the bucketer. The information value and dtypes are also given dt_bucketer . summary () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } column num_prebuckets num_buckets IV_score dtype 0 EDUCATION not_prebucketed not_bucketed 0.057606 int64 1 MARRIAGE not_prebucketed not_bucketed 0.016267 int64 2 LIMIT_BAL not_prebucketed 13 0.178036 float64 3 BILL_AMT1 not_prebucketed not_bucketed 2.915613 float64","title":".summary()"},{"location":"tutorials/methods/#bucket_table","text":"To look at the buckets in a more granular level, the bucket_table() method outputs, among others, a table containing the counts in each bin, the percentages, and the event rate. dt_bucketer . bucket_table ( 'LIMIT_BAL' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bucket label Count Count (%) Non-event Event Event Rate WoE IV 0 -4 Special: in [20000,30000] 723.0 12.05 453.0 270.0 0.373444 -0.724 -0.075 1 -3 Special: =50000 676.0 11.27 518.0 158.0 0.233728 -0.054 -0.000 2 -1 Missing 0.0 0.00 0.0 0.0 NaN 0.000 0.000 3 0 [-inf, 75000.0) 462.0 7.70 313.0 149.0 0.322511 -0.499 -0.022 4 1 [75000.0, 85000.0) 319.0 5.32 243.0 76.0 0.238245 -0.079 -0.000 5 2 [85000.0, 105000.0) 330.0 5.50 241.0 89.0 0.269697 -0.245 -0.004 6 3 [105000.0, 145000.0) 566.0 9.43 436.0 130.0 0.229682 -0.031 -0.000 7 4 [145000.0, 175000.0) 449.0 7.48 380.0 69.0 0.153675 0.464 -0.014 8 5 [175000.0, 225000.0) 769.0 12.82 630.0 139.0 0.180754 0.270 -0.009 9 6 [225000.0, 275000.0) 501.0 8.35 419.0 82.0 0.163673 0.390 -0.011 10 7 [275000.0, 325000.0) 379.0 6.32 326.0 53.0 0.139842 0.575 -0.018 11 8 [325000.0, 385000.0) 350.0 5.83 287.0 63.0 0.180000 0.275 -0.004 12 9 [385000.0, inf) 476.0 7.93 409.0 67.0 0.140756 0.567 -0.022","title":".bucket_table()"},{"location":"tutorials/methods/#plot_bucket","text":"We have already seen that we can plot the above bucket table for a better visualisation of the buckets dt_bucketer . plot_bucket ( 'LIMIT_BAL' , format = \"png\" , scale = 2 , width = 1050 , height = 525 ) # remove format argument for an interactive plotly plot.)","title":".plot_bucket()"},{"location":"tutorials/methods/#save_yml","text":"We can save the generated bucket to a yaml file. This yaml file can later be used to generate a bucketer as we show in the create_bucketer_from_file tutorial dt_bucketer . save_yml ( open ( \"my_output.yml\" , \"w\" ))","title":".save_yml()"},{"location":"tutorials/methods/#bucket-mapping","text":"If you're interested into digging into the internals of the buckets, you can access the fitted attribute features_bucket_mapping_ . For example: ```python bucketer.features_bucket_mapping_.get('pet_ownership').labels # {0: 'cat lover, rabbit', # 1: 'no pets', # 2: 'dog lover', # 3: 'gold fish', # 4: 'other', # 5: 'Missing'} ```","title":"Bucket mapping"},{"location":"tutorials/missing_values/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Missing Values \u00b6 skorecard bucketers offer native support for missing values and will put them in a separate bucket by default. In the example below, you can see that the single missing value is put into a new bucket '-1'. import numpy as np import pandas as pd from skorecard.bucketers import EqualFrequencyBucketer df = pd . DataFrame ({ 'counts' : [ 1 , 2 , 2 , 1 , 4 , 2 , np . nan , 1 , 3 ]}) EqualFrequencyBucketer ( n_bins = 2 ) . fit_transform ( df ) . value_counts () counts 0 6 1 2 -1 1 dtype: int64 Specific \u00b6 Alternatively, the user can give a specific bucket for the missing values. In the example below, you can see we put the missing value into bucket 1 EqualFrequencyBucketer ( n_bins = 2 , missing_treatment = { 'counts' : 1 }) . fit_transform ( df ) . value_counts () counts 0 6 1 3 dtype: int64 Passthrough \u00b6 If the user wishes the missing values to be left untouched, they can specify this with the passthrough argument EqualFrequencyBucketer ( n_bins = 2 , missing_treatment = 'passthrough' ) . fit_transform ( df ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } counts 0 0.0 1 0.0 2 0.0 3 0.0 4 1.0 5 0.0 6 NaN 7 0.0 8 1.0 Most frequent \u00b6 It's also possible to put the missing values into the most common bucket. Below, we see that the missing values are put into the '0' bucket EqualFrequencyBucketer ( n_bins = 2 , missing_treatment = 'most_frequent' ) . fit_transform ( df ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } counts 0 0 1 0 2 0 3 0 4 1 5 0 6 0 7 0 8 1 Using the target to bucket \u00b6 It's also possible to use the target to decide which bucket to use for the missing values. In the below examples, we use y as the target. Neutral \u00b6 Here the missing values are placed into the bucket that has a Weight of Evidence closest to 0 X = pd . DataFrame ({ 'counts' : [ 1 , 2 , 2 , 1 , 4 , 2 , np . nan , 1 , 3 ]}) y = pd . DataFrame ({ 'target' : [ 0 , 0 , 1 , 0 , 1 , 0 , 1 , 0 , 1 ]}) EqualFrequencyBucketer ( n_bins = 2 , missing_treatment = 'neutral' ) . fit_transform ( X , y ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } counts 0 0 1 0 2 0 3 0 4 1 5 0 6 0 7 0 8 1 Similar \u00b6 We can also put the missing values into the bucket that has a Weight of Evidence closest to the bucket containing only missing values EqualFrequencyBucketer ( n_bins = 2 , missing_treatment = 'similar' ) . fit_transform ( X , y ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } counts 0 0 1 0 2 0 3 0 4 1 5 0 6 1 7 0 8 1 Least risky \u00b6 Missing values are put into the bucket containing the largest percentage of Class 0. a = EqualFrequencyBucketer ( n_bins = 2 , missing_treatment = 'least_risky' ) #.fit_transform(X, y) a . fit_transform ( X , y ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } counts 0 0 1 0 2 0 3 0 4 1 5 0 6 0 7 0 8 1 EqualFrequencyBucketer ( n_bins = 2 , missing_treatment = 'least_risky' ) . fit_transform ( X , y ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } counts 0 0 1 0 2 0 3 0 4 1 5 0 6 0 7 0 8 1 Most risky \u00b6 Missing values are put into the bucket containing the largest percentage of Class 1. EqualFrequencyBucketer ( n_bins = 2 , missing_treatment = 'most_risky' ) . fit_transform ( X , y ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } counts 0 0 1 0 2 0 3 0 4 1 5 0 6 1 7 0 8 1","title":"Missing Values"},{"location":"tutorials/missing_values/#missing-values","text":"skorecard bucketers offer native support for missing values and will put them in a separate bucket by default. In the example below, you can see that the single missing value is put into a new bucket '-1'. import numpy as np import pandas as pd from skorecard.bucketers import EqualFrequencyBucketer df = pd . DataFrame ({ 'counts' : [ 1 , 2 , 2 , 1 , 4 , 2 , np . nan , 1 , 3 ]}) EqualFrequencyBucketer ( n_bins = 2 ) . fit_transform ( df ) . value_counts () counts 0 6 1 2 -1 1 dtype: int64","title":"Missing Values"},{"location":"tutorials/missing_values/#specific","text":"Alternatively, the user can give a specific bucket for the missing values. In the example below, you can see we put the missing value into bucket 1 EqualFrequencyBucketer ( n_bins = 2 , missing_treatment = { 'counts' : 1 }) . fit_transform ( df ) . value_counts () counts 0 6 1 3 dtype: int64","title":"Specific"},{"location":"tutorials/missing_values/#passthrough","text":"If the user wishes the missing values to be left untouched, they can specify this with the passthrough argument EqualFrequencyBucketer ( n_bins = 2 , missing_treatment = 'passthrough' ) . fit_transform ( df ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } counts 0 0.0 1 0.0 2 0.0 3 0.0 4 1.0 5 0.0 6 NaN 7 0.0 8 1.0","title":"Passthrough"},{"location":"tutorials/missing_values/#most-frequent","text":"It's also possible to put the missing values into the most common bucket. Below, we see that the missing values are put into the '0' bucket EqualFrequencyBucketer ( n_bins = 2 , missing_treatment = 'most_frequent' ) . fit_transform ( df ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } counts 0 0 1 0 2 0 3 0 4 1 5 0 6 0 7 0 8 1","title":"Most frequent"},{"location":"tutorials/missing_values/#using-the-target-to-bucket","text":"It's also possible to use the target to decide which bucket to use for the missing values. In the below examples, we use y as the target.","title":"Using the target to bucket"},{"location":"tutorials/missing_values/#neutral","text":"Here the missing values are placed into the bucket that has a Weight of Evidence closest to 0 X = pd . DataFrame ({ 'counts' : [ 1 , 2 , 2 , 1 , 4 , 2 , np . nan , 1 , 3 ]}) y = pd . DataFrame ({ 'target' : [ 0 , 0 , 1 , 0 , 1 , 0 , 1 , 0 , 1 ]}) EqualFrequencyBucketer ( n_bins = 2 , missing_treatment = 'neutral' ) . fit_transform ( X , y ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } counts 0 0 1 0 2 0 3 0 4 1 5 0 6 0 7 0 8 1","title":"Neutral"},{"location":"tutorials/missing_values/#similar","text":"We can also put the missing values into the bucket that has a Weight of Evidence closest to the bucket containing only missing values EqualFrequencyBucketer ( n_bins = 2 , missing_treatment = 'similar' ) . fit_transform ( X , y ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } counts 0 0 1 0 2 0 3 0 4 1 5 0 6 1 7 0 8 1","title":"Similar"},{"location":"tutorials/missing_values/#least-risky","text":"Missing values are put into the bucket containing the largest percentage of Class 0. a = EqualFrequencyBucketer ( n_bins = 2 , missing_treatment = 'least_risky' ) #.fit_transform(X, y) a . fit_transform ( X , y ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } counts 0 0 1 0 2 0 3 0 4 1 5 0 6 0 7 0 8 1 EqualFrequencyBucketer ( n_bins = 2 , missing_treatment = 'least_risky' ) . fit_transform ( X , y ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } counts 0 0 1 0 2 0 3 0 4 1 5 0 6 0 7 0 8 1","title":"Least risky"},{"location":"tutorials/missing_values/#most-risky","text":"Missing values are put into the bucket containing the largest percentage of Class 1. EqualFrequencyBucketer ( n_bins = 2 , missing_treatment = 'most_risky' ) . fit_transform ( X , y ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } counts 0 0 1 0 2 0 3 0 4 1 5 0 6 1 7 0 8 1","title":"Most risky"},{"location":"tutorials/reporting/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Reporting \u00b6 Reporting plays a crucial role in building scorecard models. Skorecard bucketers include a reporting module, and this tutorial shows how to extract it % matplotlib inline from skorecard.datasets import load_uci_credit_card X , y = load_uci_credit_card ( return_X_y = True ) X . head ( 4 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } EDUCATION MARRIAGE LIMIT_BAL BILL_AMT1 0 1 2 400000.0 201800.0 1 2 2 80000.0 80610.0 2 1 2 500000.0 499452.0 3 1 1 140000.0 450.0 Reporting in bucketers \u00b6 Once a bucketer is fitted, the reporting module is incorporated directly in the bucketer object from skorecard.bucketers import DecisionTreeBucketer bucketer = DecisionTreeBucketer ( max_n_bins = 10 ) X_transformed = bucketer . fit_transform ( X , y ) X_transformed . head ( 4 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } EDUCATION MARRIAGE LIMIT_BAL BILL_AMT1 0 0 1 9 9 1 1 1 3 7 2 0 1 9 9 3 0 0 5 0 Retrieve the bucket summary table bucketer . bucket_table ( column = 'LIMIT_BAL' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bucket label Count Count (%) Non-event Event Event Rate WoE IV 0 -1 Missing 0.0 0.00 0.0 0.0 NaN 0.000 0.000 1 0 [-inf, 45000.0) 849.0 14.15 533.0 316.0 0.372203 -0.719 0.087 2 1 [45000.0, 55000.0) 676.0 11.27 518.0 158.0 0.233728 -0.054 0.000 3 2 [55000.0, 75000.0) 336.0 5.60 233.0 103.0 0.306548 -0.425 0.011 4 3 [75000.0, 85000.0) 319.0 5.32 243.0 76.0 0.238245 -0.079 0.000 5 4 [85000.0, 105000.0) 330.0 5.50 241.0 89.0 0.269697 -0.245 0.004 6 5 [105000.0, 145000.0) 566.0 9.43 436.0 130.0 0.229682 -0.031 0.000 7 6 [145000.0, 275000.0) 1719.0 28.65 1429.0 290.0 0.168703 0.353 0.032 8 7 [275000.0, 325000.0) 379.0 6.32 326.0 53.0 0.139842 0.575 0.018 9 8 [325000.0, 385000.0) 350.0 5.83 287.0 63.0 0.180000 0.275 0.004 10 9 [385000.0, inf) 476.0 7.93 409.0 67.0 0.140756 0.567 0.022 Plotting the buckets bucketer . plot_bucket ( column = 'LIMIT_BAL' , format = \"png\" , scale = 2 , width = 1050 , height = 525 ) Statistical Significance \u00b6 You will often want to report and analyse the statistical significance of the coefficients generated by the Logistic Regression Model. We do this by calculating the p-values of the coefficients. Typically, any coefficient with p-value > 0.05 is regarded as insignificant , and hence should not be reported as a contributing feature. Below, we show an example of how to get the summary statistics including the p-values using the .get_stats() function. As can be seen from the resulting dataframe, there are 2 features - EDUCATION and BILL_AMT1 - with \"unreliable\" p-values . The coefficients can be further analysed using the weight_plot() function. The 2-sigma confidence interval is plotted. Assuming a Gaussian distribution, 95% of data exists within this spread. The plot corroboartes the p-values : we can see that there is a significant chance the coefficients of EDUCATION and BILL_AMT1 are 0. from skorecard.datasets import load_uci_credit_card from skorecard.bucketers import EqualFrequencyBucketer from skorecard.linear_model import LogisticRegression from skorecard.reporting import weight_plot from sklearn.pipeline import Pipeline X , y = load_uci_credit_card ( return_X_y = True ) pipeline = Pipeline ([ ( 'bucketer' , EqualFrequencyBucketer ( n_bins = 10 )), ( 'clf' , LogisticRegression ( calculate_stats = True )) ]) pipeline . fit ( X , y ) stats = pipeline . named_steps [ 'clf' ] . get_stats () stats /Users/iv58uq/Documents/open_source/skorecard/skorecard/bucketers/bucketers.py:502: ApproximationWarning: Approximated quantiles - too many unique values /Users/iv58uq/Documents/open_source/skorecard/skorecard/bucketers/bucketers.py:502: ApproximationWarning: Approximated quantiles - too many unique values .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Coef. Std.Err z P>|z| const -0.537571 0.096108 -5.593394 2.226735e-08 EDUCATION 0.010091 0.044874 0.224876 8.220757e-01 MARRIAGE -0.255608 0.062513 -4.088864 4.334903e-05 LIMIT_BAL -0.136681 0.011587 -11.796145 4.086051e-32 BILL_AMT1 -0.006634 0.011454 -0.579160 5.624809e-01 weight_plot ( stats , format = \"png\" , scale = 2 , width = 1050 , height = 525 ) Reporting in Bucketing Process \u00b6 The Bucketing Process module incorporates two bucketing steps: - the prebucketing step - bucketing step Let's first fit a bucketing process step from skorecard import datasets from skorecard.bucketers import DecisionTreeBucketer , OptimalBucketer , AsIsCategoricalBucketer from skorecard.pipeline import BucketingProcess from sklearn.pipeline import make_pipeline df = datasets . load_uci_credit_card ( as_frame = True ) y = df [ \"default\" ] X = df . drop ( columns = [ \"default\" ]) num_cols = [ \"LIMIT_BAL\" , \"BILL_AMT1\" ] cat_cols = [ \"EDUCATION\" , \"MARRIAGE\" ] bucketing_process = BucketingProcess ( prebucketing_pipeline = make_pipeline ( DecisionTreeBucketer ( variables = num_cols , max_n_bins = 100 , min_bin_size = 0.05 ), AsIsCategoricalBucketer ( variables = cat_cols ) ), bucketing_pipeline = make_pipeline ( OptimalBucketer ( variables = num_cols , max_n_bins = 10 , min_bin_size = 0.05 ), OptimalBucketer ( variables = cat_cols , variables_type = 'categorical' , max_n_bins = 10 , min_bin_size = 0.05 ), ) ) _ = bucketing_process . fit ( X , y ) Prebucketing step \u00b6 Retrieve the bucketing report of the prebucketing step by calling the prebucket_table . In addition to the statstics, the prebucket_table returns also the recommended bucket for the merging. bucketing_process . prebucket_table ( \"LIMIT_BAL\" ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pre-bucket label Count Count (%) Non-event Event Event Rate WoE IV bucket 0 -1 Missing 0.0 0.00 0.0 0.0 NaN 0.000 0.000 -1 1 0 [-inf, 25000.0) 479.0 7.98 300.0 179.0 0.373695 -0.725 0.050 0 2 1 [25000.0, 45000.0) 370.0 6.17 233.0 137.0 0.370270 -0.710 0.037 1 3 2 [45000.0, 55000.0) 676.0 11.27 518.0 158.0 0.233728 -0.054 0.000 2 4 3 [55000.0, 75000.0) 336.0 5.60 233.0 103.0 0.306548 -0.425 0.011 2 5 4 [75000.0, 85000.0) 319.0 5.32 243.0 76.0 0.238245 -0.079 0.000 3 6 5 [85000.0, 105000.0) 330.0 5.50 241.0 89.0 0.269697 -0.245 0.004 3 7 6 [105000.0, 145000.0) 566.0 9.43 436.0 130.0 0.229682 -0.031 0.000 4 8 7 [145000.0, 175000.0) 449.0 7.48 380.0 69.0 0.153675 0.464 0.014 5 9 8 [175000.0, 225000.0) 769.0 12.82 630.0 139.0 0.180754 0.270 0.009 5 10 9 [225000.0, 275000.0) 501.0 8.35 419.0 82.0 0.163673 0.390 0.011 6 11 10 [275000.0, 325000.0) 379.0 6.32 326.0 53.0 0.139842 0.575 0.018 7 12 11 [325000.0, 385000.0) 350.0 5.83 287.0 63.0 0.180000 0.275 0.004 7 13 12 [385000.0, inf) 476.0 7.93 409.0 67.0 0.140756 0.567 0.022 8 Visualizing the bucketing bucketing_process . plot_prebucket ( \"LIMIT_BAL\" , format = \"png\" , scale = 2 , width = 1050 , height = 525 ) Bucketing step \u00b6 Retreving the bucketing table from the second step is the same like in every bucketer, ie bucketing_process . bucket_table ( \"LIMIT_BAL\" ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bucket label Count Count (%) Non-event Event Event Rate WoE IV 0 -1 Missing 0.0 0.00 0.0 0.0 NaN 0.000 0.000 1 0 [-inf, 1.0) 479.0 7.98 300.0 179.0 0.373695 -0.725 0.050 2 1 [1.0, 2.0) 370.0 6.17 233.0 137.0 0.370270 -0.710 0.037 3 2 [2.0, 4.0) 1012.0 16.87 751.0 261.0 0.257905 -0.185 0.006 4 3 [4.0, 6.0) 649.0 10.82 484.0 165.0 0.254237 -0.165 0.003 5 4 [6.0, 7.0) 566.0 9.43 436.0 130.0 0.229682 -0.031 0.000 6 5 [7.0, 9.0) 1218.0 20.30 1010.0 208.0 0.170772 0.339 0.021 7 6 [9.0, 10.0) 501.0 8.35 419.0 82.0 0.163673 0.390 0.011 8 7 [10.0, 12.0) 729.0 12.15 613.0 116.0 0.159122 0.423 0.019 9 8 [12.0, inf) 476.0 7.93 409.0 67.0 0.140756 0.567 0.022 and the same applies to plotting the bucketing step bucketing_process . plot_bucket ( \"LIMIT_BAL\" , format = \"png\" , scale = 2 , width = 1050 , height = 525 )","title":"Reporting"},{"location":"tutorials/reporting/#reporting","text":"Reporting plays a crucial role in building scorecard models. Skorecard bucketers include a reporting module, and this tutorial shows how to extract it % matplotlib inline from skorecard.datasets import load_uci_credit_card X , y = load_uci_credit_card ( return_X_y = True ) X . head ( 4 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } EDUCATION MARRIAGE LIMIT_BAL BILL_AMT1 0 1 2 400000.0 201800.0 1 2 2 80000.0 80610.0 2 1 2 500000.0 499452.0 3 1 1 140000.0 450.0","title":"Reporting"},{"location":"tutorials/reporting/#reporting-in-bucketers","text":"Once a bucketer is fitted, the reporting module is incorporated directly in the bucketer object from skorecard.bucketers import DecisionTreeBucketer bucketer = DecisionTreeBucketer ( max_n_bins = 10 ) X_transformed = bucketer . fit_transform ( X , y ) X_transformed . head ( 4 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } EDUCATION MARRIAGE LIMIT_BAL BILL_AMT1 0 0 1 9 9 1 1 1 3 7 2 0 1 9 9 3 0 0 5 0 Retrieve the bucket summary table bucketer . bucket_table ( column = 'LIMIT_BAL' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bucket label Count Count (%) Non-event Event Event Rate WoE IV 0 -1 Missing 0.0 0.00 0.0 0.0 NaN 0.000 0.000 1 0 [-inf, 45000.0) 849.0 14.15 533.0 316.0 0.372203 -0.719 0.087 2 1 [45000.0, 55000.0) 676.0 11.27 518.0 158.0 0.233728 -0.054 0.000 3 2 [55000.0, 75000.0) 336.0 5.60 233.0 103.0 0.306548 -0.425 0.011 4 3 [75000.0, 85000.0) 319.0 5.32 243.0 76.0 0.238245 -0.079 0.000 5 4 [85000.0, 105000.0) 330.0 5.50 241.0 89.0 0.269697 -0.245 0.004 6 5 [105000.0, 145000.0) 566.0 9.43 436.0 130.0 0.229682 -0.031 0.000 7 6 [145000.0, 275000.0) 1719.0 28.65 1429.0 290.0 0.168703 0.353 0.032 8 7 [275000.0, 325000.0) 379.0 6.32 326.0 53.0 0.139842 0.575 0.018 9 8 [325000.0, 385000.0) 350.0 5.83 287.0 63.0 0.180000 0.275 0.004 10 9 [385000.0, inf) 476.0 7.93 409.0 67.0 0.140756 0.567 0.022 Plotting the buckets bucketer . plot_bucket ( column = 'LIMIT_BAL' , format = \"png\" , scale = 2 , width = 1050 , height = 525 )","title":"Reporting in bucketers"},{"location":"tutorials/reporting/#statistical-significance","text":"You will often want to report and analyse the statistical significance of the coefficients generated by the Logistic Regression Model. We do this by calculating the p-values of the coefficients. Typically, any coefficient with p-value > 0.05 is regarded as insignificant , and hence should not be reported as a contributing feature. Below, we show an example of how to get the summary statistics including the p-values using the .get_stats() function. As can be seen from the resulting dataframe, there are 2 features - EDUCATION and BILL_AMT1 - with \"unreliable\" p-values . The coefficients can be further analysed using the weight_plot() function. The 2-sigma confidence interval is plotted. Assuming a Gaussian distribution, 95% of data exists within this spread. The plot corroboartes the p-values : we can see that there is a significant chance the coefficients of EDUCATION and BILL_AMT1 are 0. from skorecard.datasets import load_uci_credit_card from skorecard.bucketers import EqualFrequencyBucketer from skorecard.linear_model import LogisticRegression from skorecard.reporting import weight_plot from sklearn.pipeline import Pipeline X , y = load_uci_credit_card ( return_X_y = True ) pipeline = Pipeline ([ ( 'bucketer' , EqualFrequencyBucketer ( n_bins = 10 )), ( 'clf' , LogisticRegression ( calculate_stats = True )) ]) pipeline . fit ( X , y ) stats = pipeline . named_steps [ 'clf' ] . get_stats () stats /Users/iv58uq/Documents/open_source/skorecard/skorecard/bucketers/bucketers.py:502: ApproximationWarning: Approximated quantiles - too many unique values /Users/iv58uq/Documents/open_source/skorecard/skorecard/bucketers/bucketers.py:502: ApproximationWarning: Approximated quantiles - too many unique values .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Coef. Std.Err z P>|z| const -0.537571 0.096108 -5.593394 2.226735e-08 EDUCATION 0.010091 0.044874 0.224876 8.220757e-01 MARRIAGE -0.255608 0.062513 -4.088864 4.334903e-05 LIMIT_BAL -0.136681 0.011587 -11.796145 4.086051e-32 BILL_AMT1 -0.006634 0.011454 -0.579160 5.624809e-01 weight_plot ( stats , format = \"png\" , scale = 2 , width = 1050 , height = 525 )","title":"Statistical Significance"},{"location":"tutorials/reporting/#reporting-in-bucketing-process","text":"The Bucketing Process module incorporates two bucketing steps: - the prebucketing step - bucketing step Let's first fit a bucketing process step from skorecard import datasets from skorecard.bucketers import DecisionTreeBucketer , OptimalBucketer , AsIsCategoricalBucketer from skorecard.pipeline import BucketingProcess from sklearn.pipeline import make_pipeline df = datasets . load_uci_credit_card ( as_frame = True ) y = df [ \"default\" ] X = df . drop ( columns = [ \"default\" ]) num_cols = [ \"LIMIT_BAL\" , \"BILL_AMT1\" ] cat_cols = [ \"EDUCATION\" , \"MARRIAGE\" ] bucketing_process = BucketingProcess ( prebucketing_pipeline = make_pipeline ( DecisionTreeBucketer ( variables = num_cols , max_n_bins = 100 , min_bin_size = 0.05 ), AsIsCategoricalBucketer ( variables = cat_cols ) ), bucketing_pipeline = make_pipeline ( OptimalBucketer ( variables = num_cols , max_n_bins = 10 , min_bin_size = 0.05 ), OptimalBucketer ( variables = cat_cols , variables_type = 'categorical' , max_n_bins = 10 , min_bin_size = 0.05 ), ) ) _ = bucketing_process . fit ( X , y )","title":"Reporting in Bucketing Process"},{"location":"tutorials/reporting/#prebucketing-step","text":"Retrieve the bucketing report of the prebucketing step by calling the prebucket_table . In addition to the statstics, the prebucket_table returns also the recommended bucket for the merging. bucketing_process . prebucket_table ( \"LIMIT_BAL\" ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pre-bucket label Count Count (%) Non-event Event Event Rate WoE IV bucket 0 -1 Missing 0.0 0.00 0.0 0.0 NaN 0.000 0.000 -1 1 0 [-inf, 25000.0) 479.0 7.98 300.0 179.0 0.373695 -0.725 0.050 0 2 1 [25000.0, 45000.0) 370.0 6.17 233.0 137.0 0.370270 -0.710 0.037 1 3 2 [45000.0, 55000.0) 676.0 11.27 518.0 158.0 0.233728 -0.054 0.000 2 4 3 [55000.0, 75000.0) 336.0 5.60 233.0 103.0 0.306548 -0.425 0.011 2 5 4 [75000.0, 85000.0) 319.0 5.32 243.0 76.0 0.238245 -0.079 0.000 3 6 5 [85000.0, 105000.0) 330.0 5.50 241.0 89.0 0.269697 -0.245 0.004 3 7 6 [105000.0, 145000.0) 566.0 9.43 436.0 130.0 0.229682 -0.031 0.000 4 8 7 [145000.0, 175000.0) 449.0 7.48 380.0 69.0 0.153675 0.464 0.014 5 9 8 [175000.0, 225000.0) 769.0 12.82 630.0 139.0 0.180754 0.270 0.009 5 10 9 [225000.0, 275000.0) 501.0 8.35 419.0 82.0 0.163673 0.390 0.011 6 11 10 [275000.0, 325000.0) 379.0 6.32 326.0 53.0 0.139842 0.575 0.018 7 12 11 [325000.0, 385000.0) 350.0 5.83 287.0 63.0 0.180000 0.275 0.004 7 13 12 [385000.0, inf) 476.0 7.93 409.0 67.0 0.140756 0.567 0.022 8 Visualizing the bucketing bucketing_process . plot_prebucket ( \"LIMIT_BAL\" , format = \"png\" , scale = 2 , width = 1050 , height = 525 )","title":"Prebucketing step"},{"location":"tutorials/reporting/#bucketing-step","text":"Retreving the bucketing table from the second step is the same like in every bucketer, ie bucketing_process . bucket_table ( \"LIMIT_BAL\" ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bucket label Count Count (%) Non-event Event Event Rate WoE IV 0 -1 Missing 0.0 0.00 0.0 0.0 NaN 0.000 0.000 1 0 [-inf, 1.0) 479.0 7.98 300.0 179.0 0.373695 -0.725 0.050 2 1 [1.0, 2.0) 370.0 6.17 233.0 137.0 0.370270 -0.710 0.037 3 2 [2.0, 4.0) 1012.0 16.87 751.0 261.0 0.257905 -0.185 0.006 4 3 [4.0, 6.0) 649.0 10.82 484.0 165.0 0.254237 -0.165 0.003 5 4 [6.0, 7.0) 566.0 9.43 436.0 130.0 0.229682 -0.031 0.000 6 5 [7.0, 9.0) 1218.0 20.30 1010.0 208.0 0.170772 0.339 0.021 7 6 [9.0, 10.0) 501.0 8.35 419.0 82.0 0.163673 0.390 0.011 8 7 [10.0, 12.0) 729.0 12.15 613.0 116.0 0.159122 0.423 0.019 9 8 [12.0, inf) 476.0 7.93 409.0 67.0 0.140756 0.567 0.022 and the same applies to plotting the bucketing step bucketing_process . plot_bucket ( \"LIMIT_BAL\" , format = \"png\" , scale = 2 , width = 1050 , height = 525 )","title":"Bucketing step"},{"location":"tutorials/specials/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Special values \u00b6 You might have some features with values that you need to have in a separate bucket. You can define a dictionary with the buckets you want, and pass them to the bucketer. In the example below, the special values for the variable \"EDUCATION\" are put into a separate bucket, -3. Note that this bucket is not included in the n_bins parameter from skorecard.bucketers import EqualWidthBucketer from skorecard.datasets import load_uci_credit_card X , y = load_uci_credit_card ( return_X_y = True ) specials = { \"LIMIT_BAL\" : { \"=50000\" : [ 50000 ], \"in [20001,30000]\" : [ 20000 , 30000 ]}, \"EDUCATION\" : { \"=High School, Graduate School\" : [ 1 , 3 ]} } cols = [ 'LIMIT_BAL' , 'EDUCATION' ] X_transformed = EqualWidthBucketer ( n_bins = 3 , specials = specials , variables = cols ) . fit_transform ( X , y ) X_transformed [ 'EDUCATION' ] . value_counts () -3 3199 0 2726 2 62 1 13 Name: EDUCATION, dtype: int64","title":"Special Values"},{"location":"tutorials/specials/#special-values","text":"You might have some features with values that you need to have in a separate bucket. You can define a dictionary with the buckets you want, and pass them to the bucketer. In the example below, the special values for the variable \"EDUCATION\" are put into a separate bucket, -3. Note that this bucket is not included in the n_bins parameter from skorecard.bucketers import EqualWidthBucketer from skorecard.datasets import load_uci_credit_card X , y = load_uci_credit_card ( return_X_y = True ) specials = { \"LIMIT_BAL\" : { \"=50000\" : [ 50000 ], \"in [20001,30000]\" : [ 20000 , 30000 ]}, \"EDUCATION\" : { \"=High School, Graduate School\" : [ 1 , 3 ]} } cols = [ 'LIMIT_BAL' , 'EDUCATION' ] X_transformed = EqualWidthBucketer ( n_bins = 3 , specials = specials , variables = cols ) . fit_transform ( X , y ) X_transformed [ 'EDUCATION' ] . value_counts () -3 3199 0 2726 2 62 1 13 Name: EDUCATION, dtype: int64","title":"Special values"},{"location":"tutorials/the_basics/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); The Basics \u00b6 Dummy dataset \u00b6 Let's start first with a dummy dataset based on the UCI credit card dataset. from skorecard.datasets import load_uci_credit_card X , y = load_uci_credit_card ( return_X_y = True ) X . head ( 4 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } EDUCATION MARRIAGE LIMIT_BAL BILL_AMT1 0 1 2 400000.0 201800.0 1 2 2 80000.0 80610.0 2 1 2 500000.0 499452.0 3 1 1 140000.0 450.0 A basic bucketer \u00b6 skorecard offers a set of bucketers that have a scikit-learn compatible interface. By default they will bucket all variables into n_bins buckets. Some bucketers like OptimalBucketer and DecisionTreeBucketer are supervised and can use information from y to find good buckets. You can control the numbers of buckets using max_n_bins instead of n_bins . from skorecard.bucketers import DecisionTreeBucketer bucketer = DecisionTreeBucketer ( max_n_bins = 10 ) X_transformed = bucketer . fit_transform ( X , y ) X_transformed . head ( 4 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } EDUCATION MARRIAGE LIMIT_BAL BILL_AMT1 0 0 1 9 9 1 1 1 3 7 2 0 1 9 9 3 0 0 5 0 X_transformed [ 'BILL_AMT1' ] . value_counts () . sort_index () 0 1343 1 404 2 574 3 462 4 400 5 359 6 857 7 789 8 500 9 312 Name: BILL_AMT1, dtype: int64 Bucketing specific variables \u00b6 Instead of applying a bucketer on all features, you'll likely want to apply it only to specific features. You can use the variables parameter for that: bucketer = DecisionTreeBucketer ( max_n_bins = 10 , variables = [ \"BILL_AMT1\" ]) bucketer . fit_transform ( X , y ) . head ( 4 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } EDUCATION MARRIAGE LIMIT_BAL BILL_AMT1 0 1 2 400000.0 9 1 2 2 80000.0 7 2 1 2 500000.0 9 3 1 1 140000.0 0 Inspecting bucketing results \u00b6 skorecard bucketers have some methods to help you inspect the result of the bucketing process: from skorecard.bucketers import EqualWidthBucketer bucketer = EqualWidthBucketer ( n_bins = 5 , variables = [ \"BILL_AMT1\" ]) bucketer . fit ( X , y ) bucketer . bucket_table ( 'BILL_AMT1' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bucket label Count Count (%) Non-event Event Event Rate WoE IV 0 -1 Missing 0.0 0.00 0.0 0.0 NaN 0.000 0.000 1 0 (-inf, -10319.399999999994] 3.0 0.05 3.0 0.0 0.000000 4.181 -0.003 2 1 (-10319.399999999994, 144941.2] 5408.0 90.13 4188.0 1220.0 0.225592 -0.008 -0.000 3 2 (144941.2, 300201.80000000005] 490.0 8.17 395.0 95.0 0.193878 0.183 -0.003 4 3 (300201.80000000005, 455462.4] 75.0 1.25 55.0 20.0 0.266667 -0.230 -0.001 5 4 (455462.4, inf] 24.0 0.40 14.0 10.0 0.416667 -0.903 -0.004 bucketer . plot_bucket ( 'BILL_AMT1' , format = \"png\" , scale = 2 , width = 1050 , height = 525 ) # remove format argument for an interactive plotly plot.","title":"The Basics"},{"location":"tutorials/the_basics/#the-basics","text":"","title":"The Basics"},{"location":"tutorials/the_basics/#dummy-dataset","text":"Let's start first with a dummy dataset based on the UCI credit card dataset. from skorecard.datasets import load_uci_credit_card X , y = load_uci_credit_card ( return_X_y = True ) X . head ( 4 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } EDUCATION MARRIAGE LIMIT_BAL BILL_AMT1 0 1 2 400000.0 201800.0 1 2 2 80000.0 80610.0 2 1 2 500000.0 499452.0 3 1 1 140000.0 450.0","title":"Dummy dataset"},{"location":"tutorials/the_basics/#a-basic-bucketer","text":"skorecard offers a set of bucketers that have a scikit-learn compatible interface. By default they will bucket all variables into n_bins buckets. Some bucketers like OptimalBucketer and DecisionTreeBucketer are supervised and can use information from y to find good buckets. You can control the numbers of buckets using max_n_bins instead of n_bins . from skorecard.bucketers import DecisionTreeBucketer bucketer = DecisionTreeBucketer ( max_n_bins = 10 ) X_transformed = bucketer . fit_transform ( X , y ) X_transformed . head ( 4 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } EDUCATION MARRIAGE LIMIT_BAL BILL_AMT1 0 0 1 9 9 1 1 1 3 7 2 0 1 9 9 3 0 0 5 0 X_transformed [ 'BILL_AMT1' ] . value_counts () . sort_index () 0 1343 1 404 2 574 3 462 4 400 5 359 6 857 7 789 8 500 9 312 Name: BILL_AMT1, dtype: int64","title":"A basic bucketer"},{"location":"tutorials/the_basics/#bucketing-specific-variables","text":"Instead of applying a bucketer on all features, you'll likely want to apply it only to specific features. You can use the variables parameter for that: bucketer = DecisionTreeBucketer ( max_n_bins = 10 , variables = [ \"BILL_AMT1\" ]) bucketer . fit_transform ( X , y ) . head ( 4 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } EDUCATION MARRIAGE LIMIT_BAL BILL_AMT1 0 1 2 400000.0 9 1 2 2 80000.0 7 2 1 2 500000.0 9 3 1 1 140000.0 0","title":"Bucketing specific variables"},{"location":"tutorials/the_basics/#inspecting-bucketing-results","text":"skorecard bucketers have some methods to help you inspect the result of the bucketing process: from skorecard.bucketers import EqualWidthBucketer bucketer = EqualWidthBucketer ( n_bins = 5 , variables = [ \"BILL_AMT1\" ]) bucketer . fit ( X , y ) bucketer . bucket_table ( 'BILL_AMT1' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bucket label Count Count (%) Non-event Event Event Rate WoE IV 0 -1 Missing 0.0 0.00 0.0 0.0 NaN 0.000 0.000 1 0 (-inf, -10319.399999999994] 3.0 0.05 3.0 0.0 0.000000 4.181 -0.003 2 1 (-10319.399999999994, 144941.2] 5408.0 90.13 4188.0 1220.0 0.225592 -0.008 -0.000 3 2 (144941.2, 300201.80000000005] 490.0 8.17 395.0 95.0 0.193878 0.183 -0.003 4 3 (300201.80000000005, 455462.4] 75.0 1.25 55.0 20.0 0.266667 -0.230 -0.001 5 4 (455462.4, inf] 24.0 0.40 14.0 10.0 0.416667 -0.903 -0.004 bucketer . plot_bucket ( 'BILL_AMT1' , format = \"png\" , scale = 2 , width = 1050 , height = 525 ) # remove format argument for an interactive plotly plot.","title":"Inspecting bucketing results"},{"location":"tutorials/using-bucketing-process/","text":"(function() { function addWidgetsRenderer() { var requireJsScript = document.createElement('script'); requireJsScript.src = 'https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js'; var mimeElement = document.querySelector('script[type=\"application/vnd.jupyter.widget-view+json\"]'); var jupyterWidgetsScript = document.createElement('script'); var widgetRendererSrc = 'https://unpkg.com/@jupyter-widgets/html-manager@*/dist/embed-amd.js'; var widgetState; // Fallback for older version: try { widgetState = mimeElement && JSON.parse(mimeElement.innerHTML); if (widgetState && (widgetState.version_major < 2 || !widgetState.version_major)) { widgetRendererSrc = 'jupyter-js-widgets@*/dist/embed.js'; } } catch(e) {} jupyterWidgetsScript.src = widgetRendererSrc; document.body.appendChild(requireJsScript); document.body.appendChild(jupyterWidgetsScript); } document.addEventListener('DOMContentLoaded', addWidgetsRenderer); }()); Using the BucketingProcess \u00b6 The BucketingProcess enables a two-step bucketing approach, where a feature is first pre-bucketed to e.g. 100 pre-buckets, and then bucketed. This is a common practice - it reduces the complexity of finding exact boundaries to the problem of finding which of 100 buckets to merge together. Define the BucketingProcess \u00b6 The bucketing process incorporates a pre-bucketing pipeline and a bucketing pipeline. You can also pass specials or variables and BucketingProcess will pass those settings on to the bucketers in the pipelines. In the example below, we prebucket numerical features to max 100 bins, and prebucket categorical columns as-is (each unique value is a category and new categories end up in the other bucket). from skorecard import datasets from skorecard.bucketers import DecisionTreeBucketer , OptimalBucketer , AsIsCategoricalBucketer from skorecard.pipeline import BucketingProcess from sklearn.pipeline import make_pipeline df = datasets . load_uci_credit_card ( as_frame = True ) y = df [ \"default\" ] X = df . drop ( columns = [ \"default\" ]) num_cols = [ \"LIMIT_BAL\" , \"BILL_AMT1\" ] cat_cols = [ \"EDUCATION\" , \"MARRIAGE\" ] specials = { \"EDUCATION\" : { \"Is 1\" : [ 1 ] } } bucketing_process = BucketingProcess ( prebucketing_pipeline = make_pipeline ( DecisionTreeBucketer ( variables = num_cols , max_n_bins = 100 , min_bin_size = 0.05 ), AsIsCategoricalBucketer ( variables = cat_cols ) ), bucketing_pipeline = make_pipeline ( OptimalBucketer ( variables = num_cols , max_n_bins = 10 , min_bin_size = 0.05 ), OptimalBucketer ( variables = cat_cols , variables_type = 'categorical' , max_n_bins = 10 , min_bin_size = 0.05 ), ), specials = specials ) bucketing_process . fit_transform ( X , y ) . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } EDUCATION MARRIAGE LIMIT_BAL BILL_AMT1 0 -3 0 8 5 1 1 0 3 4 2 -3 0 8 5 3 -3 1 4 0 4 1 1 8 3 Methods and Attributes \u00b6 A BucketingProcess instance has all the similar methods & attributes of a bucketer: .summary() .bucket_table(column) .plot_bucket(column) .features_bucket_mapping .save_to_yaml() .fit_interactive() but also adds a few unique ones: .prebucket_table(column) .plot_prebucket(column) bucketing_process . summary () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } column num_prebuckets num_buckets IV_score dtype 0 EDUCATION 9 5 0.036308 int64 1 MARRIAGE 6 4 0.013054 int64 2 LIMIT_BAL 14 10 0.168862 float64 3 BILL_AMT1 15 7 0.005823 float64 bucketing_process . prebucket_table ( 'MARRIAGE' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pre-bucket label Count Count (%) Non-event Event Event Rate WoE IV bucket 0 -2 Other 0.0 0.00 0.0 0.0 NaN 0.000 0.000 -2 1 -1 Missing 0.0 0.00 0.0 0.0 NaN 0.000 0.000 -1 2 0 2 3138.0 52.30 2493.0 645.0 0.205545 0.110 0.006 0 3 1 1 2784.0 46.40 2108.0 676.0 0.242816 -0.104 0.005 1 4 2 3 64.0 1.07 42.0 22.0 0.343750 -0.594 0.004 1 5 3 0 14.0 0.23 12.0 2.0 0.142857 0.547 0.001 0 bucketing_process . bucket_table ( 'MARRIAGE' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bucket label Count Count (%) Non-event Event Event Rate WoE IV 0 -2 Other 0.0 0.00 0.0 0.0 NaN 0.000 0.000 1 -1 Missing 0.0 0.00 0.0 0.0 NaN 0.000 0.000 2 0 0, 3 3152.0 52.53 2505.0 647.0 0.205266 0.112 0.006 3 1 1, 2 2848.0 47.47 2150.0 698.0 0.245084 -0.117 0.007 bucketing_process . plot_prebucket ( \"LIMIT_BAL\" , format = \"png\" , scale = 2 , width = 1050 , height = 525 ) The .features_bucket_mapping attribute \u00b6 All skorecard bucketing classes have a .features_bucket_mapping attribute to access the stored bucketing information to go from an input feature to a bucketed feature. In the case of BucketingProcess , because there is a prebucketing and bucketing step, this means the bucket mapping reflects the net effect of merging both steps into one. This is demonstrated below: bucketing_process . pre_pipeline_ . features_bucket_mapping_ . get ( 'MARRIAGE' ) . labels {3: '0', 1: '1', 0: '2', 2: '3', -1: 'Missing', -2: 'Other'} bucketing_process . pipeline_ . features_bucket_mapping_ . get ( 'EDUCATION' ) BucketMapping(feature_name='EDUCATION', type='categorical', missing_bucket=None, other_bucket=None, map={1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 0: 1}, right=False, specials={'Is 1': [-3]}) bucketing_process . features_bucket_mapping_ . get ( 'EDUCATION' ) BucketMapping(feature_name='EDUCATION', type='categorical', missing_bucket=None, other_bucket=None, map={0: 0, 3: 0, 4: 0, 5: 0, 6: 0, 2: 1}, right=True, specials={'Is 1': [1]}) The .fit_interactive() method \u00b6 All skorecard bucketing classes have a .fit_interactive() method. In the case of BucketingProcess this will launch a slightly different app that shows the pre-buckets and the buckets, and allows you to edit the prebucketing as well. # bucketing_process.fit_interactive(X, y) # not run","title":"BucketingProcess"},{"location":"tutorials/using-bucketing-process/#using-the-bucketingprocess","text":"The BucketingProcess enables a two-step bucketing approach, where a feature is first pre-bucketed to e.g. 100 pre-buckets, and then bucketed. This is a common practice - it reduces the complexity of finding exact boundaries to the problem of finding which of 100 buckets to merge together.","title":"Using the BucketingProcess"},{"location":"tutorials/using-bucketing-process/#define-the-bucketingprocess","text":"The bucketing process incorporates a pre-bucketing pipeline and a bucketing pipeline. You can also pass specials or variables and BucketingProcess will pass those settings on to the bucketers in the pipelines. In the example below, we prebucket numerical features to max 100 bins, and prebucket categorical columns as-is (each unique value is a category and new categories end up in the other bucket). from skorecard import datasets from skorecard.bucketers import DecisionTreeBucketer , OptimalBucketer , AsIsCategoricalBucketer from skorecard.pipeline import BucketingProcess from sklearn.pipeline import make_pipeline df = datasets . load_uci_credit_card ( as_frame = True ) y = df [ \"default\" ] X = df . drop ( columns = [ \"default\" ]) num_cols = [ \"LIMIT_BAL\" , \"BILL_AMT1\" ] cat_cols = [ \"EDUCATION\" , \"MARRIAGE\" ] specials = { \"EDUCATION\" : { \"Is 1\" : [ 1 ] } } bucketing_process = BucketingProcess ( prebucketing_pipeline = make_pipeline ( DecisionTreeBucketer ( variables = num_cols , max_n_bins = 100 , min_bin_size = 0.05 ), AsIsCategoricalBucketer ( variables = cat_cols ) ), bucketing_pipeline = make_pipeline ( OptimalBucketer ( variables = num_cols , max_n_bins = 10 , min_bin_size = 0.05 ), OptimalBucketer ( variables = cat_cols , variables_type = 'categorical' , max_n_bins = 10 , min_bin_size = 0.05 ), ), specials = specials ) bucketing_process . fit_transform ( X , y ) . head () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } EDUCATION MARRIAGE LIMIT_BAL BILL_AMT1 0 -3 0 8 5 1 1 0 3 4 2 -3 0 8 5 3 -3 1 4 0 4 1 1 8 3","title":"Define the BucketingProcess"},{"location":"tutorials/using-bucketing-process/#methods-and-attributes","text":"A BucketingProcess instance has all the similar methods & attributes of a bucketer: .summary() .bucket_table(column) .plot_bucket(column) .features_bucket_mapping .save_to_yaml() .fit_interactive() but also adds a few unique ones: .prebucket_table(column) .plot_prebucket(column) bucketing_process . summary () .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } column num_prebuckets num_buckets IV_score dtype 0 EDUCATION 9 5 0.036308 int64 1 MARRIAGE 6 4 0.013054 int64 2 LIMIT_BAL 14 10 0.168862 float64 3 BILL_AMT1 15 7 0.005823 float64 bucketing_process . prebucket_table ( 'MARRIAGE' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } pre-bucket label Count Count (%) Non-event Event Event Rate WoE IV bucket 0 -2 Other 0.0 0.00 0.0 0.0 NaN 0.000 0.000 -2 1 -1 Missing 0.0 0.00 0.0 0.0 NaN 0.000 0.000 -1 2 0 2 3138.0 52.30 2493.0 645.0 0.205545 0.110 0.006 0 3 1 1 2784.0 46.40 2108.0 676.0 0.242816 -0.104 0.005 1 4 2 3 64.0 1.07 42.0 22.0 0.343750 -0.594 0.004 1 5 3 0 14.0 0.23 12.0 2.0 0.142857 0.547 0.001 0 bucketing_process . bucket_table ( 'MARRIAGE' ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } bucket label Count Count (%) Non-event Event Event Rate WoE IV 0 -2 Other 0.0 0.00 0.0 0.0 NaN 0.000 0.000 1 -1 Missing 0.0 0.00 0.0 0.0 NaN 0.000 0.000 2 0 0, 3 3152.0 52.53 2505.0 647.0 0.205266 0.112 0.006 3 1 1, 2 2848.0 47.47 2150.0 698.0 0.245084 -0.117 0.007 bucketing_process . plot_prebucket ( \"LIMIT_BAL\" , format = \"png\" , scale = 2 , width = 1050 , height = 525 )","title":"Methods and Attributes"},{"location":"tutorials/using-bucketing-process/#the-features_bucket_mapping-attribute","text":"All skorecard bucketing classes have a .features_bucket_mapping attribute to access the stored bucketing information to go from an input feature to a bucketed feature. In the case of BucketingProcess , because there is a prebucketing and bucketing step, this means the bucket mapping reflects the net effect of merging both steps into one. This is demonstrated below: bucketing_process . pre_pipeline_ . features_bucket_mapping_ . get ( 'MARRIAGE' ) . labels {3: '0', 1: '1', 0: '2', 2: '3', -1: 'Missing', -2: 'Other'} bucketing_process . pipeline_ . features_bucket_mapping_ . get ( 'EDUCATION' ) BucketMapping(feature_name='EDUCATION', type='categorical', missing_bucket=None, other_bucket=None, map={1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 0: 1}, right=False, specials={'Is 1': [-3]}) bucketing_process . features_bucket_mapping_ . get ( 'EDUCATION' ) BucketMapping(feature_name='EDUCATION', type='categorical', missing_bucket=None, other_bucket=None, map={0: 0, 3: 0, 4: 0, 5: 0, 6: 0, 2: 1}, right=True, specials={'Is 1': [1]})","title":"The .features_bucket_mapping attribute"},{"location":"tutorials/using-bucketing-process/#the-fit_interactive-method","text":"All skorecard bucketing classes have a .fit_interactive() method. In the case of BucketingProcess this will launch a slightly different app that shows the pre-buckets and the buckets, and allows you to edit the prebucketing as well. # bucketing_process.fit_interactive(X, y) # not run","title":"The .fit_interactive() method"}]}